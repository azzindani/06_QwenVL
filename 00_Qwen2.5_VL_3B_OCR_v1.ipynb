{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows omni_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:06:22.308399Z",
     "iopub.status.busy": "2025-04-14T07:06:22.308179Z",
     "iopub.status.idle": "2025-04-14T07:06:49.493562Z",
     "shell.execute_reply": "2025-04-14T07:06:49.492696Z",
     "shell.execute_reply.started": "2025-04-14T07:06:22.308379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import io\n",
    "import ast\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:06:49.494981Z",
     "iopub.status.busy": "2025-04-14T07:06:49.494404Z",
     "iopub.status.idle": "2025-04-14T07:09:20.429998Z",
     "shell.execute_reply": "2025-04-14T07:09:20.429296Z",
     "shell.execute_reply.started": "2025-04-14T07:06:49.494957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    ").to(device) #''\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_text_bounding_boxes(image_path, bounding_boxes, input_width, input_height):\n",
    "    \"\"\"\n",
    "    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n",
    "\n",
    "    Args:\n",
    "        image_path: The path to the image file.\n",
    "        bounding_boxes: A list of bounding boxes containing the name of the object\n",
    "         and their positions in normalized [y1 x1 y2 x2] format.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the image\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    print(img.size)\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Parsing out the markdown fencing\n",
    "    bounding_boxes = parse_json(bounding_boxes)\n",
    "\n",
    "    font = ImageFont.truetype('./00_Dataset/NotoSansCJK-Regular.ttc', size = 10)\n",
    "\n",
    "    # Iterate over the bounding boxes\n",
    "    for i, bounding_box in enumerate(ast.literal_eval(bounding_boxes)):\n",
    "      color = 'green'\n",
    "\n",
    "      # Convert normalized coordinates to absolute coordinates\n",
    "      abs_y1 = int(bounding_box[\"bbox_2d\"][1] / input_height * height)\n",
    "      abs_x1 = int(bounding_box[\"bbox_2d\"][0] / input_width * width)\n",
    "      abs_y2 = int(bounding_box[\"bbox_2d\"][3] / input_height * height)\n",
    "      abs_x2 = int(bounding_box[\"bbox_2d\"][2] / input_width * width)\n",
    "\n",
    "      if abs_x1 > abs_x2:\n",
    "        abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "\n",
    "      if abs_y1 > abs_y2:\n",
    "        abs_y1, abs_y2 = abs_y2, abs_y1\n",
    "\n",
    "      # Draw the bounding box\n",
    "      draw.rectangle(\n",
    "          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline = color, width = 1\n",
    "      )\n",
    "\n",
    "      # Draw the text\n",
    "      if 'text_content' in bounding_box:\n",
    "        draw.text((abs_x1, abs_y2), bounding_box['text_content'], fill = color, font = font)\n",
    "\n",
    "    # Display the image\n",
    "    display(img)\n",
    "\n",
    "# @title Parsing JSON output\n",
    "def parse_json(json_output):\n",
    "    # Parsing out the markdown fencing\n",
    "    lines = json_output.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"```json\":\n",
    "            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n",
    "            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n",
    "            break  # Exit the loop once \"```json\" is found\n",
    "    return json_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    prompt,\n",
    "    image_path,\n",
    "    system_prompt = 'You are a helpful assistant',\n",
    "    max_new_tokens = 4096,\n",
    "    min_pixels = 512 * 28 * 28,\n",
    "    max_pixels = 2048 * 28 * 28,\n",
    "    return_input = False\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : 'user',\n",
    "            'content' : [\n",
    "                {\n",
    "                    'type' : 'image',\n",
    "                    'image' : image_path,\n",
    "                    'min_pixels' : min_pixels,\n",
    "                    'max_pixels' : max_pixels,\n",
    "                },\n",
    "                {'type' : 'text', 'text' : prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize = False, add_generation_prompt = True\n",
    "    )\n",
    "    print('input:\\n', text)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    streamer = TextStreamer(processor.tokenizer, skip_special_tokens = True, skip_prompt = True)\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens, streamer = streamer)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n",
    "    )\n",
    "\n",
    "    print('output:\\n', output_text[0])\n",
    "\n",
    "    input_height = inputs['image_grid_thw'][0][1] * 14\n",
    "    input_width = inputs['image_grid_thw'][0][2] * 14\n",
    "\n",
    "    if return_input:\n",
    "        return output_text[0], inputs\n",
    "    else:\n",
    "        return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.01 Full-page OCR for English Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_path = './00_Dataset/ocr_example2.jpg'\n",
    "prompt = 'Read all the text in the image.'\n",
    "\n",
    "image = Image.open(image_path)\n",
    "display(image.resize((400, 600)))\n",
    "\n",
    "response = inference(prompt, image_path)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.02 Full Page OCR for Multilingual Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_path = './00_Dataset/ocr_example1.jpg'\n",
    "prompt = 'Please output only the text content from the image without any additional descriptions or formatting.'\n",
    "\n",
    "image = Image.open(image_path)\n",
    "display(image.resize((400, 400)))\n",
    "\n",
    "response = inference(prompt, image_path)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_path = './00_Dataset/ocr_example5.jpg'\n",
    "prompt = '请识别出图中所有的文字。'\n",
    "\n",
    "image = Image.open(image_path)\n",
    "display(image.resize((512, 512)))\n",
    "\n",
    "response = inference(prompt, image_path)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.03 Text Spotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_path = './00_Dataset/ocr_example3.jpg'\n",
    "prompt = 'Spotting all the text in the image with line-level, and output in JSON format.'\n",
    "\n",
    "image = Image.open(image_path)\n",
    "display(image.resize((512, 512)))\n",
    "\n",
    "response, inputs = inference(prompt, image_path, return_input = True)\n",
    "\n",
    "display(Markdown(response))\n",
    "input_height = inputs['image_grid_thw'][0][1] * 14\n",
    "input_width = inputs['image_grid_thw'][0][2] * 14\n",
    "print(input_height, input_width)\n",
    "plot_text_bounding_boxes(image_path, response, input_width, input_height)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.04 Visual Information Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_path = './00_Dataset/ocr_example3.jpg'\n",
    "prompt = \"Extract the key-value information in the format:{\\\"company\\\": \\\"\\\", \\\"date\\\": \\\"\\\", \\\"address\\\": \\\"\\\", \\\"total\\\": \\\"\\\"}\"\n",
    "\n",
    "image = Image.open(image_path)\n",
    "display(image.resize((300, 500)))\n",
    "\n",
    "response = inference(prompt, image_path)\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image_path = './00_Dataset/ocr_example4.jpg'\n",
    "prompt = \"提取图中的：['发票代码','发票号码','到站','燃油费','票价','乘车日期','开车时间','车次','座号']，并且按照json格式输出。\"\n",
    "\n",
    "image = Image.open(image_path)\n",
    "display(image.resize((400, 400)))\n",
    "\n",
    "response = inference(prompt, image_path)\n",
    "display(Markdown(response))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "omni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
