{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-07-24T08:05:42.095106Z",
     "iopub.status.busy": "2025-07-24T08:05:42.094449Z",
     "iopub.status.idle": "2025-07-24T08:07:27.259381Z",
     "shell.execute_reply": "2025-07-24T08:07:27.258573Z",
     "shell.execute_reply.started": "2025-07-24T08:05:42.095074Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install qwen-vl-utils\n",
    "!pip install qwen_agent\n",
    "!pip install openai\n",
    "!pip install icecream\n",
    "!pip install dotenv\n",
    "!pip install bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:07:27.260995Z",
     "iopub.status.busy": "2025-07-24T08:07:27.260740Z",
     "iopub.status.idle": "2025-07-24T08:07:30.835171Z",
     "shell.execute_reply": "2025-07-24T08:07:30.834154Z",
     "shell.execute_reply.started": "2025-07-24T08:07:27.260969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!git clone https://github.com/QwenLM/Qwen2.5-VL.git\n",
    "%cd Qwen2.5-VL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:07:30.836480Z",
     "iopub.status.busy": "2025-07-24T08:07:30.836227Z",
     "iopub.status.idle": "2025-07-24T08:08:09.631680Z",
     "shell.execute_reply": "2025-07-24T08:08:09.630927Z",
     "shell.execute_reply.started": "2025-07-24T08:07:30.836453Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import io\n",
    "import ast\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as osp\n",
    "import math\n",
    "import numpy as np\n",
    "import base64\n",
    "import hashlib\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from enum import Enum\n",
    "from typing import Optional, Dict, Any, List, Tuple, Union\n",
    "from copy import deepcopy\n",
    "\n",
    "from icecream import ic\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info, smart_resize\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "from IPython.display import display, Markdown, Video\n",
    "from matplotlib import pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "from qwen_agent.llm.fncall_prompts.nous_fncall_prompt import (\n",
    "    NousFnCallPrompt,\n",
    "    Message,\n",
    "    ContentItem,\n",
    ")\n",
    "from cookbooks.utils.agent_function_call import ComputerUse, MobileUse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:08:09.634086Z",
     "iopub.status.busy": "2025-07-24T08:08:09.633529Z",
     "iopub.status.idle": "2025-07-24T08:08:09.637779Z",
     "shell.execute_reply": "2025-07-24T08:08:09.637097Z",
     "shell.execute_reply.started": "2025-07-24T08:08:09.634062Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:08:09.638570Z",
     "iopub.status.busy": "2025-07-24T08:08:09.638387Z",
     "iopub.status.idle": "2025-07-24T08:08:09.675743Z",
     "shell.execute_reply": "2025-07-24T08:08:09.675162Z",
     "shell.execute_reply.started": "2025-07-24T08:08:09.638554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "'''model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_id, torch_dtype=\"auto\", device_map=\"auto\"\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id) #'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:08:09.676516Z",
     "iopub.status.busy": "2025-07-24T08:08:09.676339Z",
     "iopub.status.idle": "2025-07-24T08:09:17.602742Z",
     "shell.execute_reply": "2025-07-24T08:09:17.601901Z",
     "shell.execute_reply.started": "2025-07-24T08:08:09.676496Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")#.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id) #'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:09:17.604026Z",
     "iopub.status.busy": "2025-07-24T08:09:17.603794Z",
     "iopub.status.idle": "2025-07-24T08:09:17.652786Z",
     "shell.execute_reply": "2025-07-24T08:09:17.651881Z",
     "shell.execute_reply.started": "2025-07-24T08:09:17.604008Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Task Types Enumeration\n",
    "class TaskType(Enum):\n",
    "    COMPUTER_AGENT = \"computer_agent\"\n",
    "    DOCUMENT_PARSING = \"document_parsing\"\n",
    "    MOBILE_AGENT = \"mobile_agent\"\n",
    "    OCR = \"ocr\"\n",
    "    RECOGNITION = \"recognition\"\n",
    "    SPATIAL_UNDERSTANDING = \"spatial_understanding\"\n",
    "    VIDEO_INFERENCE = \"video_inference\"\n",
    "    VIDEO_UNDERSTANDING = \"video_understanding\"\n",
    "\n",
    "# System Prompts for Different Tasks\n",
    "SYSTEM_PROMPTS = {\n",
    "    TaskType.COMPUTER_AGENT: \"You are a helpful assistant that can control computer interfaces through function calls.\",\n",
    "    TaskType.DOCUMENT_PARSING: \"You are an AI specialized in recognizing and extracting text from images. Your mission is to analyze the image document and generate the result in QwenVL Document Parser HTML format using specified tags while maintaining user privacy and data integrity.\",\n",
    "    TaskType.MOBILE_AGENT: \"You are a helpful assistant that can control mobile interfaces through function calls.\",\n",
    "    TaskType.OCR: \"You are a helpful assistant specialized in optical character recognition and text extraction from images.\",\n",
    "    TaskType.RECOGNITION: \"You are a helpful assistant specialized in image recognition and detailed description.\",\n",
    "    TaskType.SPATIAL_UNDERSTANDING: \"You are a helpful assistant specialized in understanding spatial relationships and object positioning in images.\",\n",
    "    TaskType.VIDEO_INFERENCE: \"You are a helpful assistant specialized in analyzing and describing video content.\",\n",
    "    TaskType.VIDEO_UNDERSTANDING: \"You are a helpful assistant specialized in detailed video analysis and understanding temporal relationships.\"\n",
    "}\n",
    "\n",
    "# Utility Functions\n",
    "additional_colors = [colorname for (colorname, colorcode) in ImageColor.colormap.items()]\n",
    "\n",
    "def download_font_if_needed():\n",
    "    \"\"\"Download font file if running in Kaggle or similar environment\"\"\"\n",
    "    font_dir = './fonts'\n",
    "    font_path = os.path.join(font_dir, 'NotoSansCJK-Regular.ttc')\n",
    "    \n",
    "    if not os.path.exists(font_path):\n",
    "        try:\n",
    "            import urllib.request\n",
    "            os.makedirs(font_dir, exist_ok=True)\n",
    "            \n",
    "            # Download Noto Sans CJK from Google Fonts\n",
    "            font_url = \"https://github.com/googlefonts/noto-cjk/raw/main/Sans/OTC/NotoSansCJK-Regular.ttc\"\n",
    "            print(f\"Downloading font from {font_url}...\")\n",
    "            urllib.request.urlretrieve(font_url, font_path)\n",
    "            print(f\"Font downloaded successfully to {font_path}\")\n",
    "            return font_path\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to download font: {e}\")\n",
    "            return None\n",
    "    return font_path\n",
    "\n",
    "def get_font(size=14):\n",
    "    \"\"\"Get a font with fallback options including online download\"\"\"\n",
    "    # Try to download font if in cloud environment (like Kaggle)\n",
    "    downloaded_font = download_font_if_needed()\n",
    "    \n",
    "    font_paths = [\n",
    "        downloaded_font,  # Downloaded font (if available)\n",
    "        './00_Dataset/NotoSansCJK-Regular.ttc',  # Original path\n",
    "        './fonts/NotoSansCJK-Regular.ttc',  # Downloaded location\n",
    "        '/System/Library/Fonts/Arial.ttf',  # macOS\n",
    "        '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',  # Linux\n",
    "        'C:/Windows/Fonts/arial.ttf',  # Windows\n",
    "        '/usr/share/fonts/TTF/DejaVuSans.ttf',  # Some Linux distributions\n",
    "        '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',  # Common on Linux\n",
    "        '/opt/conda/lib/python3.*/site-packages/matplotlib/mpl-data/fonts/ttf/DejaVuSans.ttf',  # Conda environments\n",
    "    ]\n",
    "    \n",
    "    # Filter out None values\n",
    "    font_paths = [path for path in font_paths if path is not None]\n",
    "    \n",
    "    for font_path in font_paths:\n",
    "        try:\n",
    "            if os.path.exists(font_path):\n",
    "                return ImageFont.truetype(font_path, size)\n",
    "        except (OSError, IOError):\n",
    "            continue\n",
    "    \n",
    "    # Try matplotlib fonts (common in Kaggle/Colab)\n",
    "    try:\n",
    "        import matplotlib.font_manager as fm\n",
    "        font_files = fm.findSystemFonts()\n",
    "        for font_file in font_files[:5]:  # Try first 5 fonts\n",
    "            try:\n",
    "                return ImageFont.truetype(font_file, size)\n",
    "            except:\n",
    "                continue\n",
    "    except ImportError:\n",
    "        pass\n",
    "    \n",
    "    # If no truetype font is found, use default font\n",
    "    try:\n",
    "        return ImageFont.load_default()\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def draw_point(image: Image.Image, point: list, color=None):\n",
    "    \"\"\"Draw a point on the image for visualization\"\"\"\n",
    "    if isinstance(color, str):\n",
    "        try:\n",
    "            color = ImageColor.getrgb(color)\n",
    "            color = color + (128,)  \n",
    "        except ValueError:\n",
    "            color = (255, 0, 0, 128)  \n",
    "    else:\n",
    "        color = (255, 0, 0, 128)  \n",
    "\n",
    "    overlay = Image.new('RGBA', image.size, (255, 255, 255, 0))\n",
    "    overlay_draw = ImageDraw.Draw(overlay)\n",
    "    radius = min(image.size) * 0.05\n",
    "    x, y = point\n",
    "\n",
    "    overlay_draw.ellipse(\n",
    "        [(x - radius, y - radius), (x + radius, y + radius)],\n",
    "        fill=color\n",
    "    )\n",
    "    \n",
    "    center_radius = radius * 0.1\n",
    "    overlay_draw.ellipse(\n",
    "        [(x - center_radius, y - center_radius), \n",
    "         (x + center_radius, y + center_radius)],\n",
    "        fill=(0, 255, 0, 255)\n",
    "    )\n",
    "\n",
    "    image = image.convert('RGBA')\n",
    "    combined = Image.alpha_composite(image, overlay)\n",
    "    return combined.convert('RGB')\n",
    "\n",
    "def parse_json(json_output):\n",
    "    \"\"\"Parse JSON output from model responses\"\"\"\n",
    "    lines = json_output.splitlines()\n",
    "    for i, line in enumerate(lines):\n",
    "        if line == \"```json\":\n",
    "            json_output = \"\\n\".join(lines[i+1:])\n",
    "            json_output = json_output.split(\"```\")[0]\n",
    "            break\n",
    "    return json_output\n",
    "\n",
    "def decode_xml_points(text):\n",
    "    \"\"\"Decode XML points for spatial understanding\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(text)\n",
    "        num_points = (len(root.attrib) - 1) // 2\n",
    "        points = []\n",
    "        for i in range(num_points):\n",
    "            x = root.attrib.get(f'x{i+1}')\n",
    "            y = root.attrib.get(f'y{i+1}')\n",
    "            points.append([x, y])\n",
    "        alt = root.attrib.get('alt')\n",
    "        phrase = root.text.strip() if root.text else None\n",
    "        return {\n",
    "            \"points\": points,\n",
    "            \"alt\": alt,\n",
    "            \"phrase\": phrase\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "def clean_and_format_html(full_predict):\n",
    "    \"\"\"Clean and format HTML content for document parsing\"\"\"\n",
    "    soup = BeautifulSoup(full_predict, 'html.parser')\n",
    "    \n",
    "    color_pattern = re.compile(r'\\bcolor:[^;]+;?')\n",
    "    for tag in soup.find_all(style=True):\n",
    "        original_style = tag.get('style', '')\n",
    "        new_style = color_pattern.sub('', original_style)\n",
    "        if not new_style.strip():\n",
    "            del tag['style']\n",
    "        else:\n",
    "            new_style = new_style.rstrip(';')\n",
    "            tag['style'] = new_style\n",
    "            \n",
    "    for attr in [\"data-bbox\", \"data-polygon\"]:\n",
    "        for tag in soup.find_all(attrs={attr: True}):\n",
    "            del tag[attr]\n",
    "\n",
    "    classes_to_update = ['formula.machine_printed', 'formula.handwritten']\n",
    "    for tag in soup.find_all(class_=True):\n",
    "        if 'class' in tag.attrs:\n",
    "            new_classes = [cls if cls not in classes_to_update else 'formula' for cls in tag.get('class', [])]\n",
    "            tag['class'] = list(dict.fromkeys(new_classes))\n",
    "\n",
    "    for div in soup.find_all('div', class_='image caption'):\n",
    "        div.clear()\n",
    "        div['class'] = ['image']\n",
    "\n",
    "    classes_to_clean = ['music sheet', 'chemical formula', 'chart']\n",
    "    for class_name in classes_to_clean:\n",
    "        for tag in soup.find_all(class_=class_name):\n",
    "            tag.clear()\n",
    "            if 'format' in tag.attrs:\n",
    "                del tag['format']\n",
    "\n",
    "    output = []\n",
    "    for child in soup.body.children:\n",
    "        if hasattr(child, 'name') and child.name:\n",
    "            output.append(str(child))\n",
    "            output.append('\\n')\n",
    "        elif isinstance(child, str) and not child.strip():\n",
    "            continue\n",
    "    complete_html = f\"\"\"```html\\n<html><body>\\n{\"\".join(output)}</body></html>\\n```\"\"\"\n",
    "    return complete_html\n",
    "\n",
    "# Visualization Functions\n",
    "def draw_bbox(image_path, resized_width, resized_height, full_predict):\n",
    "    \"\"\"Draw bounding boxes for document parsing\"\"\"\n",
    "    if image_path.startswith('http'):\n",
    "        response = requests.get(image_path)\n",
    "        image = Image.open(io.BytesIO(response.content))\n",
    "    else:\n",
    "        image = Image.open(image_path)\n",
    "    \n",
    "    original_width = image.width\n",
    "    original_height = image.height\n",
    "    \n",
    "    soup = BeautifulSoup(full_predict, 'html.parser')\n",
    "    elements_with_bbox = soup.find_all(attrs={'data-bbox': True})\n",
    "\n",
    "    filtered_elements = []\n",
    "    for el in elements_with_bbox:\n",
    "        if el.name == 'ol':\n",
    "            continue\n",
    "        elif el.name == 'li' and el.parent.name == 'ol':\n",
    "            filtered_elements.append(el)\n",
    "        else:\n",
    "            filtered_elements.append(el)\n",
    "\n",
    "    font = get_font(20)\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    for element in filtered_elements:\n",
    "        bbox_str = element['data-bbox']\n",
    "        text = element.get_text(strip=True)\n",
    "        x1, y1, x2, y2 = map(int, bbox_str.split())\n",
    "        \n",
    "        scale_x = resized_width / original_width\n",
    "        scale_y = resized_height / original_height\n",
    "        \n",
    "        x1_resized = int(x1 / scale_x)\n",
    "        y1_resized = int(y1 / scale_y)\n",
    "        x2_resized = int(x2 / scale_x)\n",
    "        y2_resized = int(y2 / scale_y)\n",
    "        \n",
    "        if x1_resized > x2_resized:\n",
    "            x1_resized, x2_resized = x2_resized, x1_resized\n",
    "        if y1_resized > y2_resized:\n",
    "            y1_resized, y2_resized = y2_resized, y1_resized\n",
    "            \n",
    "        draw.rectangle([x1_resized, y1_resized, x2_resized, y2_resized], outline='red', width=2)\n",
    "        if font:\n",
    "            draw.text((x1_resized, y2_resized), text, fill='black', font=font)\n",
    "        else:\n",
    "            draw.text((x1_resized, y2_resized), text, fill='black')\n",
    "\n",
    "    display(image)\n",
    "\n",
    "def plot_bounding_boxes(im, bounding_boxes, input_width, input_height):\n",
    "    \"\"\"Plot bounding boxes for spatial understanding\"\"\"\n",
    "    img = im\n",
    "    width, height = img.size\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    colors = [\n",
    "        'red', 'green', 'blue', 'yellow', 'orange', 'pink', 'purple', 'brown', 'gray',\n",
    "        'beige', 'turquoise', 'cyan', 'magenta', 'lime', 'navy', 'maroon', 'teal',\n",
    "        'olive', 'coral', 'lavender', 'violet', 'gold', 'silver',\n",
    "    ] + additional_colors\n",
    "\n",
    "    bounding_boxes = parse_json(bounding_boxes)\n",
    "    font = get_font(14)\n",
    "\n",
    "    try:\n",
    "        json_output = ast.literal_eval(bounding_boxes)\n",
    "    except Exception as e:\n",
    "        end_idx = bounding_boxes.rfind('\"}') + len('\"}')\n",
    "        truncated_text = bounding_boxes[:end_idx] + \"]\"\n",
    "        json_output = ast.literal_eval(truncated_text)\n",
    "\n",
    "    for i, bounding_box in enumerate(json_output):\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        abs_y1 = int(bounding_box[\"bbox_2d\"][1]/input_height * height)\n",
    "        abs_x1 = int(bounding_box[\"bbox_2d\"][0]/input_width * width)\n",
    "        abs_y2 = int(bounding_box[\"bbox_2d\"][3]/input_height * height)\n",
    "        abs_x2 = int(bounding_box[\"bbox_2d\"][2]/input_width * width)\n",
    "\n",
    "        if abs_x1 > abs_x2:\n",
    "            abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "        if abs_y1 > abs_y2:\n",
    "            abs_y1, abs_y2 = abs_y2, abs_y1\n",
    "\n",
    "        draw.rectangle(((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=4)\n",
    "\n",
    "        if \"label\" in bounding_box:\n",
    "            if font:\n",
    "                draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color, font=font)\n",
    "            else:\n",
    "                draw.text((abs_x1 + 8, abs_y1 + 6), bounding_box[\"label\"], fill=color)\n",
    "\n",
    "    display(img)\n",
    "\n",
    "def plot_text_bounding_boxes(image_path, bounding_boxes, input_width, input_height):\n",
    "    \"\"\"Plot text bounding boxes for OCR\"\"\"\n",
    "    img = Image.open(image_path)\n",
    "    width, height = img.size\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    bounding_boxes = parse_json(bounding_boxes)\n",
    "    font = get_font(10)\n",
    "\n",
    "    for i, bounding_box in enumerate(ast.literal_eval(bounding_boxes)):\n",
    "        color = 'green'\n",
    "\n",
    "        abs_y1 = int(bounding_box[\"bbox_2d\"][1] / input_height * height)\n",
    "        abs_x1 = int(bounding_box[\"bbox_2d\"][0] / input_width * width)\n",
    "        abs_y2 = int(bounding_box[\"bbox_2d\"][3] / input_height * height)\n",
    "        abs_x2 = int(bounding_box[\"bbox_2d\"][2] / input_width * width)\n",
    "\n",
    "        if abs_x1 > abs_x2:\n",
    "            abs_x1, abs_x2 = abs_x2, abs_x1\n",
    "        if abs_y1 > abs_y2:\n",
    "            abs_y1, abs_y2 = abs_y2, abs_y1\n",
    "\n",
    "        draw.rectangle(((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=1)\n",
    "\n",
    "        if 'text_content' in bounding_box:\n",
    "            if font:\n",
    "                draw.text((abs_x1, abs_y2), bounding_box['text_content'], fill=color, font=font)\n",
    "            else:\n",
    "                draw.text((abs_x1, abs_y2), bounding_box['text_content'], fill=color)\n",
    "\n",
    "    display(img)\n",
    "\n",
    "# Main Unified Inference Class\n",
    "class UnifiedQwenInference:\n",
    "    def __init__(self, model, processor, device):\n",
    "        self.model = model\n",
    "        self.processor = processor\n",
    "        self.device = device\n",
    "        \n",
    "    def unified_inference(\n",
    "        self,\n",
    "        task_type: TaskType,\n",
    "        prompt: Optional[str] = None,\n",
    "        image_path: Optional[str] = None,\n",
    "        video_path: Optional[str] = None,\n",
    "        screenshot: Optional[str] = None,\n",
    "        user_query: Optional[str] = None,\n",
    "        max_new_tokens: int = 4096,\n",
    "        min_pixels: int = 512 * 28 * 28,\n",
    "        max_pixels: int = 2048 * 28 * 28,\n",
    "        total_pixels: int = 20480 * 28 * 28,\n",
    "        visualize: bool = True,\n",
    "        return_additional_info: bool = False\n",
    "    ) -> Union[str, Tuple[str, Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Unified inference function that handles all task types\n",
    "        \n",
    "        Args:\n",
    "            task_type: Type of task to perform\n",
    "            prompt: Text prompt for the task (optional, can use user_query instead)\n",
    "            image_path: Path to input image (for image tasks)\n",
    "            video_path: Path to input video (for video tasks)\n",
    "            screenshot: Path to screenshot (for agent tasks)\n",
    "            user_query: User query (for agent tasks, can be used instead of prompt)\n",
    "            max_new_tokens: Maximum tokens to generate\n",
    "            min_pixels: Minimum pixels for processing\n",
    "            max_pixels: Maximum pixels for processing\n",
    "            total_pixels: Total pixels for video processing\n",
    "            visualize: Whether to show visualizations\n",
    "            return_additional_info: Whether to return additional processing info\n",
    "            \n",
    "        Returns:\n",
    "            Model output text, optionally with additional processing information\n",
    "        \"\"\"\n",
    "        \n",
    "        # Handle prompt/user_query flexibility\n",
    "        if prompt is None and user_query is None:\n",
    "            raise ValueError(\"Either 'prompt' or 'user_query' must be provided\")\n",
    "        \n",
    "        # Use user_query if prompt is not provided\n",
    "        effective_prompt = prompt if prompt is not None else user_query\n",
    "        \n",
    "        # Determine system prompt\n",
    "        system_prompt = SYSTEM_PROMPTS.get(task_type, \"You are a helpful assistant\")\n",
    "        \n",
    "        # Handle different task types\n",
    "        if task_type in [TaskType.COMPUTER_AGENT, TaskType.MOBILE_AGENT]:\n",
    "            return self._handle_agent_task(task_type, user_query or effective_prompt, screenshot, visualize)\n",
    "        \n",
    "        elif task_type == TaskType.VIDEO_INFERENCE or task_type == TaskType.VIDEO_UNDERSTANDING:\n",
    "            return self._handle_video_task(task_type, effective_prompt, video_path, max_new_tokens, \n",
    "                                         min_pixels, total_pixels, visualize)\n",
    "        \n",
    "        else:\n",
    "            return self._handle_image_task(task_type, effective_prompt, image_path, system_prompt, \n",
    "                                         max_new_tokens, min_pixels, max_pixels, \n",
    "                                         visualize, return_additional_info)\n",
    "    \n",
    "    def _handle_agent_task(self, task_type: TaskType, user_query: str, screenshot: str, visualize: bool):\n",
    "        \"\"\"Handle computer and mobile agent tasks\"\"\"\n",
    "        dummy_image = Image.open(screenshot)\n",
    "        resized_height, resized_width = smart_resize(\n",
    "            dummy_image.height, dummy_image.width,\n",
    "            factor=self.processor.image_processor.patch_size * self.processor.image_processor.merge_size,\n",
    "            min_pixels=self.processor.image_processor.min_pixels,\n",
    "            max_pixels=self.processor.image_processor.max_pixels,\n",
    "        )\n",
    "\n",
    "        # Initialize appropriate agent\n",
    "        if task_type == TaskType.COMPUTER_AGENT:\n",
    "            agent_use = ComputerUse(\n",
    "                cfg={'display_width_px': resized_width, 'display_height_px': resized_height}\n",
    "            )\n",
    "        else:  # MOBILE_AGENT\n",
    "            # agent_use = MobileUse(\n",
    "            #     cfg={'display_width_px': resized_width, 'display_height_px': resized_height}\n",
    "            # )\n",
    "            # For now, use ComputerUse as fallback\n",
    "            agent_use = ComputerUse(\n",
    "                cfg={'display_width_px': resized_width, 'display_height_px': resized_height}\n",
    "            )\n",
    "\n",
    "        # Build messages\n",
    "        prompt_builder = NousFnCallPrompt()\n",
    "        message = prompt_builder.preprocess_fncall_messages(\n",
    "            messages=[\n",
    "                Message(role='system', content=[ContentItem(text='You are a helpful assistant.')]),\n",
    "                Message(role='user', content=[\n",
    "                    ContentItem(text=user_query),\n",
    "                    ContentItem(image=f\"file://{screenshot}\")\n",
    "                ]),\n",
    "            ],\n",
    "            functions=[agent_use.function],\n",
    "            lang=None,\n",
    "        )\n",
    "        message = [msg.model_dump() for msg in message]\n",
    "\n",
    "        text = self.processor.apply_chat_template(message, tokenize=False, add_generation_prompt=True)\n",
    "        inputs = self.processor(text=[text], images=[dummy_image], padding=True, return_tensors='pt').to(self.device)\n",
    "\n",
    "        streamer = TextStreamer(self.processor.tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "        \n",
    "        output_ids = self.model.generate(**inputs, max_new_tokens=2048, streamer=streamer)\n",
    "        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "        output_text = self.processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)[0]\n",
    "\n",
    "        # Process action and visualize\n",
    "        if visualize:\n",
    "            try:\n",
    "                action = json.loads(output_text.split('<tool_call>\\n')[1].split('\\n</tool_call>')[0])\n",
    "                display_image = dummy_image.resize((resized_width, resized_height))\n",
    "                \n",
    "                if 'click' in action['arguments']['action']:\n",
    "                    display_image = draw_point(dummy_image, action['arguments']['coordinate'], color='green')\n",
    "                display(display_image)\n",
    "            except:\n",
    "                display(dummy_image)\n",
    "\n",
    "        return output_text\n",
    "    \n",
    "    def _handle_video_task(self, task_type: TaskType, prompt: str, video_path: str, \n",
    "                          max_new_tokens: int, min_pixels: int, total_pixels: int, visualize: bool):\n",
    "        \"\"\"Handle video inference and understanding tasks\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': [\n",
    "                    {\n",
    "                        'type': 'video',\n",
    "                        'video': video_path,\n",
    "                        'min_pixels': min_pixels,\n",
    "                        'total_pixels': total_pixels,\n",
    "                    },\n",
    "                    {'type': 'text', 'text': prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        \n",
    "        if task_type == TaskType.VIDEO_UNDERSTANDING:\n",
    "            image_inputs, video_inputs, video_kwargs = process_vision_info([messages], return_video_kwargs=True)\n",
    "            fps_inputs = video_kwargs['fps']\n",
    "            inputs = self.processor(\n",
    "                text=[text], images=image_inputs, videos=video_inputs,\n",
    "                fps=fps_inputs, padding=True, return_tensors='pt',\n",
    "            )\n",
    "        else:\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            inputs = self.processor(\n",
    "                text=[text], images=image_inputs, videos=video_inputs,\n",
    "                padding=True, return_tensors='pt',\n",
    "            )\n",
    "        \n",
    "        inputs = inputs.to(self.device)\n",
    "        streamer = TextStreamer(self.processor.tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens, streamer=streamer)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        return output_text[0]\n",
    "    \n",
    "    def _handle_image_task(self, task_type: TaskType, prompt: str, image_path: str, \n",
    "                          system_prompt: str, max_new_tokens: int, min_pixels: int, \n",
    "                          max_pixels: int, visualize: bool, return_additional_info: bool):\n",
    "        \"\"\"Handle image-based tasks\"\"\"\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'user',\n",
    "                'content': [\n",
    "                    {\n",
    "                        'type': 'image',\n",
    "                        'image': image_path,\n",
    "                        'min_pixels': min_pixels,\n",
    "                        'max_pixels': max_pixels,\n",
    "                    },\n",
    "                    {'type': 'text', 'text': prompt},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = self.processor(\n",
    "            text=[text], images=image_inputs, videos=video_inputs,\n",
    "            padding=True, return_tensors='pt',\n",
    "        )\n",
    "        inputs = inputs.to(self.device)\n",
    "\n",
    "        streamer = TextStreamer(self.processor.tokenizer, skip_special_tokens=True, skip_prompt=True)\n",
    "\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=max_new_tokens, streamer=streamer)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        output_text = self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )\n",
    "\n",
    "        # Get input dimensions for visualization\n",
    "        input_height = inputs['image_grid_thw'][0][1] * 14\n",
    "        input_width = inputs['image_grid_thw'][0][2] * 14\n",
    "\n",
    "        # Handle visualization based on task type\n",
    "        if visualize:\n",
    "            image = Image.open(image_path)\n",
    "            \n",
    "            if task_type == TaskType.DOCUMENT_PARSING:\n",
    "                draw_bbox(image_path, input_width, input_height, output_text[0])\n",
    "                if return_additional_info:\n",
    "                    cleaned_html = clean_and_format_html(output_text[0])\n",
    "                    return output_text[0], {'cleaned_html': cleaned_html, 'input_height': input_height, 'input_width': input_width}\n",
    "                    \n",
    "            elif task_type == TaskType.SPATIAL_UNDERSTANDING:\n",
    "                image.thumbnail([640, 640], Image.Resampling.LANCZOS)\n",
    "                plot_bounding_boxes(image, output_text[0], input_width, input_height)\n",
    "                \n",
    "            elif task_type == TaskType.OCR and 'bbox_2d' in output_text[0]:\n",
    "                plot_text_bounding_boxes(image_path, output_text[0], input_width, input_height)\n",
    "                \n",
    "            elif task_type == TaskType.RECOGNITION:\n",
    "                display(image.resize((400, 600)) if image.size[0] > 400 else image)\n",
    "\n",
    "        if return_additional_info:\n",
    "            return output_text[0], {'input_height': input_height, 'input_width': input_width}\n",
    "        \n",
    "        return output_text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Initialize Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:09:17.654085Z",
     "iopub.status.busy": "2025-07-24T08:09:17.653769Z",
     "iopub.status.idle": "2025-07-24T08:09:22.915877Z",
     "shell.execute_reply": "2025-07-24T08:09:22.915137Z",
     "shell.execute_reply.started": "2025-07-24T08:09:17.654056Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "unified_system = UnifiedQwenInference(model, processor, device)\n",
    "\n",
    "print(\"âœ… Unified Qwen2.5-VL Multi-Modal Inference System initialized successfully!\")\n",
    "print(\"\\nAvailable task types:\")\n",
    "for task in TaskType:\n",
    "    print(f\"  - {task.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 04 Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:09:22.917145Z",
     "iopub.status.busy": "2025-07-24T08:09:22.916824Z",
     "iopub.status.idle": "2025-07-24T08:09:43.134266Z",
     "shell.execute_reply": "2025-07-24T08:09:43.133190Z",
     "shell.execute_reply.started": "2025-07-24T08:09:22.917118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example: Computer Agent Task\"\"\"\n",
    "screenshot = './cookbooks/assets/computer_use/computer_use2.jpeg'\n",
    "\n",
    "user_query = 'open the third issue'\n",
    "\n",
    "result = unified_system.unified_inference(\n",
    "    task_type=TaskType.COMPUTER_AGENT,\n",
    "    user_query=user_query,\n",
    "    screenshot=screenshot,\n",
    "    visualize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T08:09:43.137949Z",
     "iopub.status.busy": "2025-07-24T08:09:43.137411Z",
     "iopub.status.idle": "2025-07-24T08:17:09.761262Z",
     "shell.execute_reply": "2025-07-24T08:17:09.760218Z",
     "shell.execute_reply.started": "2025-07-24T08:09:43.137916Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example: Document Parsing Task\"\"\"\n",
    "image_path = './cookbooks/assets/document_parsing/docparsing_example1.jpg'\n",
    "prompt = 'QwenVL HTML'\n",
    "\n",
    "result, additional_info = unified_system.unified_inference(\n",
    "    task_type=TaskType.DOCUMENT_PARSING,\n",
    "    prompt=prompt,\n",
    "    image_path=image_path,\n",
    "    visualize=True,\n",
    "    return_additional_info=True\n",
    ")\n",
    "\n",
    "print(\"Cleaned HTML:\")\n",
    "print(additional_info['cleaned_html'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-24T08:17:09.762024Z",
     "iopub.status.idle": "2025-07-24T08:17:09.762345Z",
     "shell.execute_reply": "2025-07-24T08:17:09.762205Z",
     "shell.execute_reply.started": "2025-07-24T08:17:09.762191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example: OCR Task\"\"\"\n",
    "image_path = './cookbooks/assets/ocr/ocr_example2.jpg'\n",
    "prompt = 'Read all the text in the image.'\n",
    "\n",
    "img = mpimg.imread(image_path)\n",
    "plt.imshow(img)\n",
    "plt.axis('off')  # Hide axis\n",
    "plt.show()\n",
    "\n",
    "result = unified_system.unified_inference(\n",
    "    task_type=TaskType.OCR,\n",
    "    prompt=prompt,\n",
    "    image_path=image_path,\n",
    "    visualize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-24T08:17:09.763327Z",
     "iopub.status.idle": "2025-07-24T08:17:09.763626Z",
     "shell.execute_reply": "2025-07-24T08:17:09.763498Z",
     "shell.execute_reply.started": "2025-07-24T08:17:09.763485Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example: Recognition Task\"\"\"\n",
    "image_path = './cookbooks/assets/universal_recognition/unireco_birds_example.jpg'\n",
    "prompt = 'What is the main subject of this image? Describe it in detail.'\n",
    "\n",
    "result = unified_system.unified_inference(\n",
    "    task_type=TaskType.RECOGNITION,\n",
    "    prompt=prompt,\n",
    "    image_path=image_path,\n",
    "    visualize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-24T08:17:09.764800Z",
     "iopub.status.idle": "2025-07-24T08:17:09.765107Z",
     "shell.execute_reply": "2025-07-24T08:17:09.764951Z",
     "shell.execute_reply.started": "2025-07-24T08:17:09.764932Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example: Spatial Understanding Task\"\"\"\n",
    "image_path = './cookbooks/assets/spatial_understanding/cakes.png'\n",
    "prompt = 'Outline the position of each small cake and output all the coordinates in JSON format.'\n",
    "\n",
    "result = unified_system.unified_inference(\n",
    "    task_type=TaskType.SPATIAL_UNDERSTANDING,\n",
    "    prompt=prompt,\n",
    "    image_path=image_path,\n",
    "    visualize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-24T08:17:09.766170Z",
     "iopub.status.idle": "2025-07-24T08:17:09.766545Z",
     "shell.execute_reply": "2025-07-24T08:17:09.766367Z",
     "shell.execute_reply.started": "2025-07-24T08:17:09.766349Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example: Video Inference Task\"\"\"\n",
    "video_path = 'https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4'\n",
    "prompt = 'What is the main subject of this video? Describe it in detail.'\n",
    "\n",
    "result = unified_system.unified_inference(\n",
    "    task_type=TaskType.VIDEO_INFERENCE,\n",
    "    prompt=prompt,\n",
    "    video_path=video_path,\n",
    "    max_new_tokens=128\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-07-24T08:17:09.767940Z",
     "iopub.status.idle": "2025-07-24T08:17:09.768352Z",
     "shell.execute_reply": "2025-07-24T08:17:09.768175Z",
     "shell.execute_reply.started": "2025-07-24T08:17:09.768158Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"Example: Video Understanding Task\"\"\"\n",
    "video_path = 'https://duguang-labelling.oss-cn-shanghai.aliyuncs.com/qiansun/video_ocr/videos/50221078283.mp4'\n",
    "prompt = 'Analyze this video in detail and describe what happens.'\n",
    "\n",
    "result = unified_system.unified_inference(\n",
    "    task_type=TaskType.VIDEO_UNDERSTANDING,\n",
    "    prompt=prompt,\n",
    "    video_path=video_path,\n",
    "    max_new_tokens=2048\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
