{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    ").to(device) #''\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "# System prompts for different usage modes\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"General Assistant\": \"You are a helpful assistant that can analyze images and answer questions.\",\n",
    "    \"Image Analyzer\": \"You are an expert image analyst. Provide detailed descriptions and analysis of images.\",\n",
    "    \"OCR Reader\": \"You are an OCR specialist. Extract and transcribe all text from images accurately.\",\n",
    "    \"Medical Assistant\": \"You are a medical AI assistant. Analyze medical images and provide insights (for educational purposes only).\",\n",
    "    \"Educational Tutor\": \"You are an educational tutor. Help explain concepts shown in images and answer related questions.\"\n",
    "}\n",
    "\n",
    "def inference(\n",
    "    prompt,\n",
    "    image_path,\n",
    "    system_prompt = 'You are a helpful assistant',\n",
    "    max_new_tokens = 32000,\n",
    "    min_pixels = 512 * 28 * 28,\n",
    "    max_pixels = 2048 * 28 * 28\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : 'user',\n",
    "            'content' : [\n",
    "                {\n",
    "                    'type' : 'image',\n",
    "                    'image' : image_path,\n",
    "                    'min_pixels' : min_pixels,\n",
    "                    'max_pixels' : max_pixels,\n",
    "                },\n",
    "                {'type' : 'text', 'text' : prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize = False, add_generation_prompt = True\n",
    "    )\n",
    "    print('input:\\n', text)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    streamer = TextStreamer(processor.tokenizer, skip_special_tokens = True, skip_prompt = True)\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens, streamer = streamer)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n",
    "    )\n",
    "\n",
    "    print('output:\\n', output_text[0])\n",
    "\n",
    "    input_height = inputs['image_grid_thw'][0][1] * 14\n",
    "    input_width = inputs['image_grid_thw'][0][2] * 14\n",
    "\n",
    "    return output_text[0], input_height, input_width\n",
    "\n",
    "def chat_with_history(\n",
    "    message: str,\n",
    "    image: Optional[str],\n",
    "    history: List[Tuple[str, str]],\n",
    "    usage_mode: str,\n",
    "    custom_system_prompt: str,\n",
    "    max_tokens: int,\n",
    "    min_pixels: int,\n",
    "    max_pixels: int\n",
    ") -> Tuple[List[Tuple[str, str]], str]:\n",
    "    \"\"\"\n",
    "    Handle chat with conversation history\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine system prompt\n",
    "        if custom_system_prompt.strip():\n",
    "            system_prompt = custom_system_prompt.strip()\n",
    "        else:\n",
    "            system_prompt = SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "        \n",
    "        # If no message provided, return current history\n",
    "        if not message.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Call inference function\n",
    "        if image:\n",
    "            response, img_height, img_width = inference(\n",
    "                prompt=message,\n",
    "                image_path=image,\n",
    "                system_prompt=system_prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                min_pixels=min_pixels,\n",
    "                max_pixels=max_pixels\n",
    "            )\n",
    "        else:\n",
    "            # Handle text-only messages (you might need to modify inference function for this)\n",
    "            response = \"Image is required for this model.\"\n",
    "        \n",
    "        # Add to history\n",
    "        history.append((message, response))\n",
    "        \n",
    "        return history, \"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        history.append((message, error_msg))\n",
    "        return history, \"\"\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clear chat history\"\"\"\n",
    "    return [], \"\"\n",
    "\n",
    "def update_system_prompt(usage_mode: str) -> str:\n",
    "    \"\"\"Update system prompt based on usage mode\"\"\"\n",
    "    return SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Qwen2.5 VL Chatbot\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# Qwen2.5 VL Chatbot with Conversation History\")\n",
    "    gr.Markdown(\"Upload an image and chat with the AI assistant. The conversation history is maintained throughout the session.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # Configuration Panel\n",
    "            gr.Markdown(\"### Configuration\")\n",
    "            \n",
    "            usage_mode = gr.Dropdown(\n",
    "                choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                value=\"General Assistant\",\n",
    "                label=\"Usage Mode\",\n",
    "                info=\"Select the AI's behavior mode\"\n",
    "            )\n",
    "            \n",
    "            custom_system_prompt = gr.Textbox(\n",
    "                label=\"Custom System Prompt (Optional)\",\n",
    "                placeholder=\"Enter custom system prompt to override the selected mode...\",\n",
    "                lines=3\n",
    "            )\n",
    "            \n",
    "            with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "                max_tokens = gr.Slider(\n",
    "                    minimum=100,\n",
    "                    maximum=32000,\n",
    "                    value=2000,\n",
    "                    step=100,\n",
    "                    label=\"Max New Tokens\"\n",
    "                )\n",
    "                \n",
    "                min_pixels = gr.Number(\n",
    "                    value=512 * 28 * 28,\n",
    "                    label=\"Min Pixels\",\n",
    "                    info=\"Minimum image resolution\"\n",
    "                )\n",
    "                \n",
    "                max_pixels = gr.Number(\n",
    "                    value=2048 * 28 * 28,\n",
    "                    label=\"Max Pixels\",\n",
    "                    info=\"Maximum image resolution\"\n",
    "                )\n",
    "            \n",
    "            clear_btn = gr.Button(\"Clear History\", variant=\"secondary\")\n",
    "            \n",
    "        with gr.Column(scale=2):\n",
    "            # Chat Interface\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Conversation\",\n",
    "                height=400,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                image_input = gr.Image(\n",
    "                    type=\"filepath\",\n",
    "                    label=\"Upload Image\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg_input = gr.Textbox(\n",
    "                    label=\"Message\",\n",
    "                    placeholder=\"Type your message here...\",\n",
    "                    scale=4\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "    \n",
    "    # Auto-update system prompt when usage mode changes\n",
    "    usage_mode.change(\n",
    "        fn = update_system_prompt,\n",
    "        inputs = [usage_mode],\n",
    "        outputs = [custom_system_prompt]\n",
    "    )\n",
    "    \n",
    "    # Handle send button click\n",
    "    send_btn.click(\n",
    "        fn = chat_with_history,\n",
    "        inputs = [\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs=[chatbot, msg_input]\n",
    "    )\n",
    "    \n",
    "    # Handle enter key in message input\n",
    "    msg_input.submit(\n",
    "        fn = chat_with_history,\n",
    "        inputs = [\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs = [chatbot, msg_input]\n",
    "    )\n",
    "    \n",
    "    # Handle clear history button\n",
    "    clear_btn.click(\n",
    "        fn = clear_history,\n",
    "        outputs = [chatbot, msg_input]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        #server_name = \"0.0.0.0\",\n",
    "        #server_port = 7860,\n",
    "        #share = False,\n",
    "        debug = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    ").to(device) #''\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Custom CSS for ChatGPT-like styling\n",
    "css = \"\"\"\n",
    ".gradio-container {\n",
    "    max-width: 1200px !important;\n",
    "    margin: 0 auto !important;\n",
    "}\n",
    "\n",
    ".message-wrap.svelte-1lcyrx4 {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    ".message.user.svelte-1lcyrx4 {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    ".message.bot.svelte-1lcyrx4 {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    ".chatbot {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    ".textbox_container {\n",
    "    border-radius: 8px !important;\n",
    "}\n",
    "\n",
    ".settings-panel {\n",
    "    background: #f8f9fa;\n",
    "    padding: 15px;\n",
    "    border-radius: 8px;\n",
    "    margin-bottom: 15px;\n",
    "}\n",
    "\n",
    ".chat-container {\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "    height: 80vh;\n",
    "}\n",
    "\n",
    ".input-area {\n",
    "    border-top: 1px solid #e0e0e0;\n",
    "    padding: 10px 0;\n",
    "    background: white;\n",
    "}\n",
    "\n",
    ".attachment-area {\n",
    "    padding: 5px 0;\n",
    "    border-bottom: 1px solid #f0f0f0;\n",
    "    margin-bottom: 10px;\n",
    "}\n",
    "\n",
    "#chatbot .message {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    "body, .gradio-container, .gradio-container * {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    ".gr-button {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    ".gr-textbox {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    ".gr-dropdown {\n",
    "    font-size: 10px !important;\n",
    "}\n",
    "\n",
    "h1, h2, h3 {\n",
    "    font-size: 18px !important;\n",
    "}\n",
    "\n",
    ".gr-markdown h1 {\n",
    "    font-size: 20px !important;\n",
    "}\n",
    "\n",
    ".gr-markdown h2 {\n",
    "    font-size: 18px !important;\n",
    "}\n",
    "\n",
    ".gr-markdown h3 {\n",
    "    font-size: 16px !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# System prompts for different usage modes\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"General Assistant\": \"You are a helpful assistant that can analyze images and answer questions.\",\n",
    "    \"Image Analyzer\": \"You are an expert image analyst. Provide detailed descriptions and analysis of images.\",\n",
    "    \"OCR Reader\": \"You are an OCR specialist. Extract and transcribe all text from images accurately.\",\n",
    "    \"Medical Assistant\": \"You are a medical AI assistant. Analyze medical images and provide insights (for educational purposes only).\",\n",
    "    \"Educational Tutor\": \"You are an educational tutor. Help explain concepts shown in images and answer related questions.\"\n",
    "}\n",
    "\n",
    "def inference(\n",
    "    prompt,\n",
    "    image_path,\n",
    "    system_prompt = 'You are a helpful assistant',\n",
    "    max_new_tokens = 32000,\n",
    "    min_pixels = 512 * 28 * 28,\n",
    "    max_pixels = 2048 * 28 * 28\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : 'user',\n",
    "            'content' : [\n",
    "                {\n",
    "                    'type' : 'image',\n",
    "                    'image' : image_path,\n",
    "                    'min_pixels' : min_pixels,\n",
    "                    'max_pixels' : max_pixels,\n",
    "                },\n",
    "                {'type' : 'text', 'text' : prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize = False, add_generation_prompt = True\n",
    "    )\n",
    "    print('input:\\n', text)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    streamer = TextStreamer(processor.tokenizer, skip_special_tokens = True, skip_prompt = True)\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens, streamer = streamer)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n",
    "    )\n",
    "\n",
    "    print('output:\\n', output_text[0])\n",
    "\n",
    "    input_height = inputs['image_grid_thw'][0][1] * 14\n",
    "    input_width = inputs['image_grid_thw'][0][2] * 14\n",
    "\n",
    "    return output_text[0], input_height, input_width\n",
    "\n",
    "def chat_with_history(\n",
    "    message: str,\n",
    "    image: Optional[str],\n",
    "    history: List[Tuple[str, str]],\n",
    "    usage_mode: str,\n",
    "    custom_system_prompt: str,\n",
    "    max_tokens: int,\n",
    "    min_pixels: int,\n",
    "    max_pixels: int\n",
    ") -> Tuple[List[Tuple[str, str]], str]:\n",
    "    \"\"\"\n",
    "    Handle chat with conversation history\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Determine system prompt\n",
    "        if custom_system_prompt.strip():\n",
    "            system_prompt = custom_system_prompt.strip()\n",
    "        else:\n",
    "            system_prompt = SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "        \n",
    "        # If no message provided, return current history\n",
    "        if not message.strip():\n",
    "            return history, \"\"\n",
    "        \n",
    "        # Call inference function\n",
    "        if image:\n",
    "            response, img_height, img_width = inference(\n",
    "                prompt=message,\n",
    "                image_path=image,\n",
    "                system_prompt=system_prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                min_pixels=min_pixels,\n",
    "                max_pixels=max_pixels\n",
    "            )\n",
    "        else:\n",
    "            # Handle text-only messages (you might need to modify inference function for this)\n",
    "            response = \"Image is required for this model.\"\n",
    "        \n",
    "        # Add to history\n",
    "        history.append((message, response))\n",
    "        \n",
    "        return history, \"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        history.append((message, error_msg))\n",
    "        return history, \"\"\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clear chat history\"\"\"\n",
    "    return [], \"\"\n",
    "\n",
    "def update_system_prompt(usage_mode: str) -> str:\n",
    "    \"\"\"Update system prompt based on usage mode\"\"\"\n",
    "    return SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "\n",
    "# Create Gradio interface\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(title=\"Qwen2.5 VL Chatbot\", theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\"# Qwen2.5 VL Chatbot with Conversation History\")\n",
    "    gr.Markdown(\"Upload an image and chat with the AI assistant. The conversation history is maintained throughout the session.\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            # Configuration Panel\n",
    "            gr.Markdown(\"### Configuration\")\n",
    "            \n",
    "            usage_mode = gr.Dropdown(\n",
    "                choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                value=\"General Assistant\",\n",
    "                label=\"Usage Mode\",\n",
    "                info=\"Select the AI's behavior mode\"\n",
    "            )\n",
    "            \n",
    "            custom_system_prompt = gr.Textbox(\n",
    "                label=\"Custom System Prompt (Optional)\",\n",
    "                placeholder=\"Enter custom system prompt to override the selected mode...\",\n",
    "                lines=3\n",
    "            )\n",
    "            \n",
    "            with gr.Accordion(\"Advanced Settings\", open=False):\n",
    "                max_tokens = gr.Slider(\n",
    "                    minimum=100,\n",
    "                    maximum=32000,\n",
    "                    value=2000,\n",
    "                    step=100,\n",
    "                    label=\"Max New Tokens\"\n",
    "                )\n",
    "                \n",
    "                min_pixels = gr.Number(\n",
    "                    value=512 * 28 * 28,\n",
    "                    label=\"Min Pixels\",\n",
    "                    info=\"Minimum image resolution\"\n",
    "                )\n",
    "                \n",
    "                max_pixels = gr.Number(\n",
    "                    value=2048 * 28 * 28,\n",
    "                    label=\"Max Pixels\",\n",
    "                    info=\"Maximum image resolution\"\n",
    "                )\n",
    "            \n",
    "            clear_btn = gr.Button(\"Clear History\", variant=\"secondary\")\n",
    "            \n",
    "        with gr.Column(scale=2):\n",
    "            # Chat Interface\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Conversation\",\n",
    "                height=400,\n",
    "                show_copy_button=True\n",
    "            )\n",
    "            \n",
    "            with gr.Row():\n",
    "                image_input = gr.Image(\n",
    "                    type=\"filepath\",\n",
    "                    label=\"Upload Image\"\n",
    "                )\n",
    "            \n",
    "            with gr.Row():\n",
    "                msg_input = gr.Textbox(\n",
    "                    label=\"Message\",\n",
    "                    placeholder=\"Type your message here...\",\n",
    "                    scale=4\n",
    "                )\n",
    "                send_btn = gr.Button(\"Send\", variant=\"primary\", scale=1)\n",
    "    \n",
    "    # Auto-update system prompt when usage mode changes\n",
    "    usage_mode.change(\n",
    "        fn = update_system_prompt,\n",
    "        inputs = [usage_mode],\n",
    "        outputs = [custom_system_prompt]\n",
    "    )\n",
    "    \n",
    "    # Handle send button click\n",
    "    send_btn.click(\n",
    "        fn = chat_with_history,\n",
    "        inputs = [\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs=[chatbot, msg_input]\n",
    "    )\n",
    "    \n",
    "    # Handle enter key in message input\n",
    "    msg_input.submit(\n",
    "        fn = chat_with_history,\n",
    "        inputs = [\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs = [chatbot, msg_input]\n",
    "    )\n",
    "    \n",
    "    # Handle clear history button\n",
    "    clear_btn.click(\n",
    "        fn = clear_history,\n",
    "        outputs = [chatbot, msg_input]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        #server_name = \"0.0.0.0\",\n",
    "        #server_port = 7860,\n",
    "        #share = False,\n",
    "        debug = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    ").to(device) #''\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Custom CSS for ChatGPT-like interface\n",
    "custom_css = \"\"\"\n",
    "/* Main container */\n",
    ".gradio-container {\n",
    "    font-size: 14px !important;\n",
    "    max-width: 100% !important;\n",
    "}\n",
    "\n",
    "/* Sidebar styling */\n",
    ".sidebar {\n",
    "    background-color: #f7f7f8;\n",
    "    border-right: 1px solid #e5e5e5;\n",
    "    padding: 16px;\n",
    "    height: 100vh;\n",
    "    overflow-y: auto;\n",
    "}\n",
    "\n",
    "/* Chat container */\n",
    ".chat-container {\n",
    "    height: calc(100vh - 100px);\n",
    "    display: flex;\n",
    "    flex-direction: column;\n",
    "}\n",
    "\n",
    "/* Chatbot styling */\n",
    ".chatbot {\n",
    "    flex-grow: 1;\n",
    "    border: none !important;\n",
    "    font-size: 14px !important;\n",
    "}\n",
    "\n",
    "/* Message input area */\n",
    ".message-input-container {\n",
    "    padding: 16px;\n",
    "    border-top: 1px solid #e5e5e5;\n",
    "    background-color: white;\n",
    "}\n",
    "\n",
    "/* Input styling */\n",
    ".message-input {\n",
    "    border-radius: 24px !important;\n",
    "    border: 2px solid #e5e5e5 !important;\n",
    "    padding: 12px 16px !important;\n",
    "    font-size: 14px !important;\n",
    "}\n",
    "\n",
    "/* Button styling */\n",
    ".send-button {\n",
    "    border-radius: 20px !important;\n",
    "    background: #10a37f !important;\n",
    "    border: none !important;\n",
    "    padding: 8px 16px !important;\n",
    "    font-size: 14px !important;\n",
    "}\n",
    "\n",
    "/* Sidebar toggle button */\n",
    ".sidebar-toggle {\n",
    "    position: fixed;\n",
    "    top: 16px;\n",
    "    left: 16px;\n",
    "    z-index: 1000;\n",
    "    background: #f7f7f8 !important;\n",
    "    border: 1px solid #e5e5e5 !important;\n",
    "    border-radius: 6px !important;\n",
    "    padding: 8px !important;\n",
    "}\n",
    "\n",
    "/* Hide sidebar when collapsed */\n",
    ".sidebar-hidden {\n",
    "    display: none !important;\n",
    "}\n",
    "\n",
    "/* Adjust main content when sidebar is hidden */\n",
    ".main-content-expanded {\n",
    "    margin-left: 0 !important;\n",
    "}\n",
    "\n",
    "/* Image attachment styling */\n",
    ".image-attachment {\n",
    "    max-width: 150px;\n",
    "    max-height: 150px;\n",
    "    border-radius: 8px;\n",
    "    margin: 5px 0;\n",
    "    cursor: pointer;\n",
    "}\n",
    "\n",
    "/* Smaller text for all components */\n",
    "* {\n",
    "    font-size: 14px !important;\n",
    "}\n",
    "\n",
    "/* File upload area */\n",
    ".file-upload {\n",
    "    border: 2px dashed #e5e5e5 !important;\n",
    "    border-radius: 12px !important;\n",
    "    padding: 20px !important;\n",
    "    text-align: center !important;\n",
    "    background-color: #fafafa !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# System prompts for different usage modes\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"General Assistant\": \"You are a helpful assistant that can analyze images and answer questions.\",\n",
    "    \"Image Analyzer\": \"You are an expert image analyst. Provide detailed descriptions and analysis of images.\",\n",
    "    \"OCR Reader\": \"You are an OCR specialist. Extract and transcribe all text from images accurately.\",\n",
    "    \"Medical Assistant\": \"You are a medical AI assistant. Analyze medical images and provide insights (for educational purposes only).\",\n",
    "    \"Educational Tutor\": \"You are an educational tutor. Help explain concepts shown in images and answer related questions.\"\n",
    "}\n",
    "\n",
    "def inference(\n",
    "    prompt,\n",
    "    image_path,\n",
    "    system_prompt = 'You are a helpful assistant',\n",
    "    max_new_tokens = 32000,\n",
    "    min_pixels = 512 * 28 * 28,\n",
    "    max_pixels = 2048 * 28 * 28\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : 'user',\n",
    "            'content' : [\n",
    "                {\n",
    "                    'type' : 'image',\n",
    "                    'image' : image_path,\n",
    "                    'min_pixels' : min_pixels,\n",
    "                    'max_pixels' : max_pixels,\n",
    "                },\n",
    "                {'type' : 'text', 'text' : prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize = False, add_generation_prompt = True\n",
    "    )\n",
    "    print('input:\\n', text)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    streamer = TextStreamer(processor.tokenizer, skip_special_tokens = True, skip_prompt = True)\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens, streamer = streamer)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n",
    "    )\n",
    "\n",
    "    print('output:\\n', output_text[0])\n",
    "\n",
    "    input_height = inputs['image_grid_thw'][0][1] * 14\n",
    "    input_width = inputs['image_grid_thw'][0][2] * 14\n",
    "\n",
    "    return output_text[0], input_height, input_width\n",
    "\n",
    "def stream_response(text: str):\n",
    "    \"\"\"Simulate streaming response\"\"\"\n",
    "    words = text.split()\n",
    "    current_text = \"\"\n",
    "    for word in words:\n",
    "        current_text += word + \" \"\n",
    "        yield current_text.strip()\n",
    "        time.sleep(0.05)  # Adjust speed as needed\n",
    "\n",
    "def chat_with_history(\n",
    "    message: str,\n",
    "    image: Optional[str],\n",
    "    history: List[Tuple[str, str]],\n",
    "    usage_mode: str,\n",
    "    custom_system_prompt: str,\n",
    "    max_tokens: int,\n",
    "    min_pixels: int,\n",
    "    max_pixels: int\n",
    "):\n",
    "    \"\"\"Handle chat with conversation history and streaming\"\"\"\n",
    "    try:\n",
    "        # Determine system prompt\n",
    "        if custom_system_prompt.strip():\n",
    "            system_prompt = custom_system_prompt.strip()\n",
    "        else:\n",
    "            system_prompt = SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "        \n",
    "        # If no message provided, return current history\n",
    "        if not message.strip() and not image:\n",
    "            return history, \"\", None\n",
    "        \n",
    "        # Prepare user message with image if provided\n",
    "        user_message = message if message.strip() else \"Please analyze this image.\"\n",
    "        if image:\n",
    "            user_message_display = f'<img src=\"{image}\" style=\"max-width: 150px; max-height: 150px; margin: 5px 0; border-radius: 8px; cursor: pointer;\" onclick=\"window.open(this.src)\"> <br>{user_message}'\n",
    "        else:\n",
    "            user_message_display = user_message\n",
    "        \n",
    "        # Add user message to history\n",
    "        history.append((user_message_display, \"\"))\n",
    "        yield history, \"\", None\n",
    "        \n",
    "        # Call inference function\n",
    "        if image:\n",
    "            response, img_height, img_width = inference(\n",
    "                prompt=user_message,\n",
    "                image_path=image,\n",
    "                system_prompt=system_prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                min_pixels=min_pixels,\n",
    "                max_pixels=max_pixels\n",
    "            )\n",
    "        else:\n",
    "            response = \"Please upload an image to continue the conversation.\"\n",
    "        \n",
    "        # Stream the response\n",
    "        history[-1] = (user_message_display, \"\")\n",
    "        for partial_response in stream_response(response):\n",
    "            history[-1] = (user_message_display, partial_response)\n",
    "            yield history, \"\", None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error: {str(e)}\"\n",
    "        if history and history[-1][1] == \"\":\n",
    "            history[-1] = (history[-1][0], error_msg)\n",
    "        else:\n",
    "            history.append((message, error_msg))\n",
    "        yield history, \"\", None\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clear chat history\"\"\"\n",
    "    return [], \"\", None\n",
    "\n",
    "def update_system_prompt(usage_mode: str) -> str:\n",
    "    \"\"\"Update system prompt based on usage mode\"\"\"\n",
    "    return SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "\n",
    "def toggle_sidebar():\n",
    "    \"\"\"Toggle sidebar visibility\"\"\"\n",
    "    return gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "def show_sidebar():\n",
    "    \"\"\"Show sidebar\"\"\"\n",
    "    return gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(css=custom_css, title=\"Qwen2.5 VL Chat\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    # State for sidebar visibility\n",
    "    sidebar_visible = gr.State(True)\n",
    "    \n",
    "    with gr.Row(elem_classes=\"main-container\"):\n",
    "        # Sidebar\n",
    "        with gr.Column(scale=1, elem_classes=\"sidebar\", visible=True) as sidebar:\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Settings\", elem_classes=\"sidebar-title\")\n",
    "            \n",
    "            # Hide sidebar button\n",
    "            hide_sidebar_btn = gr.Button(\"‚Üê Hide\", size=\"sm\", elem_classes=\"hide-sidebar-btn\")\n",
    "            \n",
    "            usage_mode = gr.Dropdown(\n",
    "                choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                value=\"General Assistant\",\n",
    "                label=\"Usage Mode\",\n",
    "                info=\"Select AI behavior\",\n",
    "                elem_classes=\"dropdown-small\"\n",
    "            )\n",
    "            \n",
    "            custom_system_prompt = gr.Textbox(\n",
    "                label=\"Custom System Prompt\",\n",
    "                placeholder=\"Override with custom prompt...\",\n",
    "                lines=3,\n",
    "                elem_classes=\"textbox-small\"\n",
    "            )\n",
    "            \n",
    "            with gr.Accordion(\"Advanced\", open=False):\n",
    "                max_tokens = gr.Slider(\n",
    "                    minimum=100,\n",
    "                    maximum=32000,\n",
    "                    value=2000,\n",
    "                    step=100,\n",
    "                    label=\"Max Tokens\",\n",
    "                    elem_classes=\"slider-small\"\n",
    "                )\n",
    "                \n",
    "                min_pixels = gr.Number(\n",
    "                    value=512 * 28 * 28,\n",
    "                    label=\"Min Pixels\",\n",
    "                    elem_classes=\"number-small\"\n",
    "                )\n",
    "                \n",
    "                max_pixels = gr.Number(\n",
    "                    value=2048 * 28 * 28,\n",
    "                    label=\"Max Pixels\",\n",
    "                    elem_classes=\"number-small\"\n",
    "                )\n",
    "            \n",
    "            clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", variant=\"secondary\", elem_classes=\"clear-btn\")\n",
    "        \n",
    "        # Show sidebar button (hidden by default)\n",
    "        show_sidebar_btn = gr.Button(\"‚ò∞\", elem_classes=\"sidebar-toggle\", visible=False)\n",
    "        \n",
    "        # Main chat area\n",
    "        with gr.Column(scale=4, elem_classes=\"chat-container\"):\n",
    "            # Header\n",
    "            gr.Markdown(\"# Qwen2.5 VL Chat\", elem_classes=\"chat-header\")\n",
    "            \n",
    "            # Chatbot\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"\",\n",
    "                height=600,\n",
    "                show_copy_button=True,\n",
    "                elem_classes=\"chatbot\",\n",
    "                avatar_images=None,\n",
    "                bubble_full_width=False\n",
    "            )\n",
    "            \n",
    "            # Message input area\n",
    "            with gr.Row(elem_classes=\"message-input-container\"):\n",
    "                with gr.Column(scale=1):\n",
    "                    # Image attachment (hidden by default)\n",
    "                    image_input = gr.File(\n",
    "                        file_types=[\"image\"],\n",
    "                        label=\"üìé\",\n",
    "                        elem_classes=\"file-upload\",\n",
    "                        visible=True,\n",
    "                        height=60\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=8):\n",
    "                    msg_input = gr.Textbox(\n",
    "                        placeholder=\"Message Qwen2.5 VL...\",\n",
    "                        lines=1,\n",
    "                        max_lines=4,\n",
    "                        elem_classes=\"message-input\",\n",
    "                        show_label=False,\n",
    "                        container=False\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=1):\n",
    "                    send_btn = gr.Button(\"Send\", variant=\"primary\", elem_classes=\"send-button\")\n",
    "    \n",
    "    # Event handlers\n",
    "    \n",
    "    # Auto-update system prompt when usage mode changes\n",
    "    usage_mode.change(\n",
    "        fn=update_system_prompt,\n",
    "        inputs=[usage_mode],\n",
    "        outputs=[custom_system_prompt]\n",
    "    )\n",
    "    \n",
    "    # Sidebar toggle functions\n",
    "    hide_sidebar_btn.click(\n",
    "        fn=toggle_sidebar,\n",
    "        outputs=[sidebar, show_sidebar_btn]\n",
    "    )\n",
    "    \n",
    "    show_sidebar_btn.click(\n",
    "        fn=show_sidebar,\n",
    "        outputs=[sidebar, show_sidebar_btn]\n",
    "    )\n",
    "    \n",
    "    # Chat functions with streaming\n",
    "    send_btn.click(\n",
    "        fn=chat_with_history,\n",
    "        inputs=[\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "    \n",
    "    msg_input.submit(\n",
    "        fn=chat_with_history,\n",
    "        inputs=[\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=clear_history,\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        #server_name=\"0.0.0.0\",\n",
    "        #server_port=7860,\n",
    "        #share=False,\n",
    "        debug=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "import base64\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    ").to(device) #''\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Custom CSS with dark purple Qwen-inspired theme\n",
    "custom_css = \"\"\"\n",
    "/* Import Google Fonts */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap');\n",
    "\n",
    "/* Global styles */\n",
    "* {\n",
    "    font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif !important;\n",
    "    font-size: 13px !important;\n",
    "    line-height: 1.4 !important;\n",
    "}\n",
    "\n",
    "/* Main container */\n",
    ".gradio-container {\n",
    "    background-color: #0f0f23 !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    max-width: 100% !important;\n",
    "}\n",
    "\n",
    "/* Sidebar styling */\n",
    ".sidebar {\n",
    "    background: linear-gradient(180deg, #1a1a2e 0%, #16213e 100%) !important;\n",
    "    border-right: 1px solid #2d1b69 !important;\n",
    "    padding: 16px !important;\n",
    "    min-height: 100vh !important;\n",
    "    overflow-y: auto !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    box-shadow: 2px 0 10px rgba(45, 27, 105, 0.3) !important;\n",
    "}\n",
    "\n",
    ".sidebar h3 {\n",
    "    color: #a78bfa !important;\n",
    "    font-size: 14px !important;\n",
    "    font-weight: 600 !important;\n",
    "    margin-bottom: 16px !important;\n",
    "    padding-bottom: 8px !important;\n",
    "    border-bottom: 1px solid #2d1b69 !important;\n",
    "}\n",
    "\n",
    ".sidebar label {\n",
    "    color: #cbd5e1 !important;\n",
    "    font-weight: 400 !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\n",
    "/* Chat container */\n",
    ".chat-container {\n",
    "    background-color: #0f0f23 !important;\n",
    "    min-height: 100vh !important;\n",
    "    display: flex !important;\n",
    "    flex-direction: column !important;\n",
    "}\n",
    "\n",
    "/* Header */\n",
    ".chat-header {\n",
    "    background: linear-gradient(135deg, #5b21b6 0%, #7c3aed 50%, #8b5cf6 100%) !important;\n",
    "    color: #f8fafc !important;\n",
    "    padding: 12px 20px !important;\n",
    "    margin: 0 !important;\n",
    "    border-radius: 0 !important;\n",
    "    box-shadow: 0 2px 8px rgba(91, 33, 182, 0.3) !important;\n",
    "    border-bottom: 1px solid #6d28d9 !important;\n",
    "}\n",
    "\n",
    ".chat-header h1 {\n",
    "    font-size: 16px !important;\n",
    "    font-weight: 600 !important;\n",
    "    margin: 0 !important;\n",
    "    color: #f8fafc !important;\n",
    "}\n",
    "\n",
    "/* Chatbot container */\n",
    ".chatbot {\n",
    "    flex-grow: 1 !important;\n",
    "    border: none !important;\n",
    "    background-color: #0f0f23 !important;\n",
    "    padding: 16px !important;\n",
    "    overflow-y: auto !important;\n",
    "}\n",
    "\n",
    "/* Chat message styling */\n",
    ".message {\n",
    "    border-radius: 12px !important;\n",
    "    padding: 12px 16px !important;\n",
    "    margin: 6px 0 !important;\n",
    "    max-width: 85% !important;\n",
    "    word-wrap: break-word !important;\n",
    "    font-size: 13px !important;\n",
    "    line-height: 1.5 !important;\n",
    "}\n",
    "\n",
    "/* User messages */\n",
    ".message.user {\n",
    "    background: linear-gradient(135deg, #5b21b6 0%, #7c3aed 100%) !important;\n",
    "    color: #f8fafc !important;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: 0 !important;\n",
    "    border: 1px solid #6d28d9 !important;\n",
    "}\n",
    "\n",
    "/* Bot messages */\n",
    ".message.bot {\n",
    "    background: linear-gradient(135deg, #1e293b 0%, #334155 100%) !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    margin-left: 0 !important;\n",
    "    margin-right: auto !important;\n",
    "    border: 1px solid #475569 !important;\n",
    "    border-left: 3px solid #8b5cf6 !important;\n",
    "}\n",
    "\n",
    "/* Message input area */\n",
    ".message-input-container {\n",
    "    padding: 16px !important;\n",
    "    background: linear-gradient(180deg, #0f0f23 0%, #1a1a2e 100%) !important;\n",
    "    border-top: 1px solid #2d1b69 !important;\n",
    "    box-shadow: 0 -2px 8px rgba(0, 0, 0, 0.2) !important;\n",
    "}\n",
    "\n",
    "/* Input styling */\n",
    ".message-input textarea {\n",
    "    border-radius: 8px !important;\n",
    "    border: 1px solid #475569 !important;\n",
    "    padding: 10px 14px !important;\n",
    "    font-size: 13px !important;\n",
    "    background-color: #1e293b !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    box-shadow: inset 0 1px 3px rgba(0, 0, 0, 0.2) !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "}\n",
    "\n",
    ".message-input textarea:focus {\n",
    "    border-color: #8b5cf6 !important;\n",
    "    box-shadow: 0 0 0 2px rgba(139, 92, 246, 0.2) !important;\n",
    "    outline: none !important;\n",
    "}\n",
    "\n",
    ".message-input textarea::placeholder {\n",
    "    color: #94a3b8 !important;\n",
    "}\n",
    "\n",
    "/* Button styling */\n",
    ".send-button {\n",
    "    border-radius: 8px !important;\n",
    "    background: linear-gradient(135deg, #5b21b6 0%, #7c3aed 100%) !important;\n",
    "    border: none !important;\n",
    "    padding: 10px 16px !important;\n",
    "    font-size: 13px !important;\n",
    "    color: #f8fafc !important;\n",
    "    font-weight: 500 !important;\n",
    "    box-shadow: 0 2px 4px rgba(91, 33, 182, 0.3) !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "}\n",
    "\n",
    ".send-button:hover {\n",
    "    background: linear-gradient(135deg, #6d28d9 0%, #8b5cf6 100%) !important;\n",
    "    transform: translateY(-1px) !important;\n",
    "    box-shadow: 0 4px 8px rgba(91, 33, 182, 0.4) !important;\n",
    "}\n",
    "\n",
    "/* File upload area */\n",
    ".file-upload {\n",
    "    border: 1px dashed #475569 !important;\n",
    "    border-radius: 8px !important;\n",
    "    padding: 12px !important;\n",
    "    text-align: center !important;\n",
    "    background: linear-gradient(135deg, #1e293b 0%, #334155 100%) !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "    color: #cbd5e1 !important;\n",
    "}\n",
    "\n",
    ".file-upload:hover {\n",
    "    border-color: #8b5cf6 !important;\n",
    "    background: linear-gradient(135deg, #2d1b69 0%, #3730a3 100%) !important;\n",
    "}\n",
    "\n",
    "/* Dropdown and input field styling */\n",
    ".gradio-dropdown, .gradio-textbox, .gradio-slider {\n",
    "    background-color: #1e293b !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    border: 1px solid #475569 !important;\n",
    "    border-radius: 6px !important;\n",
    "}\n",
    "\n",
    ".gradio-dropdown select, .gradio-textbox input, .gradio-textbox textarea {\n",
    "    background-color: #1e293b !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    border: none !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\n",
    ".gradio-dropdown select:focus, .gradio-textbox input:focus, .gradio-textbox textarea:focus {\n",
    "    box-shadow: 0 0 0 2px rgba(139, 92, 246, 0.2) !important;\n",
    "    outline: none !important;\n",
    "}\n",
    "\n",
    "/* Button variants */\n",
    ".gradio-button {\n",
    "    border-radius: 6px !important;\n",
    "    font-size: 12px !important;\n",
    "    font-weight: 500 !important;\n",
    "    padding: 8px 12px !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "}\n",
    "\n",
    ".gradio-button.secondary {\n",
    "    background: linear-gradient(135deg, #374151 0%, #4b5563 100%) !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    border: 1px solid #6b7280 !important;\n",
    "}\n",
    "\n",
    ".gradio-button.secondary:hover {\n",
    "    background: linear-gradient(135deg, #4b5563 0%, #6b7280 100%) !important;\n",
    "}\n",
    "\n",
    ".gradio-button.stop {\n",
    "    background: linear-gradient(135deg, #dc2626 0%, #ef4444 100%) !important;\n",
    "    color: #f8fafc !important;\n",
    "    border: none !important;\n",
    "}\n",
    "\n",
    ".gradio-button.stop:hover {\n",
    "    background: linear-gradient(135deg, #b91c1c 0%, #dc2626 100%) !important;\n",
    "}\n",
    "\n",
    "/* Accordion styling */\n",
    ".gradio-accordion {\n",
    "    background-color: #1e293b !important;\n",
    "    border: 1px solid #475569 !important;\n",
    "    border-radius: 6px !important;\n",
    "}\n",
    "\n",
    ".gradio-accordion .accordion-header {\n",
    "    background-color: #334155 !important;\n",
    "    color: #cbd5e1 !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\n",
    "/* Image thumbnails in chat */\n",
    ".chat-thumbnail {\n",
    "    border-radius: 8px !important;\n",
    "    border: 1px solid #475569 !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "    cursor: pointer !important;\n",
    "    box-shadow: 0 2px 8px rgba(0, 0, 0, 0.3) !important;\n",
    "}\n",
    "\n",
    ".chat-thumbnail:hover {\n",
    "    transform: scale(1.02) !important;\n",
    "    border-color: #8b5cf6 !important;\n",
    "    box-shadow: 0 4px 12px rgba(139, 92, 246, 0.3) !important;\n",
    "}\n",
    "\n",
    "/* Message with image container */\n",
    ".message-with-image {\n",
    "    display: flex !important;\n",
    "    flex-direction: column !important;\n",
    "    gap: 8px !important;\n",
    "}\n",
    "\n",
    ".message-text {\n",
    "    font-size: 13px !important;\n",
    "    line-height: 1.5 !important;\n",
    "}\n",
    "\n",
    "/* Markdown content styling */\n",
    ".markdown-content h1, .markdown-content h2, .markdown-content h3 {\n",
    "    color: #a78bfa !important;\n",
    "    margin: 12px 0 6px 0 !important;\n",
    "    font-size: 14px !important;\n",
    "}\n",
    "\n",
    ".markdown-content h1 { font-size: 16px !important; }\n",
    ".markdown-content h2 { font-size: 15px !important; }\n",
    ".markdown-content h3 { font-size: 14px !important; }\n",
    "\n",
    ".markdown-content code {\n",
    "    background-color: #1e293b !important;\n",
    "    color: #e879f9 !important;\n",
    "    padding: 2px 4px !important;\n",
    "    border-radius: 4px !important;\n",
    "    font-family: 'JetBrains Mono', 'Consolas', monospace !important;\n",
    "    font-size: 12px !important;\n",
    "    border: 1px solid #475569 !important;\n",
    "}\n",
    "\n",
    ".markdown-content pre {\n",
    "    background-color: #0f172a !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    padding: 12px !important;\n",
    "    border-radius: 8px !important;\n",
    "    overflow-x: auto !important;\n",
    "    border: 1px solid #334155 !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\n",
    ".markdown-content pre code {\n",
    "    background: none !important;\n",
    "    border: none !important;\n",
    "    padding: 0 !important;\n",
    "}\n",
    "\n",
    "/* Sidebar toggle button */\n",
    ".sidebar-toggle {\n",
    "    position: fixed !important;\n",
    "    top: 16px !important;\n",
    "    left: 16px !important;\n",
    "    z-index: 1001 !important;\n",
    "    background: linear-gradient(135deg, #5b21b6 0%, #7c3aed 100%) !important;\n",
    "    border: none !important;\n",
    "    border-radius: 6px !important;\n",
    "    padding: 8px 10px !important;\n",
    "    color: #f8fafc !important;\n",
    "    box-shadow: 0 2px 8px rgba(91, 33, 182, 0.4) !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\n",
    "/* Scrollbar styling */\n",
    ".chatbot::-webkit-scrollbar, .sidebar::-webkit-scrollbar {\n",
    "    width: 6px !important;\n",
    "}\n",
    "\n",
    ".chatbot::-webkit-scrollbar-track, .sidebar::-webkit-scrollbar-track {\n",
    "    background: #1e293b !important;\n",
    "}\n",
    "\n",
    ".chatbot::-webkit-scrollbar-thumb, .sidebar::-webkit-scrollbar-thumb {\n",
    "    background: #475569 !important;\n",
    "    border-radius: 3px !important;\n",
    "}\n",
    "\n",
    ".chatbot::-webkit-scrollbar-thumb:hover, .sidebar::-webkit-scrollbar-thumb:hover {\n",
    "    background: #6b7280 !important;\n",
    "}\n",
    "\n",
    "/* Responsive design */\n",
    "@media (max-width: 768px) {\n",
    "    .sidebar {\n",
    "        width: 100% !important;\n",
    "        position: fixed !important;\n",
    "        z-index: 1000 !important;\n",
    "        height: 100vh !important;\n",
    "        left: -100% !important;\n",
    "        transition: left 0.3s ease !important;\n",
    "    }\n",
    "    \n",
    "    .sidebar.visible {\n",
    "        left: 0 !important;\n",
    "    }\n",
    "    \n",
    "    .chat-container {\n",
    "        width: 100% !important;\n",
    "        margin-left: 0 !important;\n",
    "    }\n",
    "    \n",
    "    .message {\n",
    "        max-width: 95% !important;\n",
    "    }\n",
    "    \n",
    "    .chat-header h1 {\n",
    "        font-size: 14px !important;\n",
    "    }\n",
    "    \n",
    "    .message-input-container {\n",
    "        padding: 12px !important;\n",
    "    }\n",
    "}\n",
    "\n",
    "@media (max-width: 640px) {\n",
    "    .message {\n",
    "        max-width: 100% !important;\n",
    "        margin-left: 0 !important;\n",
    "        margin-right: 0 !important;\n",
    "    }\n",
    "}\n",
    "\n",
    "/* Fix for Gradio specific elements */\n",
    ".gradio-chatbot .message-wrap {\n",
    "    background: transparent !important;\n",
    "}\n",
    "\n",
    ".gradio-chatbot .message-wrap .message {\n",
    "    background: transparent !important;\n",
    "}\n",
    "\n",
    "/* Avatar styling */\n",
    ".gradio-chatbot .avatar {\n",
    "    width: 24px !important;\n",
    "    height: 24px !important;\n",
    "    border-radius: 50% !important;\n",
    "    margin-right: 8px !important;\n",
    "}\n",
    "\n",
    "/* Copy button styling */\n",
    ".copy-button {\n",
    "    background: rgba(139, 92, 246, 0.1) !important;\n",
    "    color: #a78bfa !important;\n",
    "    border: 1px solid rgba(139, 92, 246, 0.3) !important;\n",
    "    border-radius: 4px !important;\n",
    "    font-size: 11px !important;\n",
    "    padding: 4px 8px !important;\n",
    "}\n",
    "\n",
    ".copy-button:hover {\n",
    "    background: rgba(139, 92, 246, 0.2) !important;\n",
    "    border-color: rgba(139, 92, 246, 0.5) !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# System prompts for different usage modes\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"General Assistant\": \"You are a helpful assistant that can analyze images and answer questions.\",\n",
    "    \"Image Analyzer\": \"You are an expert image analyst. Provide detailed descriptions and analysis of images.\",\n",
    "    \"OCR Reader\": \"You are an OCR specialist. Extract and transcribe all text from images accurately.\",\n",
    "    \"Medical Assistant\": \"You are a medical AI assistant. Analyze medical images and provide insights (for educational purposes only).\",\n",
    "    \"Educational Tutor\": \"You are an educational tutor. Help explain concepts shown in images and answer related questions.\"\n",
    "}\n",
    "\n",
    "def inference(\n",
    "    prompt,\n",
    "    image_path,\n",
    "    system_prompt = 'You are a helpful assistant',\n",
    "    max_new_tokens = 32000,\n",
    "    min_pixels = 512 * 28 * 28,\n",
    "    max_pixels = 2048 * 28 * 28\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : 'user',\n",
    "            'content' : [\n",
    "                {\n",
    "                    'type' : 'image',\n",
    "                    'image' : image_path,\n",
    "                    'min_pixels' : min_pixels,\n",
    "                    'max_pixels' : max_pixels,\n",
    "                },\n",
    "                {'type' : 'text', 'text' : prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize = False, add_generation_prompt = True\n",
    "    )\n",
    "    print('input:\\n', text)\n",
    "        \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n",
    "    )\n",
    "\n",
    "    print('output:\\n', output_text[0])\n",
    "\n",
    "    input_height = inputs['image_grid_thw'][0][1] * 14\n",
    "    input_width = inputs['image_grid_thw'][0][2] * 14\n",
    "\n",
    "    return output_text[0], input_height, input_width\n",
    "\n",
    "# Global storage for image thumbnails and originals\n",
    "image_storage = {}\n",
    "\n",
    "def create_image_thumbnail(image_path: str, thumbnail_size: tuple = (120, 120)) -> tuple:\n",
    "    \"\"\"Create thumbnail and store original image, return thumbnail path and storage key\"\"\"\n",
    "    try:\n",
    "        # Generate unique key for this image\n",
    "        import hashlib\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_hash = hashlib.md5(f.read()).hexdigest()\n",
    "        \n",
    "        # Create thumbnail\n",
    "        with Image.open(image_path) as img:\n",
    "            # Convert to RGB if necessary\n",
    "            if img.mode in ('RGBA', 'P'):\n",
    "                img = img.convert('RGB')\n",
    "            \n",
    "            # Create thumbnail\n",
    "            img.thumbnail(thumbnail_size, Image.Resampling.LANCZOS)\n",
    "            \n",
    "            # Save thumbnail to temp file\n",
    "            temp_thumb = tempfile.NamedTemporaryFile(delete=False, suffix='.jpg')\n",
    "            img.save(temp_thumb.name, 'JPEG', quality=85)\n",
    "            \n",
    "        # Store original and thumbnail paths\n",
    "        image_storage[image_hash] = {\n",
    "            'original': image_path,\n",
    "            'thumbnail': temp_thumb.name\n",
    "        }\n",
    "        \n",
    "        return temp_thumb.name, image_hash\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating thumbnail: {e}\")\n",
    "        return image_path, None\n",
    "\n",
    "def format_message_with_image(message: str, image_path: str, image_key: str) -> str:\n",
    "    \"\"\"Format message with embedded thumbnail image\"\"\"\n",
    "    if image_path and image_key:\n",
    "        return f\"\"\"<div class=\"message-with-image\">\n",
    "<img src=\"file://{image_path}\" \n",
    "     class=\"chat-thumbnail\" \n",
    "     data-key=\"{image_key}\"\n",
    "     style=\"max-width: 120px; max-height: 120px; border-radius: 8px; margin: 4px 0; cursor: pointer;\" />\n",
    "<div class=\"message-text\">{message}</div>\n",
    "</div>\"\"\"\n",
    "    return message\n",
    "\n",
    "def simulate_streaming_response(full_text: str):\n",
    "    \"\"\"Simulate streaming response like real chatbots\"\"\"\n",
    "    words = full_text.split(' ')\n",
    "    current_response = \"\"\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        current_response += word + \" \"\n",
    "        # Vary the delay to make it more natural\n",
    "        if word.endswith(('.', '!', '?')):\n",
    "            delay = 0.12  # Longer pause after sentences\n",
    "        elif word.endswith(','):\n",
    "            delay = 0.06  # Medium pause after commas\n",
    "        else:\n",
    "            delay = 0.025  # Quick pace for regular words\n",
    "            \n",
    "        yield current_response.strip()\n",
    "        time.sleep(delay)\n",
    "\n",
    "def chat_with_history(\n",
    "    message: str,\n",
    "    image: Optional[str],\n",
    "    history: List[Tuple[str, str]],\n",
    "    usage_mode: str,\n",
    "    custom_system_prompt: str,\n",
    "    max_tokens: int,\n",
    "    min_pixels: int,\n",
    "    max_pixels: int\n",
    "):\n",
    "    \"\"\"Handle chat with conversation history and real streaming\"\"\"\n",
    "    try:\n",
    "        # Determine system prompt\n",
    "        if custom_system_prompt.strip():\n",
    "            system_prompt = custom_system_prompt.strip()\n",
    "        else:\n",
    "            system_prompt = SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "        \n",
    "        # If no message and no image, return\n",
    "        if not message.strip() and not image:\n",
    "            return history, \"\", None\n",
    "        \n",
    "        # Prepare user message\n",
    "        user_message = message.strip() if message.strip() else \"Please analyze this image.\"\n",
    "        user_display_message = user_message\n",
    "        \n",
    "        # Handle image if provided\n",
    "        image_key = None\n",
    "        if image:\n",
    "            thumbnail_path, image_key = create_image_thumbnail(image)\n",
    "            if image_key:\n",
    "                user_display_message = format_message_with_image(user_message, thumbnail_path, image_key)\n",
    "        \n",
    "        # Add user message to history immediately\n",
    "        history.append([user_display_message, \"\"])\n",
    "        yield history, \"\", None\n",
    "        \n",
    "        # Get AI response\n",
    "        if image:\n",
    "            response, img_height, img_width = inference(\n",
    "                prompt=user_message,\n",
    "                image_path=image,\n",
    "                system_prompt=system_prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                min_pixels=min_pixels,\n",
    "                max_pixels=max_pixels\n",
    "            )\n",
    "        else:\n",
    "            response = \"Please upload an image to continue the conversation with this vision-language model.\"\n",
    "        \n",
    "        # Stream the response with markdown formatting\n",
    "        history[-1] = [user_display_message, \"\"]\n",
    "        for partial_response in simulate_streaming_response(response):\n",
    "            # Format as markdown\n",
    "            formatted_response = partial_response\n",
    "            history[-1] = [user_display_message, formatted_response]\n",
    "            yield history, \"\", None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå **Error:** {str(e)}\"\n",
    "        if history and len(history) > 0 and history[-1][1] == \"\":\n",
    "            history[-1] = [history[-1][0], error_msg]\n",
    "        else:\n",
    "            history.append([message if message else \"Error occurred\", error_msg])\n",
    "        yield history, \"\", None\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clear chat history and image storage\"\"\"\n",
    "    global image_storage\n",
    "    # Clean up temporary files\n",
    "    for data in image_storage.values():\n",
    "        try:\n",
    "            if os.path.exists(data['thumbnail']):\n",
    "                os.unlink(data['thumbnail'])\n",
    "        except:\n",
    "            pass\n",
    "    image_storage.clear()\n",
    "    return [], \"\", None\n",
    "\n",
    "def update_system_prompt(usage_mode: str) -> str:\n",
    "    \"\"\"Update system prompt based on usage mode\"\"\"\n",
    "    return SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "\n",
    "def toggle_sidebar():\n",
    "    \"\"\"Toggle sidebar visibility\"\"\"\n",
    "    return gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "def show_sidebar():\n",
    "    \"\"\"Show sidebar\"\"\"\n",
    "    return gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(css=custom_css, title=\"Qwen2.5 VL Chat\", theme=gr.themes.Soft()) as demo:\n",
    "    \n",
    "    # State for sidebar visibility\n",
    "    sidebar_visible = gr.State(True)\n",
    "    \n",
    "    with gr.Row(elem_classes=\"main-container\"):\n",
    "        # Sidebar\n",
    "        with gr.Column(scale=1, elem_classes=\"sidebar\", visible=True) as sidebar:\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Configuration\", elem_classes=\"sidebar-title\")\n",
    "            \n",
    "            # Hide sidebar button\n",
    "            hide_sidebar_btn = gr.Button(\"‚Üê Hide\", size=\"sm\", variant=\"secondary\")\n",
    "            \n",
    "            usage_mode = gr.Dropdown(\n",
    "                choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                value=\"General Assistant\",\n",
    "                label=\"üéØ Mode\",\n",
    "                info=\"Select AI behavior\"\n",
    "            )\n",
    "            \n",
    "            custom_system_prompt = gr.Textbox(\n",
    "                label=\"üìù Custom Prompt\",\n",
    "                placeholder=\"Override with custom prompt...\",\n",
    "                lines=3\n",
    "            )\n",
    "            \n",
    "            with gr.Accordion(\"üîß Advanced\", open=False):\n",
    "                max_tokens = gr.Slider(\n",
    "                    minimum=100,\n",
    "                    maximum=32000,\n",
    "                    value=2000,\n",
    "                    step=100,\n",
    "                    label=\"Max Tokens\"\n",
    "                )\n",
    "                \n",
    "                min_pixels = gr.Number(\n",
    "                    value=512 * 28 * 28,\n",
    "                    label=\"Min Pixels\"\n",
    "                )\n",
    "                \n",
    "                max_pixels = gr.Number(\n",
    "                    value=2048 * 28 * 28,\n",
    "                    label=\"Max Pixels\"\n",
    "                )\n",
    "            \n",
    "            clear_btn = gr.Button(\"üóëÔ∏è Clear\", variant=\"stop\")\n",
    "        \n",
    "        # Show sidebar button (hidden by default)\n",
    "        show_sidebar_btn = gr.Button(\"‚ò∞\", elem_classes=\"sidebar-toggle\", visible=False)\n",
    "        \n",
    "        # Main chat area\n",
    "        with gr.Column(scale=4, elem_classes=\"chat-container\"):\n",
    "            # Header\n",
    "            gr.Markdown(\"# ü§ñ Qwen2.5 VL Assistant\", elem_classes=\"chat-header\")\n",
    "            \n",
    "            # Chatbot with markdown support\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"\",\n",
    "                height=550,\n",
    "                show_copy_button=True,\n",
    "                elem_classes=\"chatbot\",\n",
    "                avatar_images=(\"üë§\", \"ü§ñ\"),\n",
    "                bubble_full_width=False,\n",
    "                render_markdown=True,\n",
    "                latex_delimiters=[{\"left\": \"$$\", \"right\": \"$$\", \"display\": True}]\n",
    "            )\n",
    "            \n",
    "            # Message input area\n",
    "            with gr.Row(elem_classes=\"message-input-container\"):\n",
    "                with gr.Column(scale=1):\n",
    "                    # Image attachment\n",
    "                    image_input = gr.File(\n",
    "                        file_types=[\"image\"],\n",
    "                        label=\"üìé Image\",\n",
    "                        elem_classes=\"file-upload\",\n",
    "                        height=70\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=6):\n",
    "                    msg_input = gr.Textbox(\n",
    "                        placeholder=\"Message Qwen2.5 VL...\",\n",
    "                        lines=2,\n",
    "                        max_lines=5,\n",
    "                        elem_classes=\"message-input\",\n",
    "                        show_label=False,\n",
    "                        container=False\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=1):\n",
    "                    send_btn = gr.Button(\"Send\", variant=\"primary\", elem_classes=\"send-button\")\n",
    "    \n",
    "    # Event handlers\n",
    "    \n",
    "    # Auto-update system prompt when usage mode changes\n",
    "    usage_mode.change(\n",
    "        fn=update_system_prompt,\n",
    "        inputs=[usage_mode],\n",
    "        outputs=[custom_system_prompt]\n",
    "    )\n",
    "    \n",
    "    # Sidebar toggle functions\n",
    "    hide_sidebar_btn.click(\n",
    "        fn=toggle_sidebar,\n",
    "        outputs=[sidebar, show_sidebar_btn]\n",
    "    )\n",
    "    \n",
    "    show_sidebar_btn.click(\n",
    "        fn=show_sidebar,\n",
    "        outputs=[sidebar, show_sidebar_btn]\n",
    "    )\n",
    "    \n",
    "    # Chat functions with real streaming\n",
    "    send_btn.click(\n",
    "        fn=chat_with_history,\n",
    "        inputs=[\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "    \n",
    "    msg_input.submit(\n",
    "        fn=chat_with_history,\n",
    "        inputs=[\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=clear_history,\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        #server_name=\"0.0.0.0\",\n",
    "        #server_port=7860,\n",
    "        #share=False,\n",
    "        debug=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "import asyncio\n",
    "import time\n",
    "import base64\n",
    "import io\n",
    "import tempfile\n",
    "\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from IPython.display import display\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")\n",
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    ").to(device) #''\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "\n",
    "# Dark purple theme CSS inspired by Qwen\n",
    "custom_css = \"\"\"\n",
    "/* Base font size and dark theme */\n",
    "* {\n",
    "    font-size: 13px !important;\n",
    "    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif !important;\n",
    "}\n",
    "\n",
    "/* Main container */\n",
    ".gradio-container {\n",
    "    background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%) !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    min-height: 100vh !important;\n",
    "}\n",
    "\n",
    "/* Sidebar styling */\n",
    ".sidebar {\n",
    "    background: linear-gradient(180deg, #2d1b69 0%, #1a1a2e 100%) !important;\n",
    "    border-right: 1px solid #4c1d95 !important;\n",
    "    padding: 16px !important;\n",
    "    color: #e2e8f0 !important;\n",
    "    box-shadow: 2px 0 10px rgba(0,0,0,0.3) !important;\n",
    "}\n",
    "\n",
    ".sidebar h3 {\n",
    "    color: #a78bfa !important;\n",
    "    font-size: 16px !important;\n",
    "    font-weight: 600 !important;\n",
    "    margin-bottom: 16px !important;\n",
    "}\n",
    "\n",
    ".sidebar label {\n",
    "    color: #c4b5fd !important;\n",
    "    font-size: 12px !important;\n",
    "    font-weight: 500 !important;\n",
    "}\n",
    "\n",
    "/* Chat container */\n",
    ".chat-container {\n",
    "    background: linear-gradient(135deg, #1e1e3f 0%, #2d1b69 100%) !important;\n",
    "    min-height: 100vh !important;\n",
    "}\n",
    "\n",
    "/* Header */\n",
    ".chat-header {\n",
    "    background: linear-gradient(90deg, #5b21b6 0%, #7c3aed 100%) !important;\n",
    "    color: #f8fafc !important;\n",
    "    padding: 12px 20px !important;\n",
    "    border-bottom: 1px solid #4c1d95 !important;\n",
    "    box-shadow: 0 2px 8px rgba(0,0,0,0.2) !important;\n",
    "}\n",
    "\n",
    ".chat-header h1 {\n",
    "    font-size: 18px !important;\n",
    "    font-weight: 600 !important;\n",
    "    margin: 0 !important;\n",
    "}\n",
    "\n",
    "/* Chatbot container */\n",
    ".chatbot {\n",
    "    background: rgba(30, 30, 63, 0.5) !important;\n",
    "    border: 1px solid #4c1d95 !important;\n",
    "    border-radius: 12px !important;\n",
    "    margin: 16px !important;\n",
    "    box-shadow: inset 0 2px 10px rgba(0,0,0,0.2) !important;\n",
    "}\n",
    "\n",
    "/* Message bubbles */\n",
    ".message {\n",
    "    font-size: 13px !important;\n",
    "    line-height: 1.5 !important;\n",
    "    margin: 8px 12px !important;\n",
    "    padding: 12px 16px !important;\n",
    "    border-radius: 16px !important;\n",
    "    max-width: 80% !important;\n",
    "}\n",
    "\n",
    "/* User messages */\n",
    ".message.user {\n",
    "    background: linear-gradient(135deg, #7c3aed 0%, #5b21b6 100%) !important;\n",
    "    color: #f8fafc !important;\n",
    "    margin-left: auto !important;\n",
    "    margin-right: 12px !important;\n",
    "    box-shadow: 0 2px 8px rgba(124, 58, 237, 0.3) !important;\n",
    "}\n",
    "\n",
    "/* Bot messages */\n",
    ".message.bot {\n",
    "    background: linear-gradient(135deg, #374151 0%, #4b5563 100%) !important;\n",
    "    color: #f1f5f9 !important;\n",
    "    margin-right: auto !important;\n",
    "    margin-left: 12px !important;\n",
    "    border-left: 3px solid #a78bfa !important;\n",
    "    box-shadow: 0 2px 8px rgba(0,0,0,0.2) !important;\n",
    "}\n",
    "\n",
    "/* Message input area */\n",
    ".message-input-container {\n",
    "    background: rgba(45, 27, 105, 0.8) !important;\n",
    "    border-top: 1px solid #4c1d95 !important;\n",
    "    padding: 16px !important;\n",
    "    backdrop-filter: blur(10px) !important;\n",
    "}\n",
    "\n",
    "/* Input styling */\n",
    ".message-input textarea {\n",
    "    background: rgba(55, 65, 81, 0.9) !important;\n",
    "    border: 1px solid #6366f1 !important;\n",
    "    border-radius: 20px !important;\n",
    "    padding: 12px 16px !important;\n",
    "    color: #f1f5f9 !important;\n",
    "    font-size: 13px !important;\n",
    "    box-shadow: 0 2px 8px rgba(0,0,0,0.1) !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "}\n",
    "\n",
    ".message-input textarea:focus {\n",
    "    border-color: #a78bfa !important;\n",
    "    box-shadow: 0 0 0 2px rgba(167, 139, 250, 0.2) !important;\n",
    "    background: rgba(55, 65, 81, 1) !important;\n",
    "}\n",
    "\n",
    ".message-input textarea::placeholder {\n",
    "    color: #9ca3af !important;\n",
    "}\n",
    "\n",
    "/* Button styling */\n",
    ".send-button {\n",
    "    background: linear-gradient(135deg, #7c3aed 0%, #5b21b6 100%) !important;\n",
    "    border: none !important;\n",
    "    border-radius: 16px !important;\n",
    "    padding: 12px 20px !important;\n",
    "    color: #f8fafc !important;\n",
    "    font-size: 13px !important;\n",
    "    font-weight: 600 !important;\n",
    "    box-shadow: 0 2px 8px rgba(124, 58, 237, 0.4) !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "}\n",
    "\n",
    ".send-button:hover {\n",
    "    transform: translateY(-1px) !important;\n",
    "    box-shadow: 0 4px 12px rgba(124, 58, 237, 0.5) !important;\n",
    "}\n",
    "\n",
    "/* File upload area */\n",
    ".file-upload {\n",
    "    background: rgba(55, 65, 81, 0.6) !important;\n",
    "    border: 2px dashed #6366f1 !important;\n",
    "    border-radius: 12px !important;\n",
    "    padding: 12px !important;\n",
    "    color: #c4b5fd !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "}\n",
    "\n",
    ".file-upload:hover {\n",
    "    border-color: #a78bfa !important;\n",
    "    background: rgba(55, 65, 81, 0.8) !important;\n",
    "}\n",
    "\n",
    "/* Dropdown and input components */\n",
    ".dropdown select {\n",
    "    background: rgba(55, 65, 81, 0.9) !important;\n",
    "    border: 1px solid #6366f1 !important;\n",
    "    color: #f1f5f9 !important;\n",
    "    border-radius: 8px !important;\n",
    "    padding: 8px 12px !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\n",
    ".textbox input, .textbox textarea {\n",
    "    background: rgba(55, 65, 81, 0.9) !important;\n",
    "    border: 1px solid #6366f1 !important;\n",
    "    color: #f1f5f9 !important;\n",
    "    border-radius: 8px !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\n",
    ".textbox input::placeholder, .textbox textarea::placeholder {\n",
    "    color: #9ca3af !important;\n",
    "}\n",
    "\n",
    "/* Number inputs and sliders */\n",
    ".number input {\n",
    "    background: rgba(55, 65, 81, 0.9) !important;\n",
    "    border: 1px solid #6366f1 !important;\n",
    "    color: #f1f5f9 !important;\n",
    "    border-radius: 6px !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\n",
    ".slider {\n",
    "    background: rgba(55, 65, 81, 0.6) !important;\n",
    "}\n",
    "\n",
    "/* Accordion */\n",
    ".accordion {\n",
    "    background: rgba(45, 27, 105, 0.3) !important;\n",
    "    border: 1px solid #4c1d95 !important;\n",
    "    border-radius: 8px !important;\n",
    "}\n",
    "\n",
    "/* Clear button */\n",
    ".clear-btn {\n",
    "    background: linear-gradient(135deg, #dc2626 0%, #991b1b 100%) !important;\n",
    "    border: none !important;\n",
    "    border-radius: 12px !important;\n",
    "    padding: 10px 16px !important;\n",
    "    color: #f8fafc !important;\n",
    "    font-size: 12px !important;\n",
    "    margin-top: 16px !important;\n",
    "}\n",
    "\n",
    "/* Hide/Show sidebar buttons */\n",
    ".sidebar-toggle {\n",
    "    background: linear-gradient(135deg, #5b21b6 0%, #4c1d95 100%) !important;\n",
    "    border: none !important;\n",
    "    border-radius: 8px !important;\n",
    "    padding: 8px 12px !important;\n",
    "    color: #f8fafc !important;\n",
    "    font-size: 12px !important;\n",
    "    position: fixed !important;\n",
    "    top: 16px !important;\n",
    "    left: 16px !important;\n",
    "    z-index: 1000 !important;\n",
    "    box-shadow: 0 2px 8px rgba(0,0,0,0.3) !important;\n",
    "}\n",
    "\n",
    "/* Image preview in chat */\n",
    ".chat-image {\n",
    "    border-radius: 12px !important;\n",
    "    border: 2px solid #6366f1 !important;\n",
    "    box-shadow: 0 4px 12px rgba(0,0,0,0.3) !important;\n",
    "    margin: 8px 0 !important;\n",
    "    max-width: 200px !important;\n",
    "    max-height: 200px !important;\n",
    "    object-fit: cover !important;\n",
    "    cursor: pointer !important;\n",
    "    transition: all 0.2s ease !important;\n",
    "}\n",
    "\n",
    ".chat-image:hover {\n",
    "    transform: scale(1.02) !important;\n",
    "    border-color: #a78bfa !important;\n",
    "}\n",
    "\n",
    "/* Message with image container */\n",
    ".message-with-image {\n",
    "    display: flex !important;\n",
    "    flex-direction: column !important;\n",
    "    gap: 8px !important;\n",
    "}\n",
    "\n",
    "/* Scrollbar styling */\n",
    "::-webkit-scrollbar {\n",
    "    width: 6px !important;\n",
    "}\n",
    "\n",
    "::-webkit-scrollbar-track {\n",
    "    background: rgba(30, 30, 63, 0.5) !important;\n",
    "}\n",
    "\n",
    "::-webkit-scrollbar-thumb {\n",
    "    background: linear-gradient(180deg, #7c3aed 0%, #5b21b6 100%) !important;\n",
    "    border-radius: 3px !important;\n",
    "}\n",
    "\n",
    "::-webkit-scrollbar-thumb:hover {\n",
    "    background: linear-gradient(180deg, #8b5cf6 0%, #6d28d9 100%) !important;\n",
    "}\n",
    "\n",
    "/* Responsive design */\n",
    "@media (max-width: 768px) {\n",
    "    .sidebar {\n",
    "        width: 100% !important;\n",
    "        position: fixed !important;\n",
    "        z-index: 999 !important;\n",
    "        height: 100vh !important;\n",
    "        left: -100% !important;\n",
    "        transition: left 0.3s ease !important;\n",
    "    }\n",
    "    \n",
    "    .sidebar.visible {\n",
    "        left: 0 !important;\n",
    "    }\n",
    "    \n",
    "    .message {\n",
    "        max-width: 90% !important;\n",
    "    }\n",
    "    \n",
    "    .chat-header h1 {\n",
    "        font-size: 16px !important;\n",
    "    }\n",
    "}\n",
    "\n",
    "/* Code blocks */\n",
    "pre {\n",
    "    background: rgba(17, 24, 39, 0.8) !important;\n",
    "    border: 1px solid #4c1d95 !important;\n",
    "    border-radius: 8px !important;\n",
    "    padding: 12px !important;\n",
    "    color: #f1f5f9 !important;\n",
    "    font-size: 12px !important;\n",
    "    overflow-x: auto !important;\n",
    "}\n",
    "\n",
    "code {\n",
    "    background: rgba(55, 65, 81, 0.6) !important;\n",
    "    color: #c4b5fd !important;\n",
    "    padding: 2px 6px !important;\n",
    "    border-radius: 4px !important;\n",
    "    font-size: 12px !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# System prompts for different usage modes\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"General Assistant\": \"You are a helpful assistant that can analyze images and answer questions.\",\n",
    "    \"Image Analyzer\": \"You are an expert image analyst. Provide detailed descriptions and analysis of images.\",\n",
    "    \"OCR Reader\": \"You are an OCR specialist. Extract and transcribe all text from images accurately.\",\n",
    "    \"Medical Assistant\": \"You are a medical AI assistant. Analyze medical images and provide insights (for educational purposes only).\",\n",
    "    \"Educational Tutor\": \"You are an educational tutor. Help explain concepts shown in images and answer related questions.\"\n",
    "}\n",
    "\n",
    "def inference_with_image(\n",
    "    prompt,\n",
    "    image_path,\n",
    "    system_prompt = 'You are a helpful assistant',\n",
    "    max_new_tokens = 32000,\n",
    "    min_pixels = 512 * 28 * 28,\n",
    "    max_pixels = 2048 * 28 * 28\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : 'user',\n",
    "            'content' : [\n",
    "                {\n",
    "                    'type' : 'image',\n",
    "                    'image' : image_path,\n",
    "                    'min_pixels' : min_pixels,\n",
    "                    'max_pixels' : max_pixels,\n",
    "                },\n",
    "                {'type' : 'text', 'text' : prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize = False, add_generation_prompt = True\n",
    "    )\n",
    "        \n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text = [text],\n",
    "        images = image_inputs,\n",
    "        videos = video_inputs,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n",
    "    )\n",
    "\n",
    "    input_height = inputs['image_grid_thw'][0][1] * 14\n",
    "    input_width = inputs['image_grid_thw'][0][2] * 14\n",
    "\n",
    "    return output_text[0], input_height, input_width\n",
    "\n",
    "def inference_text_only(\n",
    "    prompt,\n",
    "    system_prompt = 'You are a helpful assistant',\n",
    "    max_new_tokens = 32000,\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            'role' : 'user',\n",
    "            'content' : [\n",
    "                {'type' : 'text', 'text' : prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize = False, add_generation_prompt = True\n",
    "    )\n",
    "    print('input:\\n', text)\n",
    "        \n",
    "    inputs = processor(\n",
    "        text = text,\n",
    "        padding = True,\n",
    "        return_tensors = 'pt',\n",
    "    )\n",
    "    inputs = inputs.to('cuda')\n",
    "\n",
    "    # Inference: Generation of the output\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens)\n",
    "    generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n",
    "    )\n",
    "\n",
    "    return output_text[0]\n",
    "\n",
    "# Global storage for image thumbnails and originals\n",
    "image_storage = {}\n",
    "\n",
    "def create_image_display(image_path: str) -> tuple:\n",
    "    \"\"\"Create image display for chat, return HTML and storage key\"\"\"\n",
    "    try:\n",
    "        # Generate unique key for this image\n",
    "        import hashlib\n",
    "        with open(image_path, 'rb') as f:\n",
    "            image_hash = hashlib.md5(f.read()).hexdigest()\n",
    "        \n",
    "        # Store image path\n",
    "        image_storage[image_hash] = image_path\n",
    "        \n",
    "        # Create HTML for image display\n",
    "        image_html = f'<img src=\"file://{image_path}\" class=\"chat-image\" style=\"max-width: 200px; max-height: 200px; border-radius: 12px; border: 2px solid #6366f1; margin: 8px 0;\" />'\n",
    "        \n",
    "        return image_html, image_hash\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating image display: {e}\")\n",
    "        return \"\", None\n",
    "\n",
    "def simulate_streaming_response(full_text: str):\n",
    "    \"\"\"Simulate streaming response\"\"\"\n",
    "    # Ensure we have a string\n",
    "    if not isinstance(full_text, str):\n",
    "        full_text = str(full_text)\n",
    "    \n",
    "    # Handle empty or whitespace-only text\n",
    "    if not full_text.strip():\n",
    "        yield \"No response generated.\"\n",
    "        return\n",
    "    \n",
    "    words = full_text.split(' ')\n",
    "    current_response = \"\"\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        current_response += word + \" \"\n",
    "        # Vary the delay to make it more natural\n",
    "        if word.endswith(('.', '!', '?')):\n",
    "            delay = 0.1\n",
    "        elif word.endswith(','):\n",
    "            delay = 0.05\n",
    "        else:\n",
    "            delay = 0.02\n",
    "            \n",
    "        yield current_response.strip()\n",
    "        time.sleep(delay)\n",
    "\n",
    "def chat_with_history(\n",
    "    message: str,\n",
    "    image: Optional[str],\n",
    "    history: List[Tuple[str, str]],\n",
    "    usage_mode: str,\n",
    "    custom_system_prompt: str,\n",
    "    max_tokens: int,\n",
    "    min_pixels: int,\n",
    "    max_pixels: int\n",
    "):\n",
    "    \"\"\"Handle chat with conversation history and real streaming\"\"\"\n",
    "    try:\n",
    "        # Determine system prompt\n",
    "        if custom_system_prompt.strip():\n",
    "            system_prompt = custom_system_prompt.strip()\n",
    "        else:\n",
    "            system_prompt = SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "        \n",
    "        # If no message and no image, return\n",
    "        if not message.strip() and not image:\n",
    "            return history, \"\", None\n",
    "        \n",
    "        # Prepare user message\n",
    "        user_message = message.strip() if message.strip() else \"Please analyze this image.\"\n",
    "        user_display_message = user_message\n",
    "        \n",
    "        # Handle image if provided\n",
    "        if image:\n",
    "            image_html, image_key = create_image_display(image)\n",
    "            if image_html:\n",
    "                user_display_message = f\"{user_message}\\n\\n{image_html}\"\n",
    "        \n",
    "        # Add user message to history immediately\n",
    "        history.append([user_display_message, \"\"])\n",
    "        yield history, \"\", None\n",
    "        \n",
    "        # Get AI response\n",
    "        if image:\n",
    "            response = inference_with_image(\n",
    "                prompt=user_message,\n",
    "                image_path=image,\n",
    "                system_prompt=system_prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                min_pixels=min_pixels,\n",
    "                max_pixels=max_pixels\n",
    "            )\n",
    "        else:\n",
    "            # Text-only conversation\n",
    "            response = inference_text_only(\n",
    "                prompt=user_message,\n",
    "                system_prompt=system_prompt,\n",
    "                max_new_tokens=max_tokens\n",
    "            )\n",
    "        \n",
    "        # Stream the response\n",
    "        history[-1] = [user_display_message, \"\"]\n",
    "        for partial_response in simulate_streaming_response(response):\n",
    "            history[-1] = [user_display_message, partial_response]\n",
    "            yield history, \"\", None\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"‚ùå Error: {str(e)}\"\n",
    "        if history and len(history) > 0 and history[-1][1] == \"\":\n",
    "            history[-1] = [history[-1][0], error_msg]\n",
    "        else:\n",
    "            history.append([message if message else \"Error occurred\", error_msg])\n",
    "        yield history, \"\", None\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clear chat history and image storage\"\"\"\n",
    "    global image_storage\n",
    "    image_storage.clear()\n",
    "    return [], \"\", None\n",
    "\n",
    "def update_system_prompt(usage_mode: str) -> str:\n",
    "    \"\"\"Update system prompt based on usage mode\"\"\"\n",
    "    return SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "\n",
    "def toggle_sidebar():\n",
    "    \"\"\"Toggle sidebar visibility\"\"\"\n",
    "    return gr.update(visible=False), gr.update(visible=True)\n",
    "\n",
    "def show_sidebar():\n",
    "    \"\"\"Show sidebar\"\"\"\n",
    "    return gr.update(visible=True), gr.update(visible=False)\n",
    "\n",
    "# Create Gradio interface\n",
    "with gr.Blocks(css=custom_css, title=\"Qwen2.5 VL Chat\", theme=gr.themes.Base()) as demo:\n",
    "    \n",
    "    with gr.Row(elem_classes=\"main-container\"):\n",
    "        # Sidebar\n",
    "        with gr.Column(scale=1, elem_classes=\"sidebar\", visible=True) as sidebar:\n",
    "            gr.Markdown(\"### ‚öôÔ∏è Configuration\")\n",
    "            \n",
    "            # Hide sidebar button\n",
    "            hide_sidebar_btn = gr.Button(\"‚Üê Hide Panel\", size=\"sm\")\n",
    "            \n",
    "            usage_mode = gr.Dropdown(\n",
    "                choices=list(SYSTEM_PROMPTS.keys()),\n",
    "                value=\"General Assistant\",\n",
    "                label=\"üéØ Usage Mode\",\n",
    "                info=\"Select AI behavior\"\n",
    "            )\n",
    "            \n",
    "            custom_system_prompt = gr.Textbox(\n",
    "                label=\"üìù Custom System Prompt\",\n",
    "                placeholder=\"Override with custom prompt...\",\n",
    "                lines=3\n",
    "            )\n",
    "            \n",
    "            with gr.Accordion(\"üîß Advanced Settings\", open=False):\n",
    "                max_tokens = gr.Slider(\n",
    "                    minimum=100,\n",
    "                    maximum=32000,\n",
    "                    value=2000,\n",
    "                    step=100,\n",
    "                    label=\"Max Tokens\"\n",
    "                )\n",
    "                \n",
    "                min_pixels = gr.Number(\n",
    "                    value=512 * 28 * 28,\n",
    "                    label=\"Min Pixels\"\n",
    "                )\n",
    "                \n",
    "                max_pixels = gr.Number(\n",
    "                    value=2048 * 28 * 28,\n",
    "                    label=\"Max Pixels\"\n",
    "                )\n",
    "            \n",
    "            clear_btn = gr.Button(\"üóëÔ∏è Clear Chat\", elem_classes=\"clear-btn\")\n",
    "        \n",
    "        # Show sidebar button (hidden by default)\n",
    "        show_sidebar_btn = gr.Button(\"‚ò∞\", elem_classes=\"sidebar-toggle\", visible=False)\n",
    "        \n",
    "        # Main chat area\n",
    "        with gr.Column(scale=4, elem_classes=\"chat-container\"):\n",
    "            # Header\n",
    "            gr.Markdown(\"# ü§ñ Qwen2.5 VL Chat Assistant\", elem_classes=\"chat-header\")\n",
    "            \n",
    "            # Chatbot\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"\",\n",
    "                height=600,\n",
    "                show_copy_button=True,\n",
    "                elem_classes=\"chatbot\",\n",
    "                avatar_images=(\"üë§\", \"ü§ñ\"),\n",
    "                bubble_full_width=False,\n",
    "                render_markdown=True\n",
    "            )\n",
    "            \n",
    "            # Message input area\n",
    "            with gr.Row(elem_classes=\"message-input-container\"):\n",
    "                with gr.Column(scale=1):\n",
    "                    # Image attachment (optional)\n",
    "                    image_input = gr.File(\n",
    "                        file_types=[\"image\"],\n",
    "                        label=\"üìé Attach Image (Optional)\",\n",
    "                        elem_classes=\"file-upload\"\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=5):\n",
    "                    msg_input = gr.Textbox(\n",
    "                        placeholder=\"üí¨ Type your message here... (Image is optional)\",\n",
    "                        lines=2,\n",
    "                        max_lines=6,\n",
    "                        elem_classes=\"message-input\",\n",
    "                        show_label=False,\n",
    "                        container=False\n",
    "                    )\n",
    "                \n",
    "                with gr.Column(scale=1):\n",
    "                    send_btn = gr.Button(\"Send\", variant=\"primary\", elem_classes=\"send-button\")\n",
    "    \n",
    "    # Event handlers\n",
    "    \n",
    "    # Auto-update system prompt when usage mode changes\n",
    "    usage_mode.change(\n",
    "        fn=update_system_prompt,\n",
    "        inputs=[usage_mode],\n",
    "        outputs=[custom_system_prompt]\n",
    "    )\n",
    "    \n",
    "    # Sidebar toggle functions\n",
    "    hide_sidebar_btn.click(\n",
    "        fn=toggle_sidebar,\n",
    "        outputs=[sidebar, show_sidebar_btn]\n",
    "    )\n",
    "    \n",
    "    show_sidebar_btn.click(\n",
    "        fn=show_sidebar,\n",
    "        outputs=[sidebar, show_sidebar_btn]\n",
    "    )\n",
    "    \n",
    "    # Chat functions with real streaming\n",
    "    send_btn.click(\n",
    "        fn=chat_with_history,\n",
    "        inputs=[\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "    \n",
    "    msg_input.submit(\n",
    "        fn=chat_with_history,\n",
    "        inputs=[\n",
    "            msg_input,\n",
    "            image_input,\n",
    "            chatbot,\n",
    "            usage_mode,\n",
    "            custom_system_prompt,\n",
    "            max_tokens,\n",
    "            min_pixels,\n",
    "            max_pixels\n",
    "        ],\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(\n",
    "        fn=clear_history,\n",
    "        outputs=[chatbot, msg_input, image_input]\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        #server_name=\"0.0.0.0\",\n",
    "        #server_port=7860,\n",
    "        #share=False,\n",
    "        debug=True\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "omni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
