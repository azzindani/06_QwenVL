{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install gradio\n!pip install qwen-vl-utils\n!sudo apt-get update\n!sudo apt-get install poppler-utils","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## v1 - ok","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\nimport random\nimport io\nimport ast\nimport xml.etree.ElementTree as ET\nimport gradio as gr\nimport os\nimport tempfile # For creating temporary files\n\nfrom transformers import (\n    Qwen2_5_VLForConditionalGeneration,\n    AutoTokenizer,\n    AutoProcessor,\n    BitsAndBytesConfig,\n    TextStreamer\n)\n# Assuming qwen_vl_utils.py is in the same directory\nfrom qwen_vl_utils import process_vision_info\nfrom PIL import Image, ImageDraw, ImageFont, ImageColor\n\nmodel_path = 'Qwen/Qwen2.5-VL-3B-Instruct'\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    model_path,\n    device_map = 'auto',\n)\nprocessor = AutoProcessor.from_pretrained(model_path)\n\ndef plot_text_bounding_boxes(image: Image.Image, bounding_boxes, input_width, input_height):\n    \"\"\"\n    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n\n    Args:\n        image: The PIL Image object.\n        bounding_boxes: A list of bounding boxes containing the name of the object\n          and their positions in normalized [y1 x1 y2 x2] format.\n    Returns:\n        PIL.Image.Image: The image with bounding boxes drawn.\n    \"\"\"\n\n    width, height = image.size\n    # Create a drawing object\n    draw = ImageDraw.Draw(image)\n\n    # Parsing out the markdown fencing\n    bounding_boxes = parse_json(bounding_boxes)\n\n    # Ensure the font file exists and is accessible\n    try:\n        # Adjust font size based on image size for better visibility\n        font_size = max(10, min(int(height / 30), int(width / 60)))\n        font = ImageFont.truetype('./00_Dataset/NotoSansCJK-Regular.ttc', size = font_size)\n    except IOError:\n        print(\"Warning: NotoSansCJK-Regular.ttc not found. Using default font.\")\n        font = ImageFont.load_default()\n\n    # Iterate over the bounding boxes\n    for i, bounding_box in enumerate(ast.literal_eval(bounding_boxes)):\n      color = 'green'\n\n      # Convert normalized coordinates to absolute coordinates\n      abs_y1 = int(bounding_box[\"bbox_2d\"][1] / input_height * height)\n      abs_x1 = int(bounding_box[\"bbox_2d\"][0] / input_width * width)\n      abs_y2 = int(bounding_box[\"bbox_2d\"][3] / input_height * height)\n      abs_x2 = int(bounding_box[\"bbox_2d\"][2] / input_width * width)\n\n      # Ensure coordinates are in correct order (top-left to bottom-right)\n      if abs_x1 > abs_x2:\n        abs_x1, abs_x2 = abs_x2, abs_x1\n      if abs_y1 > abs_y2:\n        abs_y1, abs_y2 = abs_y2, abs_y1\n\n      # Draw the bounding box\n      draw.rectangle(\n          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline = color, width = 2 # Increased width for better visibility\n      )\n\n      # Draw the text\n      if 'text_content' in bounding_box:\n        # Position text just below the bounding box\n        draw.text((abs_x1, abs_y2), bounding_box['text_content'], fill = color, font = font)\n\n    return image\n\n# @title Parsing JSON output\ndef parse_json(json_output):\n    # Parsing out the markdown fencing\n    lines = json_output.splitlines()\n    for i, line in enumerate(lines):\n        if line == \"```json\":\n            json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n            json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n            break  # Exit the loop once \"```json\" is found\n    return json_output\n\ndef inference(\n    prompt,\n    image_path: str, # Now accepts a file path (string)\n    system_prompt = 'You are a helpful assistant',\n    max_new_tokens = 4096,\n    min_pixels = 512 * 28 * 28,\n    max_pixels = 2048 * 28 * 28,\n):\n    messages = [\n        {\n            'role' : 'user',\n            'content' : [\n                {\n                    'type' : 'image',\n                    'image' : image_path, # Pass the file path directly\n                    'min_pixels' : min_pixels,\n                    'max_pixels' : max_pixels,\n                },\n                {'type' : 'text', 'text' : prompt},\n            ],\n        }\n    ]\n\n    # Preparation for inference\n    text = processor.apply_chat_template(\n        messages, tokenize = False, add_generation_prompt = True\n    )\n    print('input:\\n', text)\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text = [text],\n        images = image_inputs,\n        videos = video_inputs,\n        padding = True,\n        return_tensors = 'pt',\n    )\n    inputs = inputs.to('cuda')\n\n    streamer = TextStreamer(processor.tokenizer, skip_special_tokens = True, skip_prompt = True)\n\n    # Inference: Generation of the output\n    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens, streamer = streamer)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n    )\n\n    print('output:\\n', output_text[0])\n\n    input_height = inputs['image_grid_thw'][0][1] * 14\n    input_width = inputs['image_grid_thw'][0][2] * 14\n\n    return output_text[0], input_width, input_height\n\ndef ocr_and_display(image: Image.Image):\n    \"\"\"\n    Gradio interface function to perform OCR and display before/after images.\n    \"\"\"\n    if image is None:\n        return None, None, \"Please upload an image.\"\n\n    # Save the PIL Image to a temporary file\n    with tempfile.NamedTemporaryFile(suffix=\".png\", delete=False) as tmp_file:\n        image_path = tmp_file.name\n        image.save(image_path)\n\n    try:\n        prompt = 'Spotting all the text in the image with line-level, and output in JSON format.'\n        ocr_response, input_width, input_height = inference(prompt, image_path)\n\n        # Create the image with bounding boxes (operate on a copy to avoid modifying the original)\n        image_with_boxes = plot_text_bounding_boxes(image.copy(), ocr_response, input_width, input_height)\n\n        # Extract plain text from the OCR response\n        try:\n            parsed_boxes = ast.literal_eval(parse_json(ocr_response))\n            extracted_text = \"\\n\".join([item.get('text_content', '') for item in parsed_boxes if 'text_content' in item])\n        except Exception as e:\n            extracted_text = f\"Error parsing OCR response: {e}\\nRaw response:\\n{ocr_response}\"\n\n        return image_with_boxes, extracted_text # Only return the OCR'd image and text\n\n    finally:\n        # Clean up the temporary file\n        os.remove(image_path)\n\n# Create the Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(\n        \"\"\"\n        # Qwen2.5-VL OCR Interface\n        Upload an image to perform OCR and visualize the detected text with bounding boxes.\n        \"\"\"\n    )\n    with gr.Row():\n        with gr.Column():\n            image_input = gr.Image(type=\"pil\", label=\"Upload Image\")\n            ocr_button = gr.Button(\"Perform OCR\")\n        with gr.Column():\n            # Removed the separate \"Original Image\" display as the input already serves this purpose\n            image_ocr = gr.Image(label=\"OCR with Bounding Boxes\")\n    with gr.Row():\n        ocr_text_output = gr.Textbox(label=\"Extracted Text\", lines=10)\n\n    ocr_button.click(\n        ocr_and_display,\n        inputs=[image_input],\n        outputs=[image_ocr, ocr_text_output] # Updated outputs\n    )\n\ndemo.launch()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## v2 - ok","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\nimport random\nimport io\nimport ast\nimport xml.etree.ElementTree as ET\nimport gradio as gr\nimport os\nimport tempfile\nfrom pdf2image import convert_from_path # Import pdf2image\n\nfrom transformers import (\n    Qwen2_5_VLForConditionalGeneration,\n    AutoTokenizer,\n    AutoProcessor,\n    BitsAndBytesConfig,\n    TextStreamer\n)\nfrom qwen_vl_utils import process_vision_info\nfrom PIL import Image, ImageDraw, ImageFont, ImageColor\n\nmodel_path = 'Qwen/Qwen2.5-VL-3B-Instruct'\n\nmodel = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n    model_path,\n    device_map = 'auto',\n)\nprocessor = AutoProcessor.from_pretrained(model_path)\n\ndef plot_text_bounding_boxes(image: Image.Image, bounding_boxes, input_width, input_height):\n    \"\"\"\n    Plots bounding boxes on an image with markers for each a name, using PIL, normalized coordinates, and different colors.\n\n    Args:\n        image: The PIL Image object.\n        bounding_boxes: A list of bounding boxes containing the name of the object\n          and their positions in normalized [y1 x1 y2 x2] format.\n    Returns:\n        PIL.Image.Image: The image with bounding boxes drawn.\n    \"\"\"\n\n    width, height = image.size\n    draw = ImageDraw.Draw(image)\n\n    # Parsing out the markdown fencing\n    parsed_json_str = parse_json(bounding_boxes)\n\n    try:\n        font_size = max(10, min(int(height / 30), int(width / 60)))\n        # IMPORTANT: Ensure this font path is correct in Kaggle or handle its absence\n        font = ImageFont.truetype('./00_Dataset/NotoSansCJK-Regular.ttc', size = font_size)\n    except IOError:\n        print(\"Warning: NotoSansCJK-Regular.ttc not found. Using default font.\")\n        font = ImageFont.load_default()\n\n    try:\n        # Safely evaluate the literal. It should be a list of dicts.\n        bounding_boxes_list = ast.literal_eval(parsed_json_str)\n        if not isinstance(bounding_boxes_list, list):\n            print(f\"Warning: Expected a list from ast.literal_eval, but got {type(bounding_boxes_list)}. Raw parsed JSON: {parsed_json_str}\")\n            bounding_boxes_list = [] # Treat as empty if not a list\n    except (ValueError, SyntaxError) as e:\n        print(f\"Error evaluating bounding boxes string: {e}\")\n        print(f\"Problematic string for bounding boxes: {parsed_json_str[:500]}...\") # Print first 500 chars for debugging\n        bounding_boxes_list = [] # Fallback to empty list on error\n\n    for i, bounding_box in enumerate(bounding_boxes_list):\n      color = 'green'\n\n      # Ensure bounding_box is a dictionary and has 'bbox_2d'\n      if not isinstance(bounding_box, dict) or 'bbox_2d' not in bounding_box or not isinstance(bounding_box['bbox_2d'], list) or len(bounding_box['bbox_2d']) != 4:\n          print(f\"Skipping malformed bounding box entry: {bounding_box}\")\n          continue\n\n      abs_y1 = int(bounding_box[\"bbox_2d\"][1] / input_height * height)\n      abs_x1 = int(bounding_box[\"bbox_2d\"][0] / input_width * width)\n      abs_y2 = int(bounding_box[\"bbox_2d\"][3] / input_height * height)\n      abs_x2 = int(bounding_box[\"bbox_2d\"][2] / input_width * width)\n\n      if abs_x1 > abs_x2:\n        abs_x1, abs_x2 = abs_x2, abs_x1\n      if abs_y1 > abs_y2:\n        abs_y1, abs_y2 = abs_y2, abs_y1\n\n      draw.rectangle(\n          ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline = color, width = 2\n      )\n\n      if 'text_content' in bounding_box:\n        draw.text((abs_x1, abs_y2), bounding_box['text_content'], fill = color, font = font)\n\n    return image\n\ndef parse_json(json_output):\n    # This function is correct for removing markdown fences\n    lines = json_output.splitlines()\n    for i, line in enumerate(lines):\n        if line == \"```json\":\n            json_output = \"\\n\".join(lines[i+1:])\n            json_output = json_output.split(\"```\")[0]\n            break\n    return json_output\n\ndef inference(\n    prompt,\n    image_path: str,\n    system_prompt = 'You are a helpful assistant',\n    max_new_tokens = 4096,\n    min_pixels = 512 * 28 * 28,\n    max_pixels = 2048 * 28 * 28,\n):\n    messages = [\n        {\n            'role' : 'user',\n            'content' : [\n                {\n                    'type' : 'image',\n                    'image' : image_path,\n                    'min_pixels' : min_pixels,\n                    'max_pixels' : max_pixels,\n                },\n                {'type' : 'text', 'text' : prompt},\n            ],\n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages, tokenize = False, add_generation_prompt = True\n    )\n    print('Input to model:\\n', text) # Changed for clarity\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text = [text],\n        images = image_inputs,\n        videos = video_inputs,\n        padding = True,\n        return_tensors = 'pt',\n    )\n    inputs = inputs.to('cuda')\n\n    streamer = TextStreamer(processor.tokenizer, skip_special_tokens = True, skip_prompt = True)\n\n    generated_ids = model.generate(**inputs, max_new_tokens = max_new_tokens, streamer = streamer)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens = True, clean_up_tokenization_spaces = False\n    )\n\n    print('Output from model:\\n', output_text[0]) # Changed for clarity\n\n    input_height = inputs['image_grid_thw'][0][1] * 14\n    input_width = inputs['image_grid_thw'][0][2] * 14\n\n    return output_text[0], input_width, input_height\n\ndef process_pdf_for_ocr(file):\n    if file is None:\n        return [], [], \"Please upload a PDF file.\"\n\n    input_file_path = file.name\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        all_ocr_texts = []\n        all_ocr_images = [] # For images with bounding boxes\n        original_images = [] # For original PDF pages\n\n        try:\n            # You might need to specify poppler_path if not in system PATH on Windows\n            # For Kaggle, usually poppler is pre-installed. If not, you might need to\n            # add a custom installation step or ensure it's in the Docker image.\n            images_from_pdf = convert_from_path(input_file_path)\n\n            for i, pil_image_orig in enumerate(images_from_pdf):\n                # Save original for display\n                original_images.append(pil_image_orig)\n\n                # Save each PIL image to a temporary file for inference\n                temp_image_path = os.path.join(temp_dir, f\"page_{i}.png\")\n                pil_image_orig.save(temp_image_path, format=\"PNG\")\n\n                prompt = 'Spotting all the text in the image with line-level, and output in JSON format.'\n                ocr_response, input_width, input_height = inference(prompt, temp_image_path)\n\n                # --- Robust Parsing for OCR Text and Bounding Boxes ---\n                page_extracted_text = f\"--- Page {i+1} ---\\n\"\n                current_page_image_with_boxes = pil_image_orig.copy() # Make a copy for drawing\n\n                try:\n                    parsed_json_str = parse_json(ocr_response)\n                    parsed_boxes = ast.literal_eval(parsed_json_str)\n\n                    if isinstance(parsed_boxes, list):\n                        # This is the expected format: a list of dictionaries\n                        texts_on_page = []\n                        for item in parsed_boxes:\n                            if isinstance(item, dict) and 'text_content' in item:\n                                texts_on_page.append(item['text_content'])\n                        page_extracted_text += \"\\n\".join(texts_on_page)\n\n                        # Plot bounding boxes only if parsing was successful and we have a list\n                        current_page_image_with_boxes = plot_text_bounding_boxes(\n                            current_page_image_with_boxes, ocr_response, input_width, input_height\n                        )\n                    else:\n                        # If it's not a list, it's an unexpected format from the model\n                        page_extracted_text += f\"OCR response was not a list of text objects. Raw:\\n{parsed_json_str}\"\n                        print(f\"Warning: OCR response for page {i+1} was not a list: {parsed_json_str}\")\n\n                except (ValueError, SyntaxError) as e:\n                    # Catch errors from ast.literal_eval if the string isn't valid Python literal\n                    page_extracted_text += f\"Error parsing OCR JSON for page {i+1}: {e}\\nRaw response (after fence removal):\\n{parsed_json_str}\"\n                    print(f\"Error parsing OCR JSON for page {i+1}: {e}\\nRaw response (after fence removal):\\n{parsed_json_str}\")\n                except Exception as e:\n                    # Catch any other unexpected errors during text extraction\n                    page_extracted_text += f\"An unexpected error occurred during OCR text extraction for page {i+1}: {e}\"\n                    print(f\"An unexpected error occurred during OCR text extraction for page {i+1}: {e}\")\n\n                all_ocr_texts.append(page_extracted_text)\n                all_ocr_images.append(current_page_image_with_boxes)\n\n        except Exception as e:\n            print(f\"Fatal Error processing PDF: {e}\")\n            # If pdf2image itself fails or there's an issue with the temp file\n            return [], [], f\"Fatal Error processing PDF: {e}\"\n\n        return original_images, all_ocr_images, \"\\n\\n\".join(all_ocr_texts)\n\n# Create the Gradio interface\nwith gr.Blocks() as demo:\n    gr.Markdown(\n        \"\"\"\n        # Qwen2.5-VL PDF OCR Interface\n        Upload a PDF file to perform OCR on each page and visualize the detected text with bounding boxes.\n        \"\"\"\n    )\n    with gr.Row():\n        with gr.Column():\n            pdf_input = gr.File(label=\"Upload PDF File\", file_types=[\".pdf\"])\n            ocr_button = gr.Button(\"Perform OCR\")\n        with gr.Column():\n            original_pages_gallery = gr.Gallery(\n                label=\"Original PDF Pages\",\n                show_label=True,\n                elem_id=\"gallery\",\n                columns=2,\n                rows=2,\n                object_fit=\"contain\",\n                height=\"auto\",\n            )\n            ocr_pages_gallery = gr.Gallery(\n                label=\"OCR with Bounding Boxes (Each Page)\",\n                show_label=True,\n                elem_id=\"gallery_ocr\",\n                columns=2,\n                rows=2,\n                object_fit=\"contain\",\n                height=\"auto\",\n            )\n    with gr.Row():\n        ocr_text_output = gr.Textbox(label=\"Extracted Text (All Pages)\", lines=20)\n\n    ocr_button.click(\n        process_pdf_for_ocr,\n        inputs=[pdf_input],\n        outputs=[original_pages_gallery, ocr_pages_gallery, ocr_text_output]\n    )\n\ndemo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## v3 - ok","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\nimport random\nimport io\nimport ast\nimport xml.etree.ElementTree as ET\nimport gradio as gr\nimport os\nimport tempfile\nfrom pdf2image import convert_from_path # Import pdf2image\nimport gc # Import garbage collection module\n\nfrom transformers import (\n    Qwen2_5_VLForConditionalGeneration,\n    AutoTokenizer,\n    AutoProcessor,\n    BitsAndBytesConfig,\n    TextStreamer\n)\nfrom qwen_vl_utils import process_vision_info\nfrom PIL import Image, ImageDraw, ImageFont, ImageColor\n\n# --- Model Loading (Moved to top level) ---\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntry:\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        'Qwen/Qwen2.5-VL-3B-Instruct',\n        device_map='auto',\n    )#.to(device)\n    processor = AutoProcessor.from_pretrained('Qwen/Qwen2.5-VL-3B-Instruct')\n    print(\"Model and Processor loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading model or processor: {e}\")\n    print(\"Please ensure 'Qwen/Qwen2.5-VL-3B-Instruct' is accessible and properly configured.\")\n    model = None\n    processor = None\n\n# --- Helper Functions ---\ndef plot_text_bounding_boxes(image: Image.Image, bounding_boxes_str: str, input_width, input_height):\n    \"\"\"\n    Plots bounding boxes on an image with markers for each text, using PIL, normalized coordinates, and different colors.\n    \"\"\"\n    if not image:\n        return None\n\n    width, height = image.size\n    draw = ImageDraw.Draw(image)\n\n    parsed_json_str = parse_json(bounding_boxes_str)\n\n    try:\n        font_path = './00_Dataset/NotoSansCJK-Regular.ttc'\n        if os.path.exists(font_path):\n            font_size = max(10, min(int(height / 30), int(width / 60)))\n            font = ImageFont.truetype(font_path, size=font_size)\n        else:\n            print(f\"Warning: Font file '{font_path}' not found. Using default font.\")\n            font = ImageFont.load_default()\n    except IOError:\n        print(f\"Warning: Error loading font from '{font_path}'. Using default font.\")\n        font = ImageFont.load_default()\n\n    try:\n        bounding_boxes_list = ast.literal_eval(parsed_json_str)\n        if not isinstance(bounding_boxes_list, list):\n            print(f\"Warning: Expected a list from ast.literal_eval, but got {type(bounding_boxes_list)}. Raw parsed JSON: {parsed_json_str[:200]}...\")\n            bounding_boxes_list = []\n    except (ValueError, SyntaxError) as e:\n        print(f\"Error evaluating bounding boxes string: {e}\")\n        print(f\"Problematic string for bounding boxes: {parsed_json_str[:500]}...\")\n        bounding_boxes_list = []\n\n    for bounding_box in bounding_boxes_list:\n        color = 'green'\n        if not isinstance(bounding_box, dict) or 'bbox_2d' not in bounding_box or not isinstance(bounding_box['bbox_2d'], list) or len(bounding_box['bbox_2d']) != 4:\n            print(f\"Skipping malformed bounding box entry: {bounding_box}\")\n            continue\n\n        abs_y1 = int(bounding_box[\"bbox_2d\"][1] / input_height * height)\n        abs_x1 = int(bounding_box[\"bbox_2d\"][0] / input_width * width)\n        abs_y2 = int(bounding_box[\"bbox_2d\"][3] / input_height * height)\n        abs_x2 = int(bounding_box[\"bbox_2d\"][2] / input_width * width)\n\n        abs_x1, abs_x2 = min(abs_x1, abs_x2), max(abs_x1, abs_x2)\n        abs_y1, abs_y2 = min(abs_y1, abs_y2), max(abs_y1, abs_y2)\n\n        draw.rectangle(\n            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=2\n        )\n\n        if 'text_content' in bounding_box:\n            draw.text((abs_x1, abs_y2), bounding_box['text_content'], fill=color, font=font)\n\n    return image\n\ndef parse_json(json_output: str) -> str:\n    \"\"\"Removes markdown fencing from JSON string.\"\"\"\n    lines = json_output.splitlines()\n    for i, line in enumerate(lines):\n        if line.strip() == \"```json\":\n            json_output = \"\\n\".join(lines[i+1:])\n            json_output = json_output.split(\"```\")[0].strip()\n            break\n    return json_output\n\ndef inference(\n    prompt: str,\n    image_path: str,\n    system_prompt: str = 'You are a helpful assistant', # Kept as requested\n    max_new_tokens: int = 4096,\n    min_pixels: int = 512 * 28 * 28,\n    max_pixels: int = 2048 * 28 * 28,\n):\n    if model is None or processor is None:\n        return \"Error: Model not loaded.\", 0, 0\n\n    messages = [\n        {\n            'role': 'user',\n            'content': [\n                {\n                    'type': 'image',\n                    'image': image_path,\n                    'min_pixels': min_pixels,\n                    'max_pixels': max_pixels,\n                },\n                {'type': 'text', 'text': prompt},\n            ],\n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    print('Input to model:\\n', text)\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors='pt',\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    streamer = TextStreamer(processor.tokenizer, skip_special_tokens=True, skip_prompt=True)\n\n    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, streamer=streamer)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n\n    print('Output from model:\\n', output_text[0])\n\n    input_height = 0\n    input_width = 0\n    if 'image_grid_thw' in inputs and len(inputs['image_grid_thw']) > 0:\n        if isinstance(inputs['image_grid_thw'][0], torch.Tensor) and inputs['image_grid_thw'][0].dim() == 1 and inputs['image_grid_thw'][0].shape[0] >= 3:\n             input_height = inputs['image_grid_thw'][0][1].item() * 14\n             input_width = inputs['image_grid_thw'][0][2].item() * 14\n        else:\n            print(f\"Warning: Unexpected format for image_grid_thw: {inputs['image_grid_thw']}\")\n\n    return output_text[0], input_width, input_height\n\ndef process_document(file, resolution: int):\n    \"\"\"\n    Handles PDF or Image input: processes it for OCR and returns results.\n    \"\"\"\n    if file is None:\n        return [], [], \"\", \"\", \"Please upload a file (PDF or Image).\"\n\n    input_file_path = file.name\n    file_extension = os.path.splitext(input_file_path)[1].lower()\n\n    all_ocr_texts_plain = []\n    all_ocr_texts_full = []\n    all_ocr_images = []\n    original_images = []\n    status_message = \"Processing complete.\"\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            if file_extension == '.pdf':\n                print(f\"Processing PDF with DPI: {resolution}\")\n                images_from_doc = convert_from_path(input_file_path, dpi=resolution)\n            elif file_extension in ['.png', '.jpg', '.jpeg']:\n                print(f\"Processing Image: {input_file_path}\")\n                images_from_doc = [Image.open(input_file_path).convert(\"RGB\")] # Ensure RGB for consistency\n            else:\n                return [], [], \"\", \"\", \"Unsupported file type. Please upload a PDF or an image (PNG, JPG).\"\n\n            if not images_from_doc:\n                return [], [], \"No content found in document.\", \"No content found in document.\", \"No content found in document.\"\n\n            for i, pil_image_orig in enumerate(images_from_doc):\n                original_images.append(pil_image_orig)\n\n                temp_image_path = os.path.join(temp_dir, f\"page_{i}.png\")\n                pil_image_orig.save(temp_image_path, format=\"PNG\")\n\n                prompt = 'Spotting all the text in the image with line-level, and output in JSON format.'\n                ocr_response, input_width, input_height = inference(prompt, temp_image_path)\n\n                page_label = f\"--- Page {i+1}\" if file_extension == '.pdf' else \"--- Image\"\n                page_extracted_text_plain = f\"{page_label} (Plain Text) ---\\n\"\n                page_extracted_text_full = f\"{page_label} (Raw OCR Output) ---\\n\"\n                current_page_image_with_boxes = pil_image_orig.copy()\n\n                page_extracted_text_full += ocr_response + \"\\n\\n\"\n\n                try:\n                    parsed_json_str = parse_json(ocr_response)\n                    parsed_boxes = ast.literal_eval(parsed_json_str)\n\n                    if isinstance(parsed_boxes, list):\n                        texts_on_page = []\n                        for item in parsed_boxes:\n                            if isinstance(item, dict) and 'text_content' in item:\n                                texts_on_page.append(item['text_content'])\n                        page_extracted_text_plain += \"\\n\".join(texts_on_page)\n\n                        current_page_image_with_boxes = plot_text_bounding_boxes(\n                            current_page_image_with_boxes, ocr_response, input_width, input_height\n                        )\n                    else:\n                        page_extracted_text_plain += f\"OCR response was not a list of text objects. Raw:\\n{parsed_json_str}\"\n                        print(f\"Warning: OCR response for {page_label} was not a list: {parsed_json_str}\")\n\n                except (ValueError, SyntaxError) as e:\n                    page_extracted_text_plain += f\"Error parsing OCR JSON for {page_label}: {e}\\nRaw response (after fence removal):\\n{parsed_json_str}\"\n                    print(f\"Error parsing OCR JSON for {page_label}: {e}\\nRaw response (after fence removal):\\n{parsed_json_str}\")\n                except Exception as e:\n                    page_extracted_text_plain += f\"An unexpected error occurred during OCR text extraction for {page_label}: {e}\"\n                    print(f\"An unexpected error occurred during OCR text extraction for {page_label}: {e}\")\n\n                all_ocr_texts_plain.append(page_extracted_text_plain)\n                all_ocr_texts_full.append(page_extracted_text_full)\n                all_ocr_images.append(current_page_image_with_boxes)\n\n        except Exception as e:\n            error_msg = f\"Fatal Error processing file: {e}\"\n            print(error_msg)\n            return [], [], error_msg, error_msg, error_msg\n\n    return original_images, all_ocr_images, \"\\n\\n\".join(all_ocr_texts_plain), \"\\n\\n\".join(all_ocr_texts_full), status_message\n\ndef clear_vram():\n    \"\"\"Clears PyTorch's CUDA memory cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect() # Aggressively collect garbage\n        print(\"CUDA VRAM cache emptied and garbage collected.\")\n        return \"VRAM Cleared!\"\n    else:\n        return \"No CUDA device found to clear VRAM.\"\n\n# --- Gradio Interface ---\nwith gr.Blocks(title=\"Qwen2.5-VL PDF/Image OCR Interface\") as demo:\n    gr.Markdown(\n        \"\"\"\n        # Qwen2.5-VL PDF/Image OCR Interface\n        Upload a PDF file or an image to perform OCR and visualize the detected text with bounding boxes.\n        Adjust the PDF conversion resolution for better OCR results.\n        \"\"\"\n    )\n\n    # --- Tabs at the very top ---\n    with gr.Tabs():\n        with gr.TabItem(\"Preview Images\"):\n            gr.Markdown(\"### Original vs. OCR'd Pages\")\n            with gr.Row():\n                original_pages_gallery = gr.Gallery(\n                    label=\"Original Document Pages\",\n                    show_label=True,\n                    elem_id=\"gallery_original\",\n                    columns=2,\n                    rows=2,\n                    object_fit=\"contain\",\n                    height=\"auto\",\n                )\n                ocr_pages_gallery = gr.Gallery(\n                    label=\"OCR with Bounding Boxes\",\n                    show_label=True,\n                    elem_id=\"gallery_ocr\",\n                    columns=2,\n                    rows=2,\n                    object_fit=\"contain\",\n                    height=\"auto\",\n                )\n        with gr.TabItem(\"Extracted Plain Text\"):\n            gr.Markdown(\"### Plain Text Output\")\n            ocr_text_plain_output = gr.Textbox(\n                label=\"Extracted Text (All Pages/Image)\", lines=20, interactive=False\n            )\n        with gr.TabItem(\"Complete OCR Output (JSON/Raw)\"):\n            gr.Markdown(\"### Raw Model Output with Bounding Box Data\")\n            ocr_text_full_output = gr.Textbox(\n                label=\"Complete OCR Output (Includes Bounding Box JSON)\",\n                lines=20,\n                interactive=False,\n            )\n\n    # --- Upload and Settings at the bottom ---\n    with gr.Row():\n        with gr.Column():\n            with gr.Group():\n                gr.Markdown(\"### 1. Upload Document\")\n                # Updated file_types to accept images as well\n                pdf_image_input = gr.File(label=\"Upload PDF or Image File\", file_types=[\".pdf\", \".png\", \".jpg\", \".jpeg\"])\n                gr.Markdown(\"### 2. OCR Settings\")\n                resolution_slider = gr.Slider(\n                    minimum=72, maximum=600, value=200, step=10,\n                    label=\"PDF to Image Resolution (DPI)\",\n                    info=\"Higher DPI can improve OCR accuracy but uses more memory/time. (Ignored for direct image uploads)\"\n                )\n                ocr_button = gr.Button(\"Perform OCR\", variant=\"primary\")\n            gr.Markdown(\"---\")\n            with gr.Row():\n                vram_reset_button = gr.Button(\"Reset VRAM\", variant=\"secondary\")\n                vram_status_output = gr.Textbox(label=\"VRAM Status\", interactive=False, max_lines=1)\n\n    # Event Listener for processing\n    ocr_button.click(\n        process_document, # Changed to the new function name\n        inputs=[pdf_image_input, resolution_slider],\n        outputs=[original_pages_gallery, ocr_pages_gallery, ocr_text_plain_output, ocr_text_full_output, vram_status_output]\n    )\n\n    # Event Listener for VRAM reset\n    vram_reset_button.click(\n        clear_vram,\n        inputs=[],\n        outputs=[vram_status_output]\n    )\n\ndemo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## v4 - ok","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\nimport random\nimport io\nimport ast\nimport xml.etree.ElementTree as ET\nimport gradio as gr\nimport os\nimport tempfile\nfrom pdf2image import convert_from_path # Used for PDF to image conversion\nimport gc # Import garbage collection module\n\nfrom transformers import (\n    Qwen2_5_VLForConditionalGeneration,\n    AutoTokenizer,\n    AutoProcessor,\n    BitsAndBytesConfig,\n    TextStreamer\n)\nfrom qwen_vl_utils import process_vision_info # Assuming this file is available\nfrom PIL import Image, ImageDraw, ImageFont, ImageColor\n\n# --- Model Loading (Moved to top level) ---\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntry:\n    # IMPORTANT: Ensure 'Qwen/Qwen2.5-VL-3B-Instruct' is accessible.\n    # On Kaggle, this typically means adding it as a dataset.\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        'Qwen/Qwen2.5-VL-3B-Instruct',\n        device_map='auto',\n    )#.to(device)\n    processor = AutoProcessor.from_pretrained('Qwen/Qwen2.5-VL-3B-Instruct')\n    print(\"Model and Processor loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading model or processor: {e}\")\n    print(\"Please ensure 'Qwen/Qwen2.5-VL-3B-Instruct' is accessible and properly configured.\")\n    # Fallback or exit strategy if model fails to load\n    model = None\n    processor = None\n\n# --- Helper Functions ---\ndef plot_text_bounding_boxes(image: Image.Image, bounding_boxes_str: str, input_width, input_height):\n    \"\"\"\n    Plots bounding boxes on an image with markers for each text, using PIL, normalized coordinates, and different colors.\n    \"\"\"\n    if not image:\n        return None\n\n    width, height = image.size\n    draw = ImageDraw.Draw(image)\n\n    parsed_json_str = parse_json(bounding_boxes_str)\n\n    try:\n        # Check if the font path is available on Kaggle. If not, default will be used.\n        # This assumes NotoSansCJK-Regular.ttc is in a Kaggle dataset mounted at ./00_Dataset/\n        font_path = './00_Dataset/NotoSansCJK-Regular.ttc'\n        if os.path.exists(font_path):\n            font_size = max(10, min(int(height / 30), int(width / 60)))\n            font = ImageFont.truetype(font_path, size=font_size)\n        else:\n            print(f\"Warning: Font file '{font_path}' not found. Using default font.\")\n            font = ImageFont.load_default()\n    except IOError:\n        print(f\"Warning: Error loading font from '{font_path}'. Using default font.\")\n        font = ImageFont.load_default()\n\n    try:\n        bounding_boxes_list = ast.literal_eval(parsed_json_str)\n        if not isinstance(bounding_boxes_list, list):\n            print(f\"Warning: Expected a list from ast.literal_eval, but got {type(bounding_boxes_list)}. Raw parsed JSON: {parsed_json_str[:200]}...\")\n            bounding_boxes_list = []\n    except (ValueError, SyntaxError) as e:\n        print(f\"Error evaluating bounding boxes string: {e}\")\n        print(f\"Problematic string for bounding boxes: {parsed_json_str[:500]}...\")\n        bounding_boxes_list = []\n\n    for bounding_box in bounding_boxes_list:\n        color = 'green'\n        if not isinstance(bounding_box, dict) or 'bbox_2d' not in bounding_box or not isinstance(bounding_box['bbox_2d'], list) or len(bounding_box['bbox_2d']) != 4:\n            print(f\"Skipping malformed bounding box entry: {bounding_box}\")\n            continue\n\n        abs_y1 = int(bounding_box[\"bbox_2d\"][1] / input_height * height)\n        abs_x1 = int(bounding_box[\"bbox_2d\"][0] / input_width * width)\n        abs_y2 = int(bounding_box[\"bbox_2d\"][3] / input_height * height)\n        abs_x2 = int(bounding_box[\"bbox_2d\"][2] / input_width * width)\n\n        # Ensure coordinates are in correct order\n        abs_x1, abs_x2 = min(abs_x1, abs_x2), max(abs_x1, abs_x2)\n        abs_y1, abs_y2 = min(abs_y1, abs_y2), max(abs_y1, abs_y2)\n\n        draw.rectangle(\n            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=2\n        )\n\n        if 'text_content' in bounding_box:\n            draw.text((abs_x1, abs_y2), bounding_box['text_content'], fill=color, font=font)\n\n    return image\n\ndef parse_json(json_output: str) -> str:\n    \"\"\"Removes markdown fencing from JSON string.\"\"\"\n    lines = json_output.splitlines()\n    for i, line in enumerate(lines):\n        if line.strip() == \"```json\":\n            json_output = \"\\n\".join(lines[i+1:])\n            json_output = json_output.split(\"```\")[0].strip()\n            break\n    return json_output\n\ndef inference(\n    prompt: str,\n    image_path: str,\n    system_prompt = 'You are a helpful assistant', # Kept as requested\n    max_new_tokens: int = 4096,\n    min_pixels: int = 512 * 28 * 28,\n    max_pixels: int = 2048 * 28 * 28,\n):\n    if model is None or processor is None:\n        return \"Error: Model not loaded.\", 0, 0\n\n    messages = [\n        {\n            'role': 'user',\n            'content': [\n                {\n                    'type': 'image',\n                    'image': image_path,\n                    'min_pixels': min_pixels,\n                    'max_pixels': max_pixels,\n                },\n                {'type': 'text', 'text': prompt},\n            ],\n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    print('Input to model:\\n', text)\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors='pt',\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    streamer = TextStreamer(processor.tokenizer, skip_special_tokens=True, skip_prompt=True)\n\n    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, streamer=streamer)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids) # Corrected dict access\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n\n    print('Output from model:\\n', output_text[0])\n\n    input_height = 0\n    input_width = 0\n    if 'image_grid_thw' in inputs and len(inputs['image_grid_thw']) > 0:\n        if isinstance(inputs['image_grid_thw'][0], torch.Tensor) and inputs['image_grid_thw'][0].dim() == 1 and inputs['image_grid_thw'][0].shape[0] >= 3:\n             input_height = inputs['image_grid_thw'][0][1].item() * 14\n             input_width = inputs['image_grid_thw'][0][2].item() * 14\n        else:\n            print(f\"Warning: Unexpected format for image_grid_thw: {inputs['image_grid_thw']}\")\n\n    return output_text[0], input_width, input_height\n\n\ndef process_file_for_ocr(file, resolution: int):\n    \"\"\"\n    Handles PDF or image input: converts pages to images (if PDF),\n    performs OCR, and returns lists of images and texts.\n    \"\"\"\n    if file is None:\n        return [], [], \"\", \"\", \"Please upload a PDF or image file.\"\n\n    input_file_path = file.name\n    file_extension = os.path.splitext(input_file_path)[1].lower()\n\n    all_ocr_texts_plain = []\n    all_ocr_texts_full = [] # To store the complete JSON/raw output\n    all_ocr_images = []\n    original_images = []\n    status_message = \"Processing complete.\"\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            images_to_process = []\n            if file_extension == '.pdf':\n                print(f\"Processing PDF with DPI: {resolution}\")\n                images_to_process = convert_from_path(input_file_path, dpi=resolution)\n            elif file_extension in ['.png', '.jpg', '.jpeg', '.webp', '.bmp', '.tiff', '.tif']: # Common image formats\n                print(f\"Processing image file: {input_file_path}\")\n                # For images, we just have one \"page\"\n                try:\n                    pil_image = Image.open(input_file_path).convert(\"RGB\") # Ensure RGB for consistent processing\n                    images_to_process = [pil_image]\n                except Exception as img_e:\n                    raise ValueError(f\"Could not open image file. Is it a valid image? Error: {img_e}\")\n            else:\n                return [], [], \"\", \"\", f\"Unsupported file type: {file_extension}. Please upload a PDF or a common image format (.png, .jpg, .jpeg, etc.).\"\n\n            if not images_to_process:\n                return [], [], \"No content found in file.\", \"No content found in file.\", \"No content found in file.\"\n\n            for i, pil_image_orig in enumerate(images_to_process):\n                original_images.append(pil_image_orig)\n\n                # Save each PIL image to a temporary file for inference\n                # Even for single images, saving to temp file is good practice for consistent API with LLM\n                temp_image_path = os.path.join(temp_dir, f\"page_{i}.png\")\n                pil_image_orig.save(temp_image_path, format=\"PNG\")\n\n                prompt = 'Spotting all the text in the image with line-level, and output in JSON format.'\n                ocr_response, input_width, input_height = inference(prompt, temp_image_path)\n\n                # --- Robust Parsing for OCR Text and Bounding Boxes ---\n                page_extracted_text_plain = f\"--- Page {i+1} (Plain Text) ---\\n\"\n                page_extracted_text_full = f\"--- Page {i+1} (Raw OCR Output) ---\\n\"\n                current_page_image_with_boxes = pil_image_orig.copy() # Make a copy for drawing\n\n                page_extracted_text_full += ocr_response + \"\\n\\n\" # Store raw output\n\n                try:\n                    parsed_json_str = parse_json(ocr_response)\n                    parsed_boxes = ast.literal_eval(parsed_json_str)\n\n                    if isinstance(parsed_boxes, list):\n                        # This is the expected format: a list of dictionaries\n                        texts_on_page = []\n                        for item in parsed_boxes:\n                            if isinstance(item, dict) and 'text_content' in item:\n                                texts_on_page.append(item['text_content'])\n                        page_extracted_text_plain += \"\\n\".join(texts_on_page)\n\n                        # Plot bounding boxes only if parsing was successful and we have a list\n                        current_page_image_with_boxes = plot_text_bounding_boxes(\n                            current_page_image_with_boxes, ocr_response, input_width, input_height\n                        )\n                    else:\n                        # If it's not a list, it's an unexpected format from the model\n                        page_extracted_text_plain += f\"OCR response was not a list of text objects. Raw:\\n{parsed_json_str}\"\n                        print(f\"Warning: OCR response for page {i+1} was not a list: {parsed_json_str}\")\n\n                except (ValueError, SyntaxError) as e:\n                    # Catch errors from ast.literal_eval if the string isn't valid Python literal\n                    page_extracted_text_plain += f\"Error parsing OCR JSON for page {i+1}: {e}\\nRaw response (after fence removal):\\n{parsed_json_str}\"\n                    print(f\"Error parsing OCR JSON for page {i+1}: {e}\\nRaw response (after fence removal):\\n{parsed_json_str}\")\n                except Exception as e:\n                    # Catch any other unexpected errors during text extraction\n                    page_extracted_text_plain += f\"An unexpected error occurred during OCR text extraction for page {i+1}: {e}\"\n                    print(f\"An unexpected error occurred during OCR text extraction for page {i+1}: {e}\")\n\n                all_ocr_texts_plain.append(page_extracted_text_plain)\n                all_ocr_texts_full.append(page_extracted_text_full)\n                all_ocr_images.append(current_page_image_with_boxes)\n\n        except Exception as e:\n            error_msg = f\"Fatal Error processing file: {e}\" # Changed \"PDF\" to \"file\"\n            print(error_msg)\n            return [], [], error_msg, error_msg, error_msg\n\n        return original_images, all_ocr_images, \"\\n\\n\".join(all_ocr_texts_plain), \"\\n\\n\".join(all_ocr_texts_full), status_message\n\ndef clear_vram():\n    \"\"\"Clears PyTorch's CUDA memory cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect() # Aggressively collect garbage\n        print(\"CUDA VRAM cache emptied and garbage collected.\")\n        return \"VRAM Cleared!\"\n    else:\n        return \"No CUDA device found to clear VRAM.\"\n\n# --- Gradio Interface ---\nwith gr.Blocks(title=\"Qwen2.5-VL PDF OCR Interface\") as demo:\n    gr.Markdown(\n        \"\"\"\n        # Qwen2.5-VL PDF OCR Interface\n        Upload a PDF or image file to perform OCR and visualize the detected text with bounding boxes.\n        Adjust the PDF conversion resolution for better OCR results.\n        \"\"\"\n    )\n\n    # Tabs for Outputs (Previews and Texts) - Stays at the top\n    with gr.Tabs():\n        with gr.TabItem(\"Preview Images\"):\n            gr.Markdown(\"### Original vs. OCR'd Pages\")\n            with gr.Row():\n                original_pages_gallery = gr.Gallery(\n                    label=\"Original File Pages/Images\",\n                    show_label=True,\n                    elem_id=\"gallery_original\",\n                    columns=2,\n                    rows=2,\n                    object_fit=\"contain\",\n                    height=\"auto\",\n                )\n                ocr_pages_gallery = gr.Gallery(\n                    label=\"OCR with Bounding Boxes\",\n                    show_label=True,\n                    elem_id=\"gallery_ocr\",\n                    columns=2,\n                    rows=2,\n                    object_fit=\"contain\",\n                    height=\"auto\",\n                )\n        with gr.TabItem(\"Extracted Plain Text\"):\n            gr.Markdown(\"### Plain Text Output\")\n            ocr_text_plain_output = gr.Textbox(\n                label=\"Extracted Text (All Pages)\", lines=20, interactive=False\n            )\n        with gr.TabItem(\"Complete OCR Output (JSON/Raw)\"):\n            gr.Markdown(\"### Raw Model Output with Bounding Box Data\")\n            ocr_text_full_output = gr.Textbox(\n                label=\"Complete OCR Output (Includes Bounding Box JSON)\",\n                lines=20,\n                interactive=False,\n            )\n\n    # Upload and Settings section - Moved to the bottom, under the previews\n    with gr.Row():\n        # Column 1: Upload File\n        with gr.Column(scale=1):\n            with gr.Group():\n                gr.Markdown(\"### 1. Upload File\")\n                pdf_image_input = gr.File(\n                    label=\"Upload PDF or Image File\",\n                    file_types=[\".pdf\", \".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".tiff\", \".tif\"]\n                )\n    \n        # Column 2: OCR Settings\n        with gr.Column(scale=1):\n            with gr.Group():\n                gr.Markdown(\"### 2. OCR Settings\")\n                resolution_slider = gr.Slider(\n                    minimum=72, maximum=600, value=200, step=10,\n                    label=\"PDF to Image Resolution (DPI)\",\n                    info=\"Applies only to PDFs. Higher DPI can improve OCR accuracy but uses more memory/time.\",\n                )\n                ocr_button = gr.Button(\"Perform OCR\", variant=\"primary\")\n    \n        # Column 3: VRAM Reset & Status\n        with gr.Column(scale=1):\n            with gr.Group():\n                gr.Markdown(\"### 3. VRAM Control\")\n                vram_reset_button = gr.Button(\"Reset VRAM\", variant=\"secondary\")\n                vram_status_output = gr.Textbox(\n                    label=\"VRAM Status\",\n                    interactive=False,\n                    max_lines=1\n                )\n\n    # Event Listeners\n    ocr_button.click(\n        process_file_for_ocr, # Changed function name to handle both file types\n        inputs=[pdf_image_input, resolution_slider],\n        outputs=[original_pages_gallery, ocr_pages_gallery, ocr_text_plain_output, ocr_text_full_output, vram_status_output]\n    )\n\n    vram_reset_button.click(\n        clear_vram,\n        inputs=[],\n        outputs=[vram_status_output]\n    )\n\ndemo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## v5 - ok","metadata":{}},{"cell_type":"code","source":"import torch\nimport json\nimport random\nimport io\nimport ast\nimport xml.etree.ElementTree as ET\nimport gradio as gr\nimport os\nimport tempfile\nfrom pdf2image import convert_from_path # Used for PDF to image conversion\nimport gc # Import garbage collection module\nimport time # Import the time module\n\nfrom transformers import (\n    Qwen2_5_VLForConditionalGeneration,\n    AutoTokenizer,\n    AutoProcessor,\n    BitsAndBytesConfig,\n    TextStreamer\n)\nfrom qwen_vl_utils import process_vision_info # Assuming this file is available\nfrom PIL import Image, ImageDraw, ImageFont, ImageColor\n\n# --- Model Loading (Moved to top level) ---\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntry:\n    # IMPORTANT: Ensure 'Qwen/Qwen2.5-VL-3B-Instruct' is accessible.\n    # On Kaggle, this typically means adding it as a dataset.\n    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n        'Qwen/Qwen2.5-VL-3B-Instruct',\n        device_map='auto',\n    )#.to(device)\n    processor = AutoProcessor.from_pretrained('Qwen/Qwen2.5-VL-3B-Instruct')\n    print(\"Model and Processor loaded successfully.\")\nexcept Exception as e:\n    print(f\"Error loading model or processor: {e}\")\n    print(\"Please ensure 'Qwen/Qwen2.5-VL-3B-Instruct' is accessible and properly configured.\")\n    # Fallback or exit strategy if model fails to load\n    model = None\n    processor = None\n\n# --- Helper Functions ---\ndef plot_text_bounding_boxes(image: Image.Image, bounding_boxes_str: str, input_width, input_height):\n    \"\"\"\n    Plots bounding boxes on an image with markers for each text, using PIL, normalized coordinates, and different colors.\n    \"\"\"\n    if not image:\n        return None\n\n    width, height = image.size\n    draw = ImageDraw.Draw(image)\n\n    parsed_json_str = parse_json(bounding_boxes_str)\n\n    try:\n        # Check if the font path is available on Kaggle. If not, default will be used.\n        # This assumes NotoSansCJK-Regular.ttc is in a Kaggle dataset mounted at ./00_Dataset/\n        font_path = './00_Dataset/NotoSansCJK-Regular.ttc'\n        if os.path.exists(font_path):\n            font_size = max(10, min(int(height / 30), int(width / 60)))\n            font = ImageFont.truetype(font_path, size=font_size)\n        else:\n            print(f\"Warning: Font file '{font_path}' not found. Using default font.\")\n            font = ImageFont.load_default()\n    except IOError:\n        print(f\"Warning: Error loading font from '{font_path}'. Using default font.\")\n        font = ImageFont.load_default()\n\n    try:\n        bounding_boxes_list = ast.literal_eval(parsed_json_str)\n        if not isinstance(bounding_boxes_list, list):\n            print(f\"Warning: Expected a list from ast.literal_eval, but got {type(bounding_boxes_list)}. Raw parsed JSON: {parsed_json_str[:200]}...\")\n            bounding_boxes_list = []\n    except (ValueError, SyntaxError) as e:\n        print(f\"Error evaluating bounding boxes string: {e}\")\n        print(f\"Problematic string for bounding boxes: {parsed_json_str[:500]}...\")\n        bounding_boxes_list = []\n\n    for bounding_box in bounding_boxes_list:\n        color = 'green'\n        if not isinstance(bounding_box, dict) or 'bbox_2d' not in bounding_box or not isinstance(bounding_box['bbox_2d'], list) or len(bounding_box['bbox_2d']) != 4:\n            print(f\"Skipping malformed bounding box entry: {bounding_box}\")\n            continue\n\n        abs_y1 = int(bounding_box[\"bbox_2d\"][1] / input_height * height)\n        abs_x1 = int(bounding_box[\"bbox_2d\"][0] / input_width * width)\n        abs_y2 = int(bounding_box[\"bbox_2d\"][3] / input_height * height)\n        abs_x2 = int(bounding_box[\"bbox_2d\"][2] / input_width * width)\n\n        # Ensure coordinates are in correct order\n        abs_x1, abs_x2 = min(abs_x1, abs_x2), max(abs_x1, abs_x2)\n        abs_y1, abs_y2 = min(abs_y1, abs_y2), max(abs_y1, abs_y2)\n\n        draw.rectangle(\n            ((abs_x1, abs_y1), (abs_x2, abs_y2)), outline=color, width=2\n        )\n\n        if 'text_content' in bounding_box:\n            draw.text((abs_x1, abs_y2), bounding_box['text_content'], fill=color, font=font)\n\n    return image\n\ndef parse_json(json_output: str) -> str:\n    \"\"\"Removes markdown fencing from JSON string.\"\"\"\n    lines = json_output.splitlines()\n    for i, line in enumerate(lines):\n        if line.strip() == \"```json\":\n            json_output = \"\\n\".join(lines[i+1:])\n            json_output = json_output.split(\"```\")[0].strip()\n            break\n    return json_output\n\ndef inference(\n    prompt: str,\n    image_path: str,\n    system_prompt = 'You are a helpful assistant', # Kept as requested\n    max_new_tokens: int = 4096,\n    min_pixels: int = 512 * 28 * 28,\n    max_pixels: int = 2048 * 28 * 28,\n):\n    if model is None or processor is None:\n        return \"Error: Model not loaded.\", 0, 0\n\n    messages = [\n        {\n            'role': 'user',\n            'content': [\n                {\n                    'type': 'image',\n                    'image': image_path,\n                    'min_pixels': min_pixels,\n                    'max_pixels': max_pixels,\n                },\n                {'type': 'text', 'text': prompt},\n            ],\n        }\n    ]\n\n    text = processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n    # print('Input to model:\\n', text) # Commented out to reduce console clutter\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors='pt',\n    )\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    streamer = TextStreamer(processor.tokenizer, skip_special_tokens=True, skip_prompt=True)\n\n    generated_ids = model.generate(**inputs, max_new_tokens=max_new_tokens, streamer=streamer)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs['input_ids'], generated_ids)\n    ]\n    output_text = processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )\n\n    # print('Output from model:\\n', output_text[0]) # Commented out to reduce console clutter\n\n    input_height = 0\n    input_width = 0\n    if 'image_grid_thw' in inputs and len(inputs['image_grid_thw']) > 0:\n        if isinstance(inputs['image_grid_thw'][0], torch.Tensor) and inputs['image_grid_thw'][0].dim() == 1 and inputs['image_grid_thw'][0].shape[0] >= 3:\n             input_height = inputs['image_grid_thw'][0][1].item() * 14\n             input_width = inputs['image_grid_thw'][0][2].item() * 14\n        else:\n            print(f\"Warning: Unexpected format for image_grid_thw: {inputs['image_grid_thw']}\")\n\n    return output_text[0], input_width, input_height\n\n\ndef process_file_for_ocr(file, resolution: int):\n    \"\"\"\n    Handles PDF or image input: converts pages to images (if PDF),\n    performs OCR, and returns lists of images and texts.\n    \"\"\"\n    if file is None:\n        return [], [], \"\", \"\", \"Please upload a PDF or image file.\", \"\" # Added an empty string for time_taken\n\n    start_time = time.time() # Start time measurement\n\n    input_file_path = file.name\n    file_extension = os.path.splitext(input_file_path)[1].lower()\n\n    all_ocr_texts_plain = []\n    all_ocr_texts_full = [] # To store the complete JSON/raw output\n    all_ocr_images = []\n    original_images = []\n    status_message = \"Processing complete.\"\n    time_taken = \"\"\n\n    with tempfile.TemporaryDirectory() as temp_dir:\n        try:\n            images_to_process = []\n            if file_extension == '.pdf':\n                print(f\"Processing PDF with DPI: {resolution}\")\n                images_to_process = convert_from_path(input_file_path, dpi=resolution)\n            elif file_extension in ['.png', '.jpg', '.jpeg', '.webp', '.bmp', '.tiff', '.tif']: # Common image formats\n                print(f\"Processing image file: {input_file_path}\")\n                try:\n                    pil_image = Image.open(input_file_path).convert(\"RGB\") # Ensure RGB for consistent processing\n                    images_to_process = [pil_image]\n                except Exception as img_e:\n                    raise ValueError(f\"Could not open image file. Is it a valid image? Error: {img_e}\")\n            else:\n                return [], [], \"\", \"\", f\"Unsupported file type: {file_extension}. Please upload a PDF or a common image format (.png, .jpg, .jpeg, etc.).\", \"\"\n\n            if not images_to_process:\n                return [], [], \"No content found in file.\", \"No content found in file.\", \"No content found in file.\", \"\"\n\n            for i, pil_image_orig in enumerate(images_to_process):\n                original_images.append(pil_image_orig)\n\n                temp_image_path = os.path.join(temp_dir, f\"page_{i}.png\")\n                pil_image_orig.save(temp_image_path, format=\"PNG\")\n\n                prompt = 'Spotting all the text in the image with line-level, and output in JSON format.'\n                ocr_response, input_width, input_height = inference(prompt, temp_image_path)\n\n                # --- Robust Parsing for OCR Text and Bounding Boxes ---\n                page_extracted_text_plain = f\"--- Page {i+1} (Plain Text) ---\\n\"\n                page_extracted_text_full = f\"--- Page {i+1} (Raw OCR Output) ---\\n\"\n                current_page_image_with_boxes = pil_image_orig.copy() # Make a copy for drawing\n\n                page_extracted_text_full += ocr_response + \"\\n\\n\" # Store raw output\n\n                try:\n                    parsed_json_str = parse_json(ocr_response)\n                    parsed_boxes = ast.literal_eval(parsed_json_str)\n\n                    if isinstance(parsed_boxes, list):\n                        texts_on_page = []\n                        for item in parsed_boxes:\n                            if isinstance(item, dict) and 'text_content' in item:\n                                texts_on_page.append(item['text_content'])\n                        page_extracted_text_plain += \"\\n\".join(texts_on_page)\n\n                        current_page_image_with_boxes = plot_text_bounding_boxes(\n                            current_page_image_with_boxes, ocr_response, input_width, input_height\n                        )\n                    else:\n                        page_extracted_text_plain += f\"OCR response was not a list of text objects. Raw:\\n{parsed_json_str}\"\n                        print(f\"Warning: OCR response for page {i+1} was not a list: {parsed_json_str}\")\n\n                except (ValueError, SyntaxError) as e:\n                    page_extracted_text_plain += f\"Error parsing OCR JSON for page {i+1}: {e}\\nRaw response (after fence removal):\\n{parsed_json_str}\"\n                    print(f\"Error parsing OCR JSON for page {i+1}: {e}\\nRaw response (after fence removal):\\n{parsed_json_str}\")\n                except Exception as e:\n                    page_extracted_text_plain += f\"An unexpected error occurred during OCR text extraction for page {i+1}: {e}\"\n                    print(f\"An unexpected error occurred during OCR text extraction for page {i+1}: {e}\")\n\n                all_ocr_texts_plain.append(page_extracted_text_plain)\n                all_ocr_texts_full.append(page_extracted_text_full)\n                all_ocr_images.append(current_page_image_with_boxes)\n\n        except Exception as e:\n            error_msg = f\"Fatal Error processing file: {e}\"\n            print(error_msg)\n            # Ensure time_taken is still returned even on error\n            end_time = time.time()\n            time_taken = f\"Time: {end_time - start_time:.2f} s\"\n            return [], [], error_msg, error_msg, error_msg, time_taken\n\n    end_time = time.time()\n    time_taken = f\"Time: {end_time - start_time:.2f} s\"\n    return original_images, all_ocr_images, \"\\n\\n\".join(all_ocr_texts_plain), \"\\n\\n\".join(all_ocr_texts_full), status_message, time_taken\n\ndef clear_vram():\n    \"\"\"Clears PyTorch's CUDA memory cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        gc.collect() # Aggressively collect garbage\n        print(\"CUDA VRAM cache emptied and garbage collected.\")\n        return \"VRAM Cleared!\", \"\" # Also clear the time counter on VRAM reset\n    else:\n        return \"No CUDA device found to clear VRAM.\", \"\"\n\n# --- Gradio Interface ---\nwith gr.Blocks(title=\"Qwen2.5-VL PDF OCR Interface\") as demo:\n    gr.Markdown(\n        \"\"\"\n        # Qwen2.5-VL PDF OCR Interface\n        Upload a PDF or image file to perform OCR and visualize the detected text with bounding boxes.\n        Adjust the PDF conversion resolution for better OCR results.\n        \"\"\"\n    )\n\n    # Tabs for Outputs (Previews and Texts) - Stays at the top\n    with gr.Tabs():\n        with gr.TabItem(\"Preview Images\"):\n            gr.Markdown(\"### Original vs. OCR'd Pages\")\n            with gr.Row():\n                original_pages_gallery = gr.Gallery(\n                    label=\"Original File Pages/Images\",\n                    show_label=True,\n                    elem_id=\"gallery_original\",\n                    columns=2,\n                    rows=2,\n                    object_fit=\"contain\",\n                    height=\"auto\",\n                )\n                ocr_pages_gallery = gr.Gallery(\n                    label=\"OCR with Bounding Boxes\",\n                    show_label=True,\n                    elem_id=\"gallery_ocr\",\n                    columns=2,\n                    rows=2,\n                    object_fit=\"contain\",\n                    height=\"auto\",\n                )\n        with gr.TabItem(\"Extracted Plain Text\"):\n            gr.Markdown(\"### Plain Text Output\")\n            ocr_text_plain_output = gr.Textbox(\n                label=\"Extracted Text (All Pages)\", lines=20, interactive=False\n            )\n        with gr.TabItem(\"Complete OCR Output (JSON/Raw)\"):\n            gr.Markdown(\"### Raw Model Output with Bounding Box Data\")\n            ocr_text_full_output = gr.Textbox(\n                label=\"Complete OCR Output (Includes Bounding Box JSON)\",\n                lines=20,\n                interactive=False,\n            )\n\n    # Upload and Settings section - Moved to the bottom, under the previews\n    with gr.Row():\n        # Column 1: Upload File\n        with gr.Column(scale=1):\n            with gr.Group():\n                gr.Markdown(\"### 1. Upload File\")\n                pdf_image_input = gr.File(\n                    label=\"Upload PDF or Image File\",\n                    file_types=[\".pdf\", \".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".tiff\", \".tif\"]\n                )\n    \n        # Column 2: OCR Settings\n        with gr.Column(scale=1):\n            with gr.Group():\n                gr.Markdown(\"### 2. OCR Settings\")\n                resolution_slider = gr.Slider(\n                    minimum=72, maximum=600, value=200, step=10,\n                    label=\"PDF to Image Resolution (DPI)\",\n                    info=\"Applies only to PDFs. Higher DPI can improve OCR accuracy but uses more memory/time.\",\n                )\n                ocr_button = gr.Button(\"Perform OCR\", variant=\"primary\")\n    \n        # Column 3: VRAM Reset & Status\n        with gr.Column(scale=1):\n            gr.Markdown(\"---\") # Separator\n            with gr.Row(): # VRAM Reset and Time Counter in their own row\n                vram_reset_button = gr.Button(\"Reset VRAM\", variant=\"secondary\")\n                vram_status_output = gr.Textbox(label=\"VRAM Status\", interactive=False, max_lines=1)\n                time_counter_output = gr.Textbox(label=\"Processing Time\", interactive=False, max_lines=1) # New time counter\n\n    # Event Listeners\n    ocr_button.click(\n        process_file_for_ocr,\n        inputs=[pdf_image_input, resolution_slider],\n        outputs=[\n            original_pages_gallery,\n            ocr_pages_gallery,\n            ocr_text_plain_output,\n            ocr_text_full_output,\n            vram_status_output,\n            time_counter_output # New output for time\n        ]\n    )\n\n    vram_reset_button.click(\n        clear_vram,\n        inputs=[],\n        outputs=[vram_status_output, time_counter_output] # Clear time counter too\n    )\n\ndemo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-28T07:32:51.859941Z","iopub.execute_input":"2025-07-28T07:32:51.860694Z","iopub.status.idle":"2025-07-28T07:33:17.038923Z","shell.execute_reply.started":"2025-07-28T07:32:51.860664Z","shell.execute_reply":"2025-07-28T07:33:17.038330Z"}},"outputs":[{"name":"stderr","text":"2025-07-28 07:32:59.707093: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753687979.729307    1428 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753687979.736075    1428 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea53e1c12034c95be3616b05508fff2"}},"metadata":{}},{"name":"stderr","text":"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\nYou have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n","output_type":"stream"},{"name":"stdout","text":"Model and Processor loaded successfully.\n* Running on local URL:  http://127.0.0.1:7860\nIt looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n\n* Running on public URL: https://4da5878345e73d4893.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://4da5878345e73d4893.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":1,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stdout","text":"Processing PDF with DPI: 300\n```json\n{\n    \"line_1\": \"Sreejesh/Aditya\",\n    \"line_2\": \"\",\n    \"line_3\": \"THE LOOK (MACAO COMMERCIAL OFFSHORE) COM\",\n    \"line_4\": \"PANYI LIMITED\",\n    \"line_5\": \"UNIT 1703-1704 17/F\",\n    \"line_6\": \"AIA TOWER\",\n    \"line_7\": \"AVENIDA COMERCIAL DE MACAU NOS 251A - 301\",\n    \"line_8\": \"MACAU 999999\",\n    \"line_9\": \"MACAO\",\n    \"line_10\": \"Phone: 853-85920822\",\n    \"line_11\": \"\",\n    \"line_12\": \"For Account Of\",\n    \"line_13\": \"adidas Emerging Markets FZE\",\n    \"line_14\": \"c/o adidas International Trading AG\",\n    \"line_15\": \"Platz 1b\",\n    \"line_16\": \"Root D4 6039\",\n    \"line_17\": \"SWITZERLAND\",\n    \"line_18\": \"Phone: +41 41 450 04 00\",\n    \"line_19\": \"\",\n    \"line_20\": \"Actual Manufacturer\",\n    \"line_21\": \"PT NIKOMAS GEMILANG\",\n    \"line_22\": \"KEL TAMBAK\",\n    \"line_23\": \"KEC KIBIN\",\n    \"line_24\": \"JALAN RAYA SERANG KM.71\",\n    \"line_25\": \"Serang,BT.00000\",\n    \"line_26\": \"INDONESIA\",\n    \"line_27\": \"\",\n    \"line_28\": \"Consignee\",\n    \"line_29\": \"adidas Emerging Markets FZE\",\n    \"line_30\": \"DLC WE-38 PO Box 61274 Dubai South\",\n    \"line_31\": \"Dubai\",\n    \"line_32\": \"UNITED ARAB EMIRATES\",\n    \"line_33\": \"Contact: Sreejesh/Aditya\",\n    \"line_34\": \"Phone: 971 4 512 3863 / 971 4 512 3855\",\n    \"line_35\": \"\",\n    \"line_36\": \"GPS Customer Number\",\n    \"line_37\": \"848012\",\n    \"line_38\": \"Customer VAT No\",\n    \"line_39\": \"Origin From\",\n    \"line_40\": \"INDONESIA\",\n    \"line_41\": \"\",\n    \"line_42\": \"Shipment Country\",\n    \"line_43\": \"INDONESIA\",\n    \"line_44\": \"Destination To\",\n    \"line_45\": \"UNITED ARAB EMIRATES\",\n    \"line_46\": \"Supplier VAT No\",\n    \"line_47\": \"\",\n    \"line_48\": \"Payment Method\",\n    \"line_49\": \"OAB 75 Days\",\n    \"line_50\": \"Incoterms 2010\",\n    \"line_51\": \"Currency\",\n    \"line_52\": \"USD\",\n    \"line_53\": \"\",\n    \"line_54\": \"FCR Date\",\n    \"line_55\": \"2024-10-17\",\n    \"line_56\": \"Ex-Factory Date\",\n    \"line_57\": \"Ship Mode\",\n    \"line_58\": \"2024-10-17\",\n    \"line_59\": \"Forwarder\",\n    \"line_60\": \"Ocean\",\n    \"line_61\": \"\",\n    \"line_62\": \"VAT Rate\",\n    \"line_63\": \"0\",\n    \"line_64\": \"Brand\",\n    \"line_65\": \"11\",\n    \"line_66\": \"\",\n    \"line_67\": \"Notes\",\n    \"line_68\": \"\",\n    \"line_69\": \"Statement of Origin Note\",\n    \"line_70\": \"\",\n    \"line_71\": \"\",\n    \"line_72\": \"Invoice Text\",\n    \"line_73\": \"Total Amount Export Quota\",\n    \"line_74\": \"\",\n    \"line_75\": \"Export Sale 0% VAT\",\n    \"line_76\": \"\",\n    \"line_77\": \"\",\n    \"line_78\": \"PO No\",\n    \"line_79\": \"PO Line\",\n    \"line_80\": \"Market PO\",\n    \"line_81\": \"Sales Order No\",\n    \"line_82\": \"Working No\",\n    \"line_83\": \"Article No\",\n    \"line_84\": \"Article Description\",\n    \"line_85\": \"Gender\",\n    \"line_86\": \"Category\",\n    \"line_87\": \"Total\",\n    \"line_88\": \"Qty\",\n    \"line_89\": \"0135851744\",\n    \"line_90\": \"1\",\n    \"line_91\": \"0301945536\",\n    \"line_92\": \"\",\n    \"line_93\": \"10091586\",\n    \"line_94\": \"JH5589\",\n    \"line_95\": \"SUPERSTAR 82 SUPCOL/PNKTIN/CREWHT\",\n    \"line_96\": \"M\",\n    \"line_97\": \"ORIGINALS\",\n    \"line_98\": \"100\",\n    \"line_99\": \"\",\n    \"line_100\": \"\",\n    \"line_101\": \"\",\n    \"line_102\": \"\",\n    \"line_103\": \"\",\n    \"line_104\": \"\",\n    \"line_105\": \"\",\n    \"line_106\": \"\",\n    \"line_107\": \"\",\n    \"line_108\": \"\",\n    \"line_109\": \"\",\n    \"line_110\": \"\",\n    \"line_111\": \"\",\n    \"line_112\": \"\",\n    \"line_113\": \"\",\n    \"line_114\": \"\",\n    \"line_115\": \"\",\n    \"line_116\": \"\",\n    \"line_117\": \"\",\n    \"line_118\": \"\",\n    \"line_119\": \"\",\n    \"line_120\": \"\",\n    \"line_121\": \"\",\n    \"line_122\": \"\",\n    \"line_123\": \"\",\n    \"line_124\": \"\",\n    \"line_125\": \"\",\n    \"line_126\": \"\",\n    \"line_127\": \"\",\n    \"line_128\": \"\",\n    \"line_129\": \"\",\n    \"line_130\": \"\",\n    \"line_131\": \"\",\n    \"line_132\": \"\",\n    \"line_133\": \"\",\n    \"line_134\": \"\",\n    \"line_135\": \"\",\n    \"line_136\": \"\",\n    \"line_137\": \"\",\n    \"line_138\": \"\",\n    \"line_139\": \"\",\n    \"line_140\": \"\",\n    \"line_141\": \"\",\n    \"line_142\": \"\",\n    \"line_143\": \"\",\n    \"line_144\": \"\",\n    \"line_145\": \"\",\n    \"line_146\": \"\",\n    \"line_147\": \"\",\n    \"line_148\": \"\",\n    \"line_149\": \"\",\n    \"line_150\": \"\",\n    \"line_151\": \"\",\n    \"line_152\": \"\",\n    \"line_153\": \"\",\n    \"line_154\": \"\",\n    \"line_155\": \"\",\n    \"line_156\": \"\",\n    \"line_157\": \"\",\n    \"line_158\": \"\",\n    \"line_159\": \"\",\n    \"line_160\": \"\",\n    \"line_161\": \"\",\n    \"line_162\": \"\",\n    \"line_163\": \"\",\n    \"line_164\": \"\",\n    \"line_165\": \"\",\n    \"line_166\": \"\",\n    \"line_167\": \"\",\n    \"line_168\": \"\",\n    \"line_169\": \"\",\n    \"line_170\": \"\",\n    \"line_171\": \"\",\n    \"line_172\": \"\",\n    \"line_173\": \"\",\n    \"line_174\": \"\",\n    \"line_175\": \"\",\n    \"line_176\": \"\",\n    \"line_177\": \"\",\n    \"line_178\": \"\",\n    \"line_179\": \"\",\n    \"line_180\": \"\",\n    \"line_181\": \"\",\n    \"line_182\": \"\",\n    \"line_183\": \"\",\n    \"line_184\": \"\",\n    \"line_185\": \"\",\n    \"line_186\": \"\",\n    \"line_187\": \"\",\n    \"line_188\": \"\",\n    \"line_189\": \"\",\n    \"line_190\": \"\",\n    \"line_191\": \"\",\n    \"line_192\": \"\",\n    \"line_193\": \"\",\n    \"line_194\": \"\",\n    \"line_195\": \"\",\n    \"line_196\": \"\",\n    \"line_197\": \"\",\n    \"line_198\": \"\",\n    \"line_199\": \"\",\n    \"line_200\": \"\",\n    \"line_201\": \"\",\n    \"line_202\": \"\",\n    \"line_203\": \"\",\n    \"line_204\": \"\",\n    \"line_205\": \"\",\n    \"line_206\": \"\",\n    \"line_207\": \"\",\n    \"line_208\": \"\",\n    \"line_209\": \"\",\n    \"line_210\": \"\",\n    \"line_211\": \"\",\n    \"line_212\": \"\",\n    \"line_213\": \"\",\n    \"line_214\": \"\",\n    \"line_215\": \"\",\n    \"line_216\": \"\",\n    \"line_217\": \"\",\n    \"line_218\": \"\",\n    \"line_219\": \"\",\n    \"line_220\": \"\",\n    \"line_221\": \"\",\n    \"line_222\": \"\",\n    \"line_223\": \"\",\n    \"line_224\": \"\",\n    \"line_225\": \"\",\n    \"line_226\": \"\",\n    \"line_227\": \"\",\n    \"line_228\": \"\",\n    \"line_229\": \"\",\n    \"line_230\": \"\",\n    \"line_231\": \"\",\n    \"line_232\": \"\",\n    \"line_233\": \"\",\n    \"line_234\": \"\",\n    \"line_235\": \"\",\n    \"line_236\": \"\",\n    \"line_237\": \"\",\n    \"line_238\": \"\",\n    \"line_239\": \"\",\n    \"line_240\": \"\",\n    \"line_241\": \"\",\n    \"line_242\": \"\",\n    \"line_243\": \"\",\n    \"line_244\": \"\",\n    \"line_245\": \"\",\n    \"line_246\": \"\",\n    \"line_247\": \"\",\n    \"line_248\": \"\",\n    \"line_249\": \"\",\n    \"line_250\": \"\",\n    \"line_251\": \"\",\n    \"line_252\": \"\",\n    \"line_253\": \"\",\n    \"line_254\": \"\",\n    \"line_255\": \"\",\n    \"line_256\": \"\",\n    \"line_257\": \"\",\n    \"line_258\": \"\",\n    \"line_259\": \"\",\n    \"line_260\": \"\",\n    \"line_261\": \"\",\n    \"line_262\": \"\",\n    \"line_263\": \"\",\n    \"line_264\": \"\",\n    \"line_265\": \"\",\n    \"line_266\": \"\",\n    \"line_267\": \"\",\n    \"line_268\": \"\",\n    \"line_269\": \"\",\n    \"line_270\": \"\",\n    \"line_271\": \"\",\n    \"line_272\": \"\",\n    \"line_273\": \"\",\n    \"line_274\": \"\",\n    \"line_275\": \"\",\n    \"line_276\": \"\",\n    \"line_277\": \"\",\n    \"line_278\": \"\",\n    \"line_279\": \"\",\n    \"line_280\": \"\",\n    \"line_281\": \"\",\n    \"line_282\": \"\",\n    \"line_283\": \"\",\n    \"line_284\": \"\",\n    \"line_285\": \"\",\n    \"line_286\": \"\",\n    \"line_287\": \"\",\n    \"line_288\": \"\",\n    \"line_289\": \"\",\n    \"line_290\": \"\",\n    \"line_291\": \"\",\n    \"line_292\": \"\",\n    \"line_293\": \"\",\n    \"line_294\": \"\",\n    \"line_295\": \"\",\n    \"line_296\": \"\",\n    \"line_297\": \"\",\n    \"line_298\": \"\",\n    \"line_299\": \"\",\n    \"line_300\": \"\",\n    \"line_301\": \"\",\n    \"line_302\": \"\",\n    \"line_303\": \"\",\n    \"line_304\": \"\",\n    \"line_305\": \"\",\n    \"line_306\": \"\",\n    \"line_307\": \"\",\n    \"line_308\": \"\",\n    \"line_309\": \"\",\n    \"line_310\": \"\",\n    \"line_311\": \"\",\n    \"line_312\": \"\",\n    \"line_313\": \"\",\n    \"line_314\": \"\",\n    \"line_315\": \"\",\n    \"line_316\": \"\",\n    \"line_317\": \"\",\n    \"line_318\": \"\",\n    \"line_319\": \"\",\n    \"line_320\": \"\",\n    \"line_321\": \"\",\n    \"line_322\": \"\",\n    \"line_323\": \"\",\n    \"line_324\": \"\",\n    \"line_325\": \"\",\n    \"line_326\": \"\",\n    \"line_327\": \"\",\n    \"line_328\": \"\",\n    \"line_329\": \"\",\n    \"line_330\": \"\",\n    \"line_331\": \"\",\n    \"line_332\": \"\",\n    \"line_333\": \"\",\n    \"line_334\": \"\",\n    \"line_335\": \"\",\n    \"line_336\": \"\",\n    \"line_337\": \"\",\n    \"line_338\": \"\",\n    \"line_339\": \"\",\n    \"line_340\": \"\",\n    \"line_341\": \"\",\n    \"line_342\": \"\",\n    \"line_343\": \"\",\n    \"line_344\": \"\",\n    \"line_345\": \"\",\n    \"line_346\": \"\",\n    \"line_347\": \"\",\n    \"line_348\": \"\",\n    \"line_349\": \"\",\n    \"line_350\": \"\",\n    \"line_351\": \"\",\n    \"line_352\": \"\",\n    \"line_353\": \"\",\n    \"line_354\": \"\",\n    \"line_355\": \"\",\n    \"line_356\": \"\",\n    \"line_357\": \"\",\n    \"line_358\": \"\",\n    \"line_359\": \"\",\n    \"line_360\": \"\",\n    \"line_361\": \"\",\n    \"line_362\": \"\",\n    \"line_363\": \"\",\n    \"line_364\": \"\",\n    \"line_365\": \"\",\n    \"line_366\": \"\",\n    \"line_367\": \"\",\n    \"line_368\": \"\",\n    \"line_369\": \"\",\n    \"line_370\": \"\",\n    \"line_371\": \"\",\n    \"line_372\": \"\",\n    \"line_373\": \"\",\n    \"line_374\": \"\",\n    \"line_375\": \"\",\n    \"line_376\": \"\",\n    \"line_377\": \"\",\n    \"line_378\": \"\",\n    \"line_379\": \"\",\n    \"line_380\": \"\",\n    \"line_381\": \"\",\n    \"line_382\": \"\",\n    \"line_383\": \"\",\n    \"line_384\": \"\",\n    \"line_385\": \"\",\n    \"line_386\": \"\",\n    \"line_387\": \"\",\n    \"line_388\": \"\",\n    \"line_389\": \"\",\n    \"line_390\": \"\",\n    \"line_391\": \"\",\n    \"line_392\": \"\",\n    \"line_393\": \"\",\n    \"line_394\": \"\",\n    \"line_395\": \"\",\n    \"line_396\": \"\",\n    \"line_397\": \"\",\n    \"line_398\": \"\",\n    \"line_399\": \"\",\n    \"line_400\": \"\",\n    \"line_401\": \"\",\n    \"line_402\": \"\",\n    \"line_403\": \"\",\n    \"line_404\": \"\",\n    \"line_405\": \"\",\n    \"line_406\": \"\",\n    \"line_407\": \"\",\n    \"line_40\nError parsing OCR JSON for page 1: unterminated string literal (detected at line 409) (<unknown>, line 409)\nRaw response (after fence removal):\n{\n    \"line_1\": \"Sreejesh/Aditya\",\n    \"line_2\": \"\",\n    \"line_3\": \"THE LOOK (MACAO COMMERCIAL OFFSHORE) COM\",\n    \"line_4\": \"PANYI LIMITED\",\n    \"line_5\": \"UNIT 1703-1704 17/F\",\n    \"line_6\": \"AIA TOWER\",\n    \"line_7\": \"AVENIDA COMERCIAL DE MACAU NOS 251A - 301\",\n    \"line_8\": \"MACAU 999999\",\n    \"line_9\": \"MACAO\",\n    \"line_10\": \"Phone: 853-85920822\",\n    \"line_11\": \"\",\n    \"line_12\": \"For Account Of\",\n    \"line_13\": \"adidas Emerging Markets FZE\",\n    \"line_14\": \"c/o adidas International Trading AG\",\n    \"line_15\": \"Platz 1b\",\n    \"line_16\": \"Root D4 6039\",\n    \"line_17\": \"SWITZERLAND\",\n    \"line_18\": \"Phone: +41 41 450 04 00\",\n    \"line_19\": \"\",\n    \"line_20\": \"Actual Manufacturer\",\n    \"line_21\": \"PT NIKOMAS GEMILANG\",\n    \"line_22\": \"KEL TAMBAK\",\n    \"line_23\": \"KEC KIBIN\",\n    \"line_24\": \"JALAN RAYA SERANG KM.71\",\n    \"line_25\": \"Serang,BT.00000\",\n    \"line_26\": \"INDONESIA\",\n    \"line_27\": \"\",\n    \"line_28\": \"Consignee\",\n    \"line_29\": \"adidas Emerging Markets FZE\",\n    \"line_30\": \"DLC WE-38 PO Box 61274 Dubai South\",\n    \"line_31\": \"Dubai\",\n    \"line_32\": \"UNITED ARAB EMIRATES\",\n    \"line_33\": \"Contact: Sreejesh/Aditya\",\n    \"line_34\": \"Phone: 971 4 512 3863 / 971 4 512 3855\",\n    \"line_35\": \"\",\n    \"line_36\": \"GPS Customer Number\",\n    \"line_37\": \"848012\",\n    \"line_38\": \"Customer VAT No\",\n    \"line_39\": \"Origin From\",\n    \"line_40\": \"INDONESIA\",\n    \"line_41\": \"\",\n    \"line_42\": \"Shipment Country\",\n    \"line_43\": \"INDONESIA\",\n    \"line_44\": \"Destination To\",\n    \"line_45\": \"UNITED ARAB EMIRATES\",\n    \"line_46\": \"Supplier VAT No\",\n    \"line_47\": \"\",\n    \"line_48\": \"Payment Method\",\n    \"line_49\": \"OAB 75 Days\",\n    \"line_50\": \"Incoterms 2010\",\n    \"line_51\": \"Currency\",\n    \"line_52\": \"USD\",\n    \"line_53\": \"\",\n    \"line_54\": \"FCR Date\",\n    \"line_55\": \"2024-10-17\",\n    \"line_56\": \"Ex-Factory Date\",\n    \"line_57\": \"Ship Mode\",\n    \"line_58\": \"2024-10-17\",\n    \"line_59\": \"Forwarder\",\n    \"line_60\": \"Ocean\",\n    \"line_61\": \"\",\n    \"line_62\": \"VAT Rate\",\n    \"line_63\": \"0\",\n    \"line_64\": \"Brand\",\n    \"line_65\": \"11\",\n    \"line_66\": \"\",\n    \"line_67\": \"Notes\",\n    \"line_68\": \"\",\n    \"line_69\": \"Statement of Origin Note\",\n    \"line_70\": \"\",\n    \"line_71\": \"\",\n    \"line_72\": \"Invoice Text\",\n    \"line_73\": \"Total Amount Export Quota\",\n    \"line_74\": \"\",\n    \"line_75\": \"Export Sale 0% VAT\",\n    \"line_76\": \"\",\n    \"line_77\": \"\",\n    \"line_78\": \"PO No\",\n    \"line_79\": \"PO Line\",\n    \"line_80\": \"Market PO\",\n    \"line_81\": \"Sales Order No\",\n    \"line_82\": \"Working No\",\n    \"line_83\": \"Article No\",\n    \"line_84\": \"Article Description\",\n    \"line_85\": \"Gender\",\n    \"line_86\": \"Category\",\n    \"line_87\": \"Total\",\n    \"line_88\": \"Qty\",\n    \"line_89\": \"0135851744\",\n    \"line_90\": \"1\",\n    \"line_91\": \"0301945536\",\n    \"line_92\": \"\",\n    \"line_93\": \"10091586\",\n    \"line_94\": \"JH5589\",\n    \"line_95\": \"SUPERSTAR 82 SUPCOL/PNKTIN/CREWHT\",\n    \"line_96\": \"M\",\n    \"line_97\": \"ORIGINALS\",\n    \"line_98\": \"100\",\n    \"line_99\": \"\",\n    \"line_100\": \"\",\n    \"line_101\": \"\",\n    \"line_102\": \"\",\n    \"line_103\": \"\",\n    \"line_104\": \"\",\n    \"line_105\": \"\",\n    \"line_106\": \"\",\n    \"line_107\": \"\",\n    \"line_108\": \"\",\n    \"line_109\": \"\",\n    \"line_110\": \"\",\n    \"line_111\": \"\",\n    \"line_112\": \"\",\n    \"line_113\": \"\",\n    \"line_114\": \"\",\n    \"line_115\": \"\",\n    \"line_116\": \"\",\n    \"line_117\": \"\",\n    \"line_118\": \"\",\n    \"line_119\": \"\",\n    \"line_120\": \"\",\n    \"line_121\": \"\",\n    \"line_122\": \"\",\n    \"line_123\": \"\",\n    \"line_124\": \"\",\n    \"line_125\": \"\",\n    \"line_126\": \"\",\n    \"line_127\": \"\",\n    \"line_128\": \"\",\n    \"line_129\": \"\",\n    \"line_130\": \"\",\n    \"line_131\": \"\",\n    \"line_132\": \"\",\n    \"line_133\": \"\",\n    \"line_134\": \"\",\n    \"line_135\": \"\",\n    \"line_136\": \"\",\n    \"line_137\": \"\",\n    \"line_138\": \"\",\n    \"line_139\": \"\",\n    \"line_140\": \"\",\n    \"line_141\": \"\",\n    \"line_142\": \"\",\n    \"line_143\": \"\",\n    \"line_144\": \"\",\n    \"line_145\": \"\",\n    \"line_146\": \"\",\n    \"line_147\": \"\",\n    \"line_148\": \"\",\n    \"line_149\": \"\",\n    \"line_150\": \"\",\n    \"line_151\": \"\",\n    \"line_152\": \"\",\n    \"line_153\": \"\",\n    \"line_154\": \"\",\n    \"line_155\": \"\",\n    \"line_156\": \"\",\n    \"line_157\": \"\",\n    \"line_158\": \"\",\n    \"line_159\": \"\",\n    \"line_160\": \"\",\n    \"line_161\": \"\",\n    \"line_162\": \"\",\n    \"line_163\": \"\",\n    \"line_164\": \"\",\n    \"line_165\": \"\",\n    \"line_166\": \"\",\n    \"line_167\": \"\",\n    \"line_168\": \"\",\n    \"line_169\": \"\",\n    \"line_170\": \"\",\n    \"line_171\": \"\",\n    \"line_172\": \"\",\n    \"line_173\": \"\",\n    \"line_174\": \"\",\n    \"line_175\": \"\",\n    \"line_176\": \"\",\n    \"line_177\": \"\",\n    \"line_178\": \"\",\n    \"line_179\": \"\",\n    \"line_180\": \"\",\n    \"line_181\": \"\",\n    \"line_182\": \"\",\n    \"line_183\": \"\",\n    \"line_184\": \"\",\n    \"line_185\": \"\",\n    \"line_186\": \"\",\n    \"line_187\": \"\",\n    \"line_188\": \"\",\n    \"line_189\": \"\",\n    \"line_190\": \"\",\n    \"line_191\": \"\",\n    \"line_192\": \"\",\n    \"line_193\": \"\",\n    \"line_194\": \"\",\n    \"line_195\": \"\",\n    \"line_196\": \"\",\n    \"line_197\": \"\",\n    \"line_198\": \"\",\n    \"line_199\": \"\",\n    \"line_200\": \"\",\n    \"line_201\": \"\",\n    \"line_202\": \"\",\n    \"line_203\": \"\",\n    \"line_204\": \"\",\n    \"line_205\": \"\",\n    \"line_206\": \"\",\n    \"line_207\": \"\",\n    \"line_208\": \"\",\n    \"line_209\": \"\",\n    \"line_210\": \"\",\n    \"line_211\": \"\",\n    \"line_212\": \"\",\n    \"line_213\": \"\",\n    \"line_214\": \"\",\n    \"line_215\": \"\",\n    \"line_216\": \"\",\n    \"line_217\": \"\",\n    \"line_218\": \"\",\n    \"line_219\": \"\",\n    \"line_220\": \"\",\n    \"line_221\": \"\",\n    \"line_222\": \"\",\n    \"line_223\": \"\",\n    \"line_224\": \"\",\n    \"line_225\": \"\",\n    \"line_226\": \"\",\n    \"line_227\": \"\",\n    \"line_228\": \"\",\n    \"line_229\": \"\",\n    \"line_230\": \"\",\n    \"line_231\": \"\",\n    \"line_232\": \"\",\n    \"line_233\": \"\",\n    \"line_234\": \"\",\n    \"line_235\": \"\",\n    \"line_236\": \"\",\n    \"line_237\": \"\",\n    \"line_238\": \"\",\n    \"line_239\": \"\",\n    \"line_240\": \"\",\n    \"line_241\": \"\",\n    \"line_242\": \"\",\n    \"line_243\": \"\",\n    \"line_244\": \"\",\n    \"line_245\": \"\",\n    \"line_246\": \"\",\n    \"line_247\": \"\",\n    \"line_248\": \"\",\n    \"line_249\": \"\",\n    \"line_250\": \"\",\n    \"line_251\": \"\",\n    \"line_252\": \"\",\n    \"line_253\": \"\",\n    \"line_254\": \"\",\n    \"line_255\": \"\",\n    \"line_256\": \"\",\n    \"line_257\": \"\",\n    \"line_258\": \"\",\n    \"line_259\": \"\",\n    \"line_260\": \"\",\n    \"line_261\": \"\",\n    \"line_262\": \"\",\n    \"line_263\": \"\",\n    \"line_264\": \"\",\n    \"line_265\": \"\",\n    \"line_266\": \"\",\n    \"line_267\": \"\",\n    \"line_268\": \"\",\n    \"line_269\": \"\",\n    \"line_270\": \"\",\n    \"line_271\": \"\",\n    \"line_272\": \"\",\n    \"line_273\": \"\",\n    \"line_274\": \"\",\n    \"line_275\": \"\",\n    \"line_276\": \"\",\n    \"line_277\": \"\",\n    \"line_278\": \"\",\n    \"line_279\": \"\",\n    \"line_280\": \"\",\n    \"line_281\": \"\",\n    \"line_282\": \"\",\n    \"line_283\": \"\",\n    \"line_284\": \"\",\n    \"line_285\": \"\",\n    \"line_286\": \"\",\n    \"line_287\": \"\",\n    \"line_288\": \"\",\n    \"line_289\": \"\",\n    \"line_290\": \"\",\n    \"line_291\": \"\",\n    \"line_292\": \"\",\n    \"line_293\": \"\",\n    \"line_294\": \"\",\n    \"line_295\": \"\",\n    \"line_296\": \"\",\n    \"line_297\": \"\",\n    \"line_298\": \"\",\n    \"line_299\": \"\",\n    \"line_300\": \"\",\n    \"line_301\": \"\",\n    \"line_302\": \"\",\n    \"line_303\": \"\",\n    \"line_304\": \"\",\n    \"line_305\": \"\",\n    \"line_306\": \"\",\n    \"line_307\": \"\",\n    \"line_308\": \"\",\n    \"line_309\": \"\",\n    \"line_310\": \"\",\n    \"line_311\": \"\",\n    \"line_312\": \"\",\n    \"line_313\": \"\",\n    \"line_314\": \"\",\n    \"line_315\": \"\",\n    \"line_316\": \"\",\n    \"line_317\": \"\",\n    \"line_318\": \"\",\n    \"line_319\": \"\",\n    \"line_320\": \"\",\n    \"line_321\": \"\",\n    \"line_322\": \"\",\n    \"line_323\": \"\",\n    \"line_324\": \"\",\n    \"line_325\": \"\",\n    \"line_326\": \"\",\n    \"line_327\": \"\",\n    \"line_328\": \"\",\n    \"line_329\": \"\",\n    \"line_330\": \"\",\n    \"line_331\": \"\",\n    \"line_332\": \"\",\n    \"line_333\": \"\",\n    \"line_334\": \"\",\n    \"line_335\": \"\",\n    \"line_336\": \"\",\n    \"line_337\": \"\",\n    \"line_338\": \"\",\n    \"line_339\": \"\",\n    \"line_340\": \"\",\n    \"line_341\": \"\",\n    \"line_342\": \"\",\n    \"line_343\": \"\",\n    \"line_344\": \"\",\n    \"line_345\": \"\",\n    \"line_346\": \"\",\n    \"line_347\": \"\",\n    \"line_348\": \"\",\n    \"line_349\": \"\",\n    \"line_350\": \"\",\n    \"line_351\": \"\",\n    \"line_352\": \"\",\n    \"line_353\": \"\",\n    \"line_354\": \"\",\n    \"line_355\": \"\",\n    \"line_356\": \"\",\n    \"line_357\": \"\",\n    \"line_358\": \"\",\n    \"line_359\": \"\",\n    \"line_360\": \"\",\n    \"line_361\": \"\",\n    \"line_362\": \"\",\n    \"line_363\": \"\",\n    \"line_364\": \"\",\n    \"line_365\": \"\",\n    \"line_366\": \"\",\n    \"line_367\": \"\",\n    \"line_368\": \"\",\n    \"line_369\": \"\",\n    \"line_370\": \"\",\n    \"line_371\": \"\",\n    \"line_372\": \"\",\n    \"line_373\": \"\",\n    \"line_374\": \"\",\n    \"line_375\": \"\",\n    \"line_376\": \"\",\n    \"line_377\": \"\",\n    \"line_378\": \"\",\n    \"line_379\": \"\",\n    \"line_380\": \"\",\n    \"line_381\": \"\",\n    \"line_382\": \"\",\n    \"line_383\": \"\",\n    \"line_384\": \"\",\n    \"line_385\": \"\",\n    \"line_386\": \"\",\n    \"line_387\": \"\",\n    \"line_388\": \"\",\n    \"line_389\": \"\",\n    \"line_390\": \"\",\n    \"line_391\": \"\",\n    \"line_392\": \"\",\n    \"line_393\": \"\",\n    \"line_394\": \"\",\n    \"line_395\": \"\",\n    \"line_396\": \"\",\n    \"line_397\": \"\",\n    \"line_398\": \"\",\n    \"line_399\": \"\",\n    \"line_400\": \"\",\n    \"line_401\": \"\",\n    \"line_402\": \"\",\n    \"line_403\": \"\",\n    \"line_404\": \"\",\n    \"line_405\": \"\",\n    \"line_406\": \"\",\n    \"line_407\": \"\",\n    \"line_40\n```json\n{\n    \"line_number\": 0,\n    \"text_content\": \"THE LOOK (MACAO COMMERCIAL OFFSHORE) COM\"\n}\n```\nWarning: OCR response for page 2 was not a list: {\n    \"line_number\": 0,\n    \"text_content\": \"THE LOOK (MACAO COMMERCIAL OFFSHORE) COM\"\n}\n```json\n[\n\t{\"bbox_2d\": [65, 63, 476, 80], \"text_content\": \"THE LOOK (MACAO COMMERCIAL OFFSHORE) COM\"},\n\t{\"bbox_2d\": [65, 81, 186, 90], \"text_content\": \"PANYI LIMITED\"},\n\t{\"bbox_2d\": [65, 91, 186, 100], \"text_content\": \"UNIT 1703-1704 17/F\"},\n\t{\"bbox_2d\": [65, 101, 134, 110], \"text_content\": \"AIA TOWER\"},\n\t{\"bbox_2d\": [65, 111, 476, 120], \"text_content\": \"AVENIDA COMERCIAL DE MACAU NOS 251A - 301\"},\n\t{\"bbox_2d\": [65, 121, 155, 130], \"text_content\": \"MACAU,999999\"},\n\t{\"bbox_2d\": [65, 131, 189, 140], \"text_content\": \"MACAO\"},\n\t{\"bbox_2d\": [65, 141, 189, 150], \"text_content\": \"Phone: 853-85920822\"},\n\t{\"bbox_2d\": [1036, 68, 1122, 78], \"text_content\": \"Invoice Number\"},\n\t{\"bbox_2d\": [1036, 81, 1126, 90], \"text_content\": \"PCI0810414340\"},\n\t{\"bbox_2d\": [220, 183, 1354, 238], \"text_content\": \"PO Lines Customer Size Sourcing Size / Manufacturing Size Code Description Amount Per Unit Total Quantity Amount Per Charge\"},\n\t{\"bbox_2d\": [217, 223, 350, 234], \"text_content\": \"10,20,30,40,50,60,70,80\"},\n\t{\"bbox_2d\": [360, 223, 472, 234], \"text_content\": \"3-,4,4-,5,5-,6,6-,7\"},\n\t{\"bbox_2d\": [482, 223, 603, 234], \"text_content\": \"3-,4,4-,5,5-,6,6-,7\"},\n\t{\"bbox_2d\": [613, 223, 688, 234], \"text_content\": \"ZMS1\"},\n\t{\"bbox_2d\": [698, 223, 888, 234], \"text_content\": \"MOS quantity flat\"},\n\t{\"bbox_2d\": [898, 223, 976, 234], \"text_content\": \"1.01 USD per Unit\"},\n\t{\"bbox_2d\": [986, 223, 1124, 234], \"text_content\": \"148\"},\n\t{\"bbox_2d\": [1282, 223, 1354, 234], \"text_content\": \"149.48\"},\n\t{\"bbox_2d\": [1285, 254, 1354, 265], \"text_content\": \"Net Amount\"},\n\t{\"bbox_2d\": [1285, 276, 1354, 287], \"text_content\": \"2,544.12\"},\n\t{\"bbox_2d\": [65, 307, 178, 319], \"text_content\": \"Goods Description\"},\n\t{\"bbox_2d\": [590, 336, 1354, 347], \"text_content\": \"Customer Size Sourcing Size/Manufacturing Size HTS Upper Material Gender Quantity\"},\n\t{\"bbox_2d\": [613, 348, 1256, 369], \"text_content\": \"3-,4,4-,5,5-,6,6-,7 3-,4,4-,5,5-,6,6-,7 64039920 FEMALES FTW WITH OUTER SOLES: RUB./PLASTIC/LEATHER W 148\"},\n\t{\"bbox_2d\": [65, 404, 1354, 465], \"text_content\": \"PO No PO Line Aggregator Market PO Number Sales Order No Working No Article No Article Description Gender Category Total Qty\"},\n\t{\"bbox_2d\": [109, 445, 178, 456], \"text_content\": \"0135879514\"},\n\t{\"bbox_2d\": [220, 408, 250, 419], \"text_content\": \"1\"},\n\t{\"bbox_2d\": [260, 408, 376, 419], \"text_content\": \"PO Line Aggregator\"},\n\t{\"bbox_2d\": [217, 420, 376, 431], \"text_content\": \"Market PO Number\"},\n\t{\"bbox_2d\": [217, 432, 376, 443], \"text_content\": \"Sales Order No\"},\n\t{\"bbox_2d\": [217, 444, 376, 455], \"text_content\": \"Working No\"},\n\t{\"bbox_2d\": [217, 456, 376, 467], \"text_content\": \"Article No\"},\n\t{\"bbox_2d\": [217, 468, 1354, 479], \"text_content\": \"Article Description Gender Category Total Qty\"},\n\t{\"bbox_2d\": [217, 470, 1354, 481], \"text_content\": \"SUPERSTAR II W CBLACK/FTWWHT/CBLACK W ORIGINS 875\"},\n\t{\"bbox_2d\": [590, 479, 1354, 490], \"text_content\": \"Customer Size 3-,4,4-,5,5-,6,6-,7 3-,4,4-,5,5-,6,6-,7 64039920 FEMALES FTW WITH OUTER SOLES: RUB./PLASTIC/LEATHER W 875\"},\n\t{\"bbox_2d\": [590, 491, 1354, 502], \"text_content\": \"Total Quantity 3- 4- 5- 6- 7- 8- 9- 3- Total Quantity 875 17.62\"},\n\t{\"bbox_2d\": [1285, 584, 1354, 595], \"text_content\": \"Net Amount\"},\n\t{\"bbox_2d\": [1295, 606, 1354, 617], \"text_content\": \"15,417.50\"},\n\t{\"bbox_2d\": [65, 554, 141, 567], \"text_content\": \"Adjustments\"},\n\t{\"bbox_2d\": [65, 636, 178, 648], \"text_content\": \"Goods Description\"},\n\t{\"bbox_2d\": [590, 666, 1354, 707], \"text_content\": \"Customer Size Sourcing Size/Manufacturing Size HTS Upper Material Gender Quantity\"},\n\t{\"bbox_2d\": [590, 667, 1256, 688], \"text_content\": \"3-,4,4-,5,5-,6,6-,7,7-,8,8-,9,9-,3 3-,4,4-,5,5-,6,6-,7,7-,8,8-,9,9-,3 64039920 FEMALES FTW WITH OUTER SOLES: RUB./PLASTIC/LEATHER W 875\"},\n\t{\"bbox_2d\": [65, 794, 1354, 805], \"text_content\": \"Total Quantity 1336\"},\n\t{\"bbox_2d\": [65, 815, 1354, 826], \"text_content\": \"Total Carton 139\"},\n\t{\"bbox_2d\": [65, 836, 1354, 847], \"text_content\": \"Total Gross Weight 1393.184 KG\"},\n\t{\"bbox_2d\": [65, 857, 1354, 868], \"text_content\": \"Total Net Weight 1281.970 KG\"},\n\t{\"bbox_2d\": [65, 878, 1354, 889], \"text_content\": \"Total Net Net Weight 1041.270 KG\"},\n\t{\"bbox_2d\": [65, 899, 1354, 910], \"text_content\": \"Total PO Net Amount 25,470.34\"},\n\t{\"bbox_2d\": [65, 911, 1354, 922], \"text_content\": \"Additional charge 57.32\"},\n\t{\"bbox_2d\": [65, 932, 1354, 943], \"text_content\": \"Total VAT 0.00\"},\n\t{\"bbox_2d\": [65, 953, 1354, 964], \"text_content\": \"Invoice Total 25,527.66\"},\n\t{\"bbox_2d\": [750, 991, 812, 1002], \"text_content\": \"Page 3 of 4\"},\n]\n```\nWarning: Font file './00_Dataset/NotoSansCJK-Regular.ttc' not found. Using default font.\n```json\n[\n\t{\"bbox_2d\": [67, 64, 475, 80], \"text_content\": \"THE LOOK (MACAO COMMERCIAL OFFSHORE) COM\"},\n\t{\"bbox_2d\": [67, 81, 153, 90], \"text_content\": \"PANYI LIMITED\"},\n\t{\"bbox_2d\": [67, 91, 186, 103], \"text_content\": \"UNIT 1703-1704 17/F\"},\n\t{\"bbox_2d\": [67, 104, 134, 115], \"text_content\": \"AIA TOWER\"},\n\t{\"bbox_2d\": [67, 116, 361, 130], \"text_content\": \"AVENIDA COMERCIAL DE MACAU NOS 251A - 301\"},\n\t{\"bbox_2d\": [67, 131, 155, 142], \"text_content\": \"MACAU 99999\"},\n\t{\"bbox_2d\": [67, 143, 112, 154], \"text_content\": \"MACAO\"},\n\t{\"bbox_2d\": [67, 155, 189, 167], \"text_content\": \"Phone: 853-85920822\"},\n\t{\"bbox_2d\": [1036, 68, 1122, 81], \"text_content\": \"Invoice Number\"},\n\t{\"bbox_2d\": [1036, 83, 1125, 96], \"text_content\": \"PCI0810414340\"},\n\t{\"bbox_2d\": [67, 223, 181, 235], \"text_content\": \"For and on behalf of\"},\n\t{\"bbox_2d\": [67, 242, 384, 256], \"text_content\": \"THE LOOK (MACAO COMMERCIAL OFFSHORE) COM\"},\n\t{\"bbox_2d\": [67, 332, 186, 345], \"text_content\": \"Authorized Signature\"},\n\t{\"bbox_2d\": [913, 365, 1238, 379], \"text_content\": \"This document has been digitally signed and authorized by\"},\n\t{\"bbox_2d\": [884, 384, 1238, 400], \"text_content\": \"The Look (Macao Commercial Offshore) Company Limited(789)\"},\n]\n```\nWarning: Font file './00_Dataset/NotoSansCJK-Regular.ttc' not found. Using default font.\n","output_type":"stream"}],"execution_count":1}]}