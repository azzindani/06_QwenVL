{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows omni_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 00 Install & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:06:22.308399Z",
     "iopub.status.busy": "2025-04-14T07:06:22.308179Z",
     "iopub.status.idle": "2025-04-14T07:06:49.493562Z",
     "shell.execute_reply": "2025-04-14T07:06:49.492696Z",
     "shell.execute_reply.started": "2025-04-14T07:06:22.308379Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import random\n",
    "import io\n",
    "import ast\n",
    "import xml.etree.ElementTree as ET\n",
    "import os.path as osp\n",
    "import math\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "from icecream import ic\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info, smart_resize\n",
    "from PIL import Image, ImageDraw, ImageFont, ImageColor\n",
    "from IPython.display import display, Markdown\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from qwen_agent.llm.fncall_prompts.nous_fncall_prompt import (\n",
    "    NousFnCallPrompt,\n",
    "    Message,\n",
    "    ContentItem,\n",
    ")\n",
    "from utils.agent_function_call import MobileUse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01 Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_compute_dtype = torch.float16,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-14T07:06:49.494981Z",
     "iopub.status.busy": "2025-04-14T07:06:49.494404Z",
     "iopub.status.idle": "2025-04-14T07:09:20.429998Z",
     "shell.execute_reply": "2025-04-14T07:09:20.429296Z",
     "shell.execute_reply.started": "2025-04-14T07:06:49.494957Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    model_path,\n",
    "    quantization_config = bnb_config,\n",
    "    device_map = 'auto',\n",
    ").to(device) #''\n",
    "processor = AutoProcessor.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02 Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_point(image: Image.Image, point: list, color = None):\n",
    "    from copy import deepcopy\n",
    "    if isinstance(color, str):\n",
    "        try:\n",
    "            color = ImageColor.getrgb(color)\n",
    "            color = color + (128,)  \n",
    "        except ValueError:\n",
    "            color = (255, 0, 0, 128)  \n",
    "    else:\n",
    "        color = (255, 0, 0, 128)  \n",
    " \n",
    "    overlay = Image.new('RGBA', image.size, (255, 255, 255, 0))\n",
    "    overlay_draw = ImageDraw.Draw(overlay)\n",
    "    radius = min(image.size) * 0.05\n",
    "    x, y = point\n",
    "\n",
    "    overlay_draw.ellipse(\n",
    "        [(x - radius, y - radius), (x + radius, y + radius)],\n",
    "        fill=color  # Red with 50% opacity\n",
    "    )\n",
    "\n",
    "    image = image.convert('RGBA')\n",
    "    combined = Image.alpha_composite(image, overlay)\n",
    "\n",
    "    return combined.convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mobile_agent(user_query, screenshot):\n",
    "    # The resolution of the device will be written into the system prompt. \n",
    "    dummy_image = Image.open(screenshot)\n",
    "    resized_height, resized_width  = smart_resize(dummy_image.height,\n",
    "        dummy_image.width,\n",
    "        factor = processor.image_processor.patch_size * processor.image_processor.merge_size,\n",
    "        min_pixels = processor.image_processor.min_pixels,\n",
    "        max_pixels = processor.image_processor.max_pixels,)\n",
    "\n",
    "    mobile_use = MobileUse(\n",
    "        cfg = {'display_width_px' : resized_width, 'display_height_px' : resized_height}\n",
    "    )\n",
    "\n",
    "    # Build messages\n",
    "    prompt_builder = NousFnCallPrompt()\n",
    "    message = prompt_builder.preprocess_fncall_messages(\n",
    "        messages = [\n",
    "            Message(role = 'system', content = [ContentItem(text = 'You are a helpful assistant.')]),\n",
    "            Message(role = 'user', content = [\n",
    "                ContentItem(text = user_query),\n",
    "                ContentItem(image = f\"file://{screenshot}\")\n",
    "            ]),\n",
    "        ],\n",
    "        functions = [mobile_use.function],\n",
    "        lang = None,\n",
    "    )\n",
    "    message = [msg.model_dump() for msg in message]\n",
    "\n",
    "    text = processor.apply_chat_template(message, tokenize = False, add_generation_prompt = True)\n",
    "    print('text', text)\n",
    "    inputs = processor(text = [text], images = [dummy_image], padding = True, return_tensors = 'pt').to('cuda')\n",
    "\n",
    "    streamer = TextStreamer(processor.tokenizer, skip_special_tokens = True, skip_prompt = True)\n",
    "    \n",
    "    output_ids = model.generate(**inputs, max_new_tokens = 2048, streamer = streamer)\n",
    "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
    "    output_text = processor.batch_decode(generated_ids, skip_special_tokens = True, clean_up_tokenization_spaces = True)[0]\n",
    "    print(output_text)\n",
    "\n",
    "    # Qwen will perform action thought function call\n",
    "    action = json.loads(output_text.split('<tool_call>\\n')[1].split('\\n</tool_call>')[0])\n",
    "\n",
    "    # As an example, we visualize the \"click\" action by draw a green circle onto the image.\n",
    "    display_image = dummy_image.resize((resized_width, resized_height))\n",
    "    if action['arguments']['action'] == 'click':\n",
    "        display_image = draw_point(dummy_image, action['arguments']['coordinate'], color = 'green')\n",
    "        display(display_image)\n",
    "    else:\n",
    "        display(display_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 03 Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.01 Mobile Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "screenshot = './00_Dataset/mobile_en_example.png'\n",
    "\n",
    "# The operation history can be orgnized by Step x: [action]; Step x+1: [action]...\n",
    "user_query = 'The user query:  Open the file manager app and view the au_uu_SzH3yR2.mp3 file in MUSIC Folder\\nTask progress (You have done the following operation on the current device): Step 1: {\"name\": \"mobile_use\", \"arguments\": {\"action\": \"open\", \"text\": \"File Manager\"}}; '\n",
    "\n",
    "mobile_agent(user_query, screenshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03.02 Chinese App & Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "screenshot = './00_Dataset/mobile_zh_example.jpg'\n",
    "\n",
    "# The operation history can be orgnized by Step x: [action]; Step x+1: [action]...\n",
    "user_query = 'The user query:在盒马中,打开购物车，结算（到付款页面即可） (You have done the following operation on the current device):'\n",
    "\n",
    "mobile_agent(user_query, screenshot)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "omni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
