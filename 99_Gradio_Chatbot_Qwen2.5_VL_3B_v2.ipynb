{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Windows omni_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextStreamer,\n",
    "    TextIteratorStreamer\n",
    ")\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import io\n",
    "import sys\n",
    "import re\n",
    "import threading\n",
    "\n",
    "# Load model and processor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "\n",
    "if not os.path.exists(model_path) or not os.path.isdir(model_path):\n",
    "    print(f\"Error: Model path '{model_path}' does not exist or is not a directory.\")\n",
    "    print(\"Please ensure your model files are in the specified directory.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    tokenizer = processor.tokenizer\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or processor: {e}\")\n",
    "    print(\"Please ensure the model path is correct and all necessary files are present.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Updating processor config files...\")\n",
    "try:\n",
    "    processor.save_pretrained(model_path)\n",
    "    print(\"Processor config files updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save processor config files. This might be due to permissions or an incomplete model download: {e}\")\n",
    "\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"General Assistant\": \"You are a helpful assistant that can analyze images and answer questions.\",\n",
    "    \"Image Analyzer\": \"You are an expert image analyst. Provide detailed descriptions and analysis of images.\",\n",
    "    \"OCR Reader\": \"You are an OCR specialist. Extract and transcribe all text from images accurately.\",\n",
    "    \"Medical Assistant\": \"You are a medical AI assistant. Analyze medical images and provide insights (for educational purposes only).\",\n",
    "    \"Educational Tutor\": \"You are an educational tutor. Help explain concepts shown in images and answer related questions.\"\n",
    "}\n",
    "\n",
    "def clean_response(text: str) -> str:\n",
    "    text = re.sub(r'<\\|im_end\\|>', '', text)\n",
    "    text = re.sub(r'^\\s*<\\|im_start\\|>assistant\\s*', '', text) \n",
    "    text = re.sub(r'\\s*<\\|im_start\\|>user\\s*', '', text) \n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def inference_with_streaming(\n",
    "    prompt,\n",
    "    image_path,\n",
    "    system_prompt='You are a helpful assistant',\n",
    "    max_new_tokens=32000,\n",
    "    min_pixels=512 * 28 * 28,\n",
    "    max_pixels=2048 * 28 * 28\n",
    "):\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': system_prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {\n",
    "                    'type': 'image',\n",
    "                    'image': image_path,\n",
    "                    'min_pixels': min_pixels,\n",
    "                    'max_pixels': max_pixels,\n",
    "                },\n",
    "                {'type': 'text', 'text': prompt},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Preparation for inference\n",
    "    text = processor.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    print('input:\\n', text)\n",
    "    image_inputs, video_inputs = process_vision_info(messages)\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images=image_inputs,\n",
    "        videos=video_inputs,\n",
    "        padding=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Create TextIteratorStreamer\n",
    "    streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "    \n",
    "    # Generation parameters\n",
    "    generation_kwargs = {\n",
    "        **inputs,\n",
    "        'max_new_tokens': max_new_tokens,\n",
    "        'streamer': streamer,\n",
    "        'do_sample': True,\n",
    "        'temperature': 0.7,\n",
    "        'pad_token_id': tokenizer.eos_token_id,\n",
    "    }\n",
    "\n",
    "    # Start generation in a separate thread\n",
    "    thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "    thread.start()\n",
    "\n",
    "    # Yield tokens as they are generated\n",
    "    generated_text = \"\"\n",
    "    for new_token in streamer:\n",
    "        generated_text += new_token\n",
    "        yield generated_text\n",
    "\n",
    "    thread.join()\n",
    "    print('output:\\n', generated_text)\n",
    "    return generated_text\n",
    "\n",
    "def chat_with_history(\n",
    "    message: str,\n",
    "    image: Optional[Image.Image],\n",
    "    history: List[Dict[str, str]], \n",
    "    usage_mode: str,\n",
    "    custom_system_prompt: str,\n",
    "    max_tokens: int,\n",
    "    min_pixels: int,\n",
    "    max_pixels: int\n",
    "):\n",
    "    try:\n",
    "        current_system_prompt = custom_system_prompt.strip() or SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "        \n",
    "        if not message.strip() and not image:\n",
    "            yield history, \"\"\n",
    "            return\n",
    "\n",
    "        # Handle image upload\n",
    "        temp_image_path = None\n",
    "        if image:\n",
    "            temp_image_dir = \"temp_images\"\n",
    "            os.makedirs(temp_image_dir, exist_ok=True)\n",
    "            temp_image_path = os.path.join(temp_image_dir, f\"uploaded_image_{os.getpid()}_{threading.get_ident()}.png\") \n",
    "            image.save(temp_image_path)\n",
    "\n",
    "            # Add to Gradio history with markdown image for display\n",
    "            image_md_for_gradio = f'![Uploaded Image](file={temp_image_path})' \n",
    "            if message:\n",
    "                history.append({\"role\": \"user\", \"content\": f\"{image_md_for_gradio}\\n\\n{message}\"})\n",
    "            else:\n",
    "                history.append({\"role\": \"user\", \"content\": image_md_for_gradio})\n",
    "        else:\n",
    "            if message.strip():\n",
    "                history.append({\"role\": \"user\", \"content\": message})\n",
    "\n",
    "        # Add assistant placeholder\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "        yield history, \"\"\n",
    "\n",
    "        # Call inference function with streaming\n",
    "        if temp_image_path:\n",
    "            for partial_response in inference_with_streaming(\n",
    "                prompt=message or \"Describe this image.\",\n",
    "                image_path=temp_image_path,\n",
    "                system_prompt=current_system_prompt,\n",
    "                max_new_tokens=max_tokens,\n",
    "                min_pixels=min_pixels,\n",
    "                max_pixels=max_pixels\n",
    "            ):\n",
    "                cleaned_partial = clean_response(partial_response)\n",
    "                history[-1][\"content\"] = cleaned_partial\n",
    "                yield history, \"\"\n",
    "        else:\n",
    "            history[-1][\"content\"] = \"Image is required for this model.\"\n",
    "            yield history, \"\"\n",
    "        \n",
    "        # Clean up temporary image file\n",
    "        if temp_image_path and os.path.exists(temp_image_path):\n",
    "            try:\n",
    "                os.remove(temp_image_path)\n",
    "            except OSError as e:\n",
    "                print(f\"Error deleting temp file {temp_image_path}: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        error_msg = f\"An unexpected error occurred: {str(e)}\"\n",
    "        print(f\"Error in chat_with_history: {e}\") \n",
    "        if not history or history[-1][\"role\"] != \"assistant\":\n",
    "            history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "        else:\n",
    "            history[-1][\"content\"] = error_msg\n",
    "        yield history, \"\"\n",
    "\n",
    "def clear_history():\n",
    "    return [], None, \"\"\n",
    "\n",
    "# Gradio Interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Qwen2.5-VL Conversational Assistant\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            image_upload = gr.Image(type=\"pil\", label=\"Upload Image\", sources=[\"upload\"], interactive=True)\n",
    "            gr.Markdown(\"Image will be displayed in the chat history.\")\n",
    "\n",
    "            with gr.Accordion(\"System Settings\", open=False):\n",
    "                usage_mode = gr.Radio(\n",
    "                    list(SYSTEM_PROMPTS.keys()),\n",
    "                    value=\"General Assistant\",\n",
    "                    label=\"Usage Mode\",\n",
    "                    interactive=True\n",
    "                )\n",
    "                custom_system_prompt = gr.Textbox(\n",
    "                    label=\"Custom System Prompt (Overrides Usage Mode)\",\n",
    "                    placeholder=\"Enter your custom system prompt here...\",\n",
    "                    lines=2,\n",
    "                    interactive=True\n",
    "                )\n",
    "                max_tokens = gr.Slider(\n",
    "                    minimum=100,\n",
    "                    maximum=32000,\n",
    "                    value=800,\n",
    "                    label=\"Max New Tokens\",\n",
    "                    step=100,\n",
    "                    interactive=True\n",
    "                )\n",
    "                min_pixels = gr.Slider(\n",
    "                    minimum=1000,\n",
    "                    maximum=1000000,\n",
    "                    value=224 * 224,\n",
    "                    label=\"Min Pixels for Image Processing\",\n",
    "                    step=1000,\n",
    "                    interactive=True\n",
    "                )\n",
    "                max_pixels = gr.Slider(\n",
    "                    minimum=1000000,\n",
    "                    maximum=50000000,\n",
    "                    value=1280 * 28 * 28,\n",
    "                    label=\"Max Pixels for Image Processing\",\n",
    "                    step=100000,\n",
    "                    interactive=True\n",
    "                )\n",
    "            \n",
    "            clear_btn = gr.Button(\"Clear Chat\")\n",
    "\n",
    "        with gr.Column(scale=2):\n",
    "            chatbot = gr.Chatbot(\n",
    "                label=\"Conversation History\",\n",
    "                elem_id=\"chatbot\",\n",
    "                height=500,\n",
    "                render_markdown=True,\n",
    "                type='messages' \n",
    "            )\n",
    "            msg = gr.Textbox(label=\"Your Message\", placeholder=\"Type your message here...\", lines=2)\n",
    "            send_btn = gr.Button(\"Send\")\n",
    "    \n",
    "    msg.submit(\n",
    "        chat_with_history,\n",
    "        inputs=[msg, image_upload, chatbot, usage_mode, custom_system_prompt, max_tokens, min_pixels, max_pixels],\n",
    "        outputs=[chatbot, msg],\n",
    "    )\n",
    "    send_btn.click(\n",
    "        chat_with_history,\n",
    "        inputs=[msg, image_upload, chatbot, usage_mode, custom_system_prompt, max_tokens, min_pixels, max_pixels],\n",
    "        outputs=[chatbot, msg],\n",
    "    )\n",
    "    clear_btn.click(clear_history, outputs=[chatbot, image_upload, msg])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        #server_name = \"0.0.0.0\",\n",
    "        #server_port = 7860,\n",
    "        #share = False,\n",
    "        debug = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "from transformers import (\n",
    "    Qwen2_5_VLForConditionalGeneration,\n",
    "    AutoTokenizer,\n",
    "    AutoProcessor,\n",
    "    BitsAndBytesConfig,\n",
    "    TextIteratorStreamer\n",
    ")\n",
    "# Make sure qwen_vl_utils.py is in the same directory as this script.\n",
    "# This file contains the 'process_vision_info' function crucial for handling\n",
    "# vision inputs for Qwen-VL.\n",
    "from qwen_vl_utils import process_vision_info \n",
    "from PIL import Image\n",
    "from typing import List, Tuple, Dict, Any, Optional\n",
    "import io\n",
    "import sys\n",
    "import re\n",
    "import threading\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "# --- Model Loading and Configuration ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "model_path = './00_Model/Qwen2.5-VL-3B-Instruct'\n",
    "\n",
    "# Check if model path exists\n",
    "if not os.path.exists(model_path) or not os.path.isdir(model_path):\n",
    "    print(f\"Error: Model path '{model_path}' does not exist or is not a directory.\")\n",
    "    print(\"Please ensure your model files are in the specified directory.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "try:\n",
    "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "        model_path,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map='auto',\n",
    "        torch_dtype=torch.float16\n",
    "    ).to(device)\n",
    "    processor = AutoProcessor.from_pretrained(model_path)\n",
    "    tokenizer = processor.tokenizer\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model or processor: {e}\")\n",
    "    print(\"Please ensure the model path is correct and all necessary files are present.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"Updating processor config files...\")\n",
    "try:\n",
    "    processor.save_pretrained(model_path)\n",
    "    print(\"Processor config files updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save processor config files. This might be due to permissions or an incomplete model download: {e}\")\n",
    "\n",
    "# --- System Prompts for Different Modes ---\n",
    "SYSTEM_PROMPTS = {\n",
    "    \"General Assistant\": \"You are a helpful assistant that can analyze images and answer questions.\",\n",
    "    \"Image Analyzer\": \"You are an expert image analyst. Provide detailed descriptions and analysis of images.\",\n",
    "    \"OCR Reader\": \"You are an OCR specialist. Extract and transcribe all text from images accurately.\",\n",
    "    \"Medical Assistant\": \"You are a medical AI assistant. Analyze medical images and provide insights (for educational purposes only).\",\n",
    "    \"Educational Tutor\": \"You are an educational tutor. Help explain concepts shown in images and answer related questions.\"\n",
    "}\n",
    "\n",
    "# --- Utility Functions ---\n",
    "def clean_response(text: str) -> str:\n",
    "    \"\"\"Removes special tokens and cleans up the generated text.\"\"\"\n",
    "    text = re.sub(r'<\\|im_end\\|>', '', text)\n",
    "    text = re.sub(r'^\\s*<\\|im_start\\|>assistant\\s*', '', text) \n",
    "    text = re.sub(r'\\s*<\\|im_start\\|>user\\s*', '', text) \n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def validate_and_process_image_paths(image_paths: Optional[List[str]]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Validates image paths and ensures they exist and are accessible.\n",
    "    Returns a list of valid image paths.\n",
    "    \"\"\"\n",
    "    if not image_paths:\n",
    "        return []\n",
    "    \n",
    "    valid_paths = []\n",
    "    for path in image_paths:\n",
    "        if not path:\n",
    "            continue\n",
    "            \n",
    "        # Convert to absolute path for consistency\n",
    "        abs_path = os.path.abspath(path)\n",
    "        \n",
    "        # Check if file exists\n",
    "        if not os.path.exists(abs_path):\n",
    "            print(f\"Warning: Image path does not exist: {abs_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a file (not a directory)\n",
    "        if not os.path.isfile(abs_path):\n",
    "            print(f\"Warning: Path is not a file: {abs_path}\")\n",
    "            continue\n",
    "            \n",
    "        # Check if it's a valid image file by trying to open it\n",
    "        try:\n",
    "            with Image.open(abs_path) as img:\n",
    "                # Just opening to validate - we don't need to do anything with it\n",
    "                pass\n",
    "            valid_paths.append(abs_path)\n",
    "            print(f\"Valid image path added: {abs_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Cannot open image {abs_path}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return valid_paths\n",
    "\n",
    "def inference_with_streaming(\n",
    "    prompt: Optional[str],\n",
    "    image_paths: Optional[List[str]],\n",
    "    system_prompt: str,\n",
    "    max_new_tokens: int,\n",
    "    min_pixels: int,\n",
    "    max_pixels: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs streaming inference with the Qwen-VL model.\n",
    "    Constructs messages for the model's expected format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        messages = [\n",
    "            {\n",
    "                'role': 'system',\n",
    "                'content': system_prompt\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        user_content_for_model = []\n",
    "        \n",
    "        # Validate and process image paths\n",
    "        valid_image_paths = validate_and_process_image_paths(image_paths)\n",
    "        \n",
    "        if valid_image_paths:\n",
    "            for path in valid_image_paths:\n",
    "                # This is the format expected by Qwen-VL's `process_vision_info`\n",
    "                user_content_for_model.append({\n",
    "                    'type': 'image',\n",
    "                    'image': path,  # Use the validated absolute path\n",
    "                    'min_pixels': min_pixels,\n",
    "                    'max_pixels': max_pixels,\n",
    "                })\n",
    "        \n",
    "        if prompt:\n",
    "            user_content_for_model.append({'type': 'text', 'text': prompt})\n",
    "        \n",
    "        # If no text or image provided, model has nothing to process\n",
    "        if not user_content_for_model:\n",
    "            yield \"Please provide a message or an image to the model.\"\n",
    "            return\n",
    "\n",
    "        messages.append({'role': 'user', 'content': user_content_for_model})\n",
    "\n",
    "        print(f\"DEBUG: Messages sent to process_vision_info and processor:\")\n",
    "        for i, msg in enumerate(messages):\n",
    "            print(f\"  Message {i}: {msg}\")\n",
    "\n",
    "        # Preparation for inference\n",
    "        text = processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        print(f'Applied chat template:\\n{text}')\n",
    "        \n",
    "        # Process vision info - this is where the original error likely occurred\n",
    "        try:\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            print(f\"DEBUG: Processed vision - Images: {len(image_inputs) if image_inputs else 0}, Videos: {len(video_inputs) if video_inputs else 0}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_vision_info: {e}\")\n",
    "            yield f\"Error processing images: {str(e)}\"\n",
    "            return\n",
    "        \n",
    "        # Process inputs for the model\n",
    "        try:\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors='pt',\n",
    "            )\n",
    "            inputs = inputs.to(device)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in processor: {e}\")\n",
    "            yield f\"Error processing inputs for model: {str(e)}\"\n",
    "            return\n",
    "\n",
    "        # Create TextIteratorStreamer for token-by-token generation\n",
    "        streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "        \n",
    "        # Generation parameters\n",
    "        generation_kwargs = {\n",
    "            **inputs,\n",
    "            'max_new_tokens': max_new_tokens,\n",
    "            'streamer': streamer,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'pad_token_id': tokenizer.eos_token_id,\n",
    "        }\n",
    "\n",
    "        # Start generation in a separate thread to allow streaming\n",
    "        thread = threading.Thread(target=model.generate, kwargs=generation_kwargs)\n",
    "        thread.start()\n",
    "\n",
    "        # Yield tokens as they are generated\n",
    "        generated_text = \"\"\n",
    "        for new_token in streamer:\n",
    "            generated_text += new_token\n",
    "            yield generated_text\n",
    "\n",
    "        thread.join()  # Wait for the generation thread to complete\n",
    "        print(f'Final output:\\n{generated_text}')\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error in inference: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        yield error_msg\n",
    "\n",
    "def chat_with_history(\n",
    "    message: Dict[str, Any],  # Input from gr.MultimodalTextbox\n",
    "    history: List[Dict[str, Any]],  # History for gr.Chatbot(type='messages')\n",
    "    usage_mode: str,\n",
    "    custom_system_prompt: str,\n",
    "    max_tokens: int,\n",
    "    min_pixels: int,\n",
    "    max_pixels: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Manages chat conversation, processes user input (text/image),\n",
    "    calls inference, and updates Gradio chatbot history.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        current_system_prompt = custom_system_prompt.strip() or SYSTEM_PROMPTS.get(usage_mode, SYSTEM_PROMPTS[\"General Assistant\"])\n",
    "        \n",
    "        user_text = message.get(\"text\", \"\").strip() if message.get(\"text\") else \"\"\n",
    "        user_files = message.get(\"files\", []) if message.get(\"files\") else []\n",
    "\n",
    "        print(f\"DEBUG: Received message - Text: '{user_text}', Files: {user_files}\")\n",
    "\n",
    "        if not user_text and not user_files:\n",
    "            # If nothing is provided, return current history and clear input\n",
    "            yield history, {\"text\": \"\", \"files\": []}\n",
    "            return\n",
    "\n",
    "        # Prepare user message content for Gradio Chatbot display\n",
    "        user_display_content_for_chatbot = []\n",
    "        \n",
    "        if user_files:\n",
    "            for file_path in user_files:\n",
    "                # For Gradio Chatbot (type='messages'), use \"file\" key for display\n",
    "                if os.path.exists(file_path):\n",
    "                    user_display_content_for_chatbot.append({\"file\": file_path})\n",
    "                else:\n",
    "                    print(f\"Warning: File does not exist for display: {file_path}\")\n",
    "                    \n",
    "        if user_text:\n",
    "            # For pure text, append the string\n",
    "            user_display_content_for_chatbot.append(user_text)\n",
    "        \n",
    "        # Determine final content for chatbot display\n",
    "        if len(user_display_content_for_chatbot) == 1 and isinstance(user_display_content_for_chatbot[0], str):\n",
    "            final_user_content_for_chatbot = user_display_content_for_chatbot[0]\n",
    "        else:\n",
    "            final_user_content_for_chatbot = user_display_content_for_chatbot\n",
    "\n",
    "        # Append user message to history\n",
    "        history.append({\"role\": \"user\", \"content\": final_user_content_for_chatbot})\n",
    "        \n",
    "        # Add an empty assistant placeholder for streaming\n",
    "        history.append({\"role\": \"assistant\", \"content\": \"\"})\n",
    "        \n",
    "        # Yield history immediately to show user's message and clear the input textbox\n",
    "        yield history, {\"text\": \"\", \"files\": []}\n",
    "\n",
    "        # Call inference function with streaming\n",
    "        response_generator = inference_with_streaming(\n",
    "            prompt=user_text if user_text else None,\n",
    "            image_paths=user_files,\n",
    "            system_prompt=current_system_prompt,\n",
    "            max_new_tokens=max_tokens,\n",
    "            min_pixels=min_pixels,\n",
    "            max_pixels=max_pixels\n",
    "        )\n",
    "\n",
    "        full_response_content = \"\"\n",
    "        for partial_response in response_generator:\n",
    "            cleaned_partial = clean_response(partial_response)\n",
    "            full_response_content = cleaned_partial\n",
    "            # Update the last assistant message in history during streaming\n",
    "            history[-1][\"content\"] = full_response_content\n",
    "            yield history, {\"text\": \"\", \"files\": []}\n",
    "\n",
    "    except Exception as e:\n",
    "        error_msg = f\"An unexpected error occurred: {str(e)}\"\n",
    "        print(f\"Error in chat_with_history: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # Update or add an assistant message with the error\n",
    "        if history and history[-1][\"role\"] == \"assistant\" and history[-1][\"content\"] == \"\":\n",
    "            history[-1][\"content\"] = error_msg\n",
    "        else:\n",
    "            history.append({\"role\": \"assistant\", \"content\": error_msg})\n",
    "        yield history, {\"text\": \"\", \"files\": []}\n",
    "\n",
    "def clear_history():\n",
    "    \"\"\"Clears the chatbot history and the MultimodalTextbox input.\"\"\"\n",
    "    return [], {\"text\": \"\", \"files\": []}\n",
    "\n",
    "# --- Gradio Interface Definition ---\n",
    "with gr.Blocks(title=\"Qwen2.5-VL Assistant\") as demo:\n",
    "    gr.Markdown(\"# Qwen2.5-VL Conversational Assistant\")\n",
    "    gr.Markdown(\"Upload images and ask questions about them, or have a regular text conversation.\")\n",
    "\n",
    "    with gr.Column():\n",
    "        gr.Markdown(\"## Conversation\")\n",
    "        chatbot = gr.Chatbot(\n",
    "            label=\"Conversation History\",\n",
    "            elem_id=\"chatbot\",\n",
    "            height=500,\n",
    "            render_markdown=True,\n",
    "            type='messages'\n",
    "        )\n",
    "        \n",
    "        # MultimodalTextbox for input\n",
    "        msg = gr.MultimodalTextbox(\n",
    "            label=\"Your Message\", \n",
    "            placeholder=\"Type your message here or upload an image...\", \n",
    "            interactive=True,\n",
    "            file_types=[\"image\"],\n",
    "        )\n",
    "        \n",
    "        # Settings section\n",
    "        with gr.Accordion(\"Settings\", open=False): \n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    usage_mode = gr.Radio(\n",
    "                        list(SYSTEM_PROMPTS.keys()),\n",
    "                        value=\"General Assistant\",\n",
    "                        label=\"Usage Mode\",\n",
    "                        interactive=True\n",
    "                    )\n",
    "                    custom_system_prompt = gr.Textbox(\n",
    "                        label=\"Custom System Prompt (Overrides Usage Mode)\",\n",
    "                        placeholder=\"Enter your custom system prompt here...\",\n",
    "                        lines=2,\n",
    "                        interactive=True\n",
    "                    )\n",
    "            with gr.Row():\n",
    "                with gr.Column():\n",
    "                    max_tokens = gr.Slider(\n",
    "                        minimum=100,\n",
    "                        maximum=32000,\n",
    "                        value=800,\n",
    "                        label=\"Max New Tokens\",\n",
    "                        step=100,\n",
    "                        interactive=True\n",
    "                    )\n",
    "                with gr.Column():\n",
    "                    min_pixels = gr.Slider(\n",
    "                        minimum=1000,\n",
    "                        maximum=1000000,\n",
    "                        value=224 * 224,\n",
    "                        label=\"Min Pixels for Image Processing\",\n",
    "                        step=1000,\n",
    "                        interactive=True\n",
    "                    )\n",
    "                with gr.Column():\n",
    "                    max_pixels = gr.Slider(\n",
    "                        minimum=1000000,\n",
    "                        maximum=50000000,\n",
    "                        value=1280 * 28 * 28,\n",
    "                        label=\"Max Pixels for Image Processing\",\n",
    "                        step=100000,\n",
    "                        interactive=True\n",
    "                    )\n",
    "            \n",
    "            clear_btn = gr.Button(\"Clear Chat\", variant=\"secondary\")\n",
    "    \n",
    "    # --- Event Handlers ---\n",
    "    msg.submit(\n",
    "        chat_with_history,\n",
    "        inputs=[msg, chatbot, usage_mode, custom_system_prompt, max_tokens, min_pixels, max_pixels],\n",
    "        outputs=[chatbot, msg],\n",
    "    )\n",
    "    \n",
    "    clear_btn.click(clear_history, outputs=[chatbot, msg])\n",
    "\n",
    "# --- Launch the Gradio App ---\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(\n",
    "        debug=True,\n",
    "        # Uncomment for public access:\n",
    "        # server_name=\"0.0.0.0\",\n",
    "        # server_port=7860,\n",
    "        # share=True,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "omni_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
